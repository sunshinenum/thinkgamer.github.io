<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>文艺与Code | Thinkgamer的博客</title>
  <icon>https://www.gravatar.com/avatar/1b9c8afc3fc1dc6be26316835c6f4fc4</icon>
  <subtitle>CTR/DL/ML/RL</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://thinkgamer.cn/"/>
  <updated>2019-04-19T13:42:15.166Z</updated>
  <id>http://thinkgamer.cn/</id>
  
  <author>
    <name>Thinkgamer</name>
    <email>thinkgamer@163.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Thinkgamer&#39;s 简历</title>
    <link href="http://thinkgamer.cn/8888/08/08/%E5%85%B3%E4%BA%8E%E6%88%91/"/>
    <id>http://thinkgamer.cn/8888/08/08/关于我/</id>
    <published>8888-08-08T00:08:08.000Z</published>
    <updated>2019-04-19T13:42:15.166Z</updated>
    
    <content type="html"><![CDATA[<hr><h1 id="个人信息"><a href="#个人信息" class="headerlink" title="个人信息"></a>个人信息</h1><p>唯一：&nbsp;&nbsp;&nbsp;&nbsp;<strong>Thinkgamer</strong><br>姓名：&nbsp;&nbsp;&nbsp;&nbsp;<strong>高阳团</strong><br>家乡：&nbsp;&nbsp;&nbsp;&nbsp;<strong>河南-郑州</strong><br>邮箱：&nbsp;&nbsp;&nbsp;&nbsp;<strong>thinkgamer@163.com</strong><br>毕业：&nbsp;&nbsp;&nbsp;&nbsp;<strong>沈阳航空航天大学-计算机学院-软件工程</strong><br>就职：&nbsp;&nbsp;&nbsp;&nbsp;<strong>北京 | 京东商城 | 算法工程师</strong></p><hr><h1 id="技术园地："><a href="#技术园地：" class="headerlink" title="技术园地："></a>技术园地：</h1><ul><li>CSDN：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a></li><li>Github：<a href="https://github.com/Thinkgamer" target="_blank" rel="external">https://github.com/Thinkgamer</a></li><li>知乎: <a href="https://www.zhihu.com/people/thinkgamer/activities" target="_blank" rel="external">https://www.zhihu.com/people/thinkgamer/activities</a></li><li>公众号：数据与算法联盟<br><img src="/assets/img/gongzhonghao.jpg" weight="250px" height="250px"></li></ul><hr><h1 id="工作经历"><a href="#工作经历" class="headerlink" title="工作经历"></a>工作经历</h1><h2 id="2017-12-25～至今-京东商城"><a href="#2017-12-25～至今-京东商城" class="headerlink" title="2017-12-25～至今 | 京东商城"></a>2017-12-25～至今 | 京东商城</h2><ul><li>个性化消息Push</li></ul><blockquote><p>简称“种草”。针对京东用户进行消息的个性化Push，增强用户黏性和交互。<br>负责种草整个联动方案，组织会议进行讨论和需求下发，资源安排等。<br>基于机器学习的个性化消息push模型开发，训练，调优，上线等。<br>深度学习模型调研，基于公司内部平台，上线基于tensorflow-serving的深度学习模型，效果较ML模型提升显著。</p></blockquote><ul><li>Plus会员个性化推荐</li></ul><blockquote><p>理解内部推荐架构原理，负责开发方案，资源安排。<br>基于机器学习的个性化消息push模型开发，训练，调优，上线等。<br>相关CTR预估模型研究与在plus业务数据集上的离线测试。</p></blockquote><ul><li>商品价格段模型</li></ul><blockquote><p>基于KMeans构建商品价格段模型，实现了基于MR和Spark两个版本的代码。</p></blockquote><ul><li>商品质量分模型</li></ul><blockquote><p>基于线性模型构建商品质量分模型，用户推荐架构中的召回粗排。 </p></blockquote><ul><li>特征监控模型</li></ul><blockquote><p>数据和特征决定了机器学习的上限,而模型和算法只是逼近这个上限。特征对于模型来说极其重要，因为对于特征的监控十分有必要，该模型支持使用者自定义监控指标和监控字段，能够有效的减少出现问题时的排查时间提高效果，并进行预警。</p></blockquote><ul><li>基础数据开发<ul><li>业务内特征开发</li><li>全站特征开发</li><li>召回数据开发</li></ul></li></ul><h2 id="2016-10～2017-12-北京万维星辰科技有限公司"><a href="#2016-10～2017-12-北京万维星辰科技有限公司" class="headerlink" title="2016-10～2017-12 | 北京万维星辰科技有限公司"></a>2016-10～2017-12 | 北京万维星辰科技有限公司</h2><ul><li>搭建基于 Hadoop 和 ELK 技术栈的日志分析系统</li></ul><blockquote><p>参与设计了基于 ELK 的日志分析系统，提出并搭建了 Hadoop 数据备份系统，研究了 ELK 周边的<br>开源产品 ，学习并使用 rails 实现 es 数据的快照备份。</p></blockquote><ul><li>异常检测算法研究与实现</li></ul><blockquote><p>1：根据合作方提供的 wlan 上网数据，对用户进行肖像刻画，从而对后入数据进行异常值估计。</p><p>2：研究基于指数平滑和线性回归的异常值检测，并使用 python 的 elasticsearch 进行实现。</p></blockquote><ul><li>中彩/德州银行日志审计项目</li></ul><blockquote><p>利用公司的日志分析系统对中国福利彩票和德州银行的日志进行分析，并形成安全事件，提出相应的整改和解决意见，形成月度报告。</p></blockquote><h2 id="2016-07～2016-09-北京广联达软件有限公司"><a href="#2016-07～2016-09-北京广联达软件有限公司" class="headerlink" title="2016-07～2016-09 | 北京广联达软件有限公司"></a>2016-07～2016-09 | 北京广联达软件有限公司</h2><blockquote><p>实习以课题形式（课题为：基于质量数据的数据分析平台搭建）展开，利用 Hadoop 等开源组件搭建了 5 台分布式系统，包含 Hadoop，Hive，Spark，Zookeeper，Sqoop 和 Hbase，在该平台上完成了豆瓣影评数据分析 Demo</p></blockquote><hr><h1 id="技能掌握"><a href="#技能掌握" class="headerlink" title="技能掌握"></a>技能掌握</h1><ul><li>熟练掌握基于机器学习和深度学习的CTR预估算法，包括GBDT/LR/FM/FFM/FTRL/XGBoost/Wide&amp;Deep/DeepFM/DNN/FNN/PFF等。</li><li>熟悉推荐系统的数据流和过滤，召回，排序，展示等架构。</li><li>熟练掌握相关机器学习算法并用来构建基本模型。</li><li>熟练用户画像/物品画像/特征工程。</li><li>熟练Spark/MR/Hive/Python开发，了解相关大数据产品。</li><li>了解爬虫/Web后端开发，曾开发多个基于Django的网站后端。</li><li>了解强化学习/迁移学习/NLP。</li><li>熟悉ELK技术栈/Linux/Docker。</li></ul><hr><h1 id="大学经历"><a href="#大学经历" class="headerlink" title="大学经历"></a>大学经历</h1><h2 id="项目经历"><a href="#项目经历" class="headerlink" title="项目经历"></a>项目经历</h2><ul><li>基于 Hadoop 和机器学习的博客统计分析平台</li></ul><blockquote><p>采用 Django 作为 Web 开发基础，Python 爬取了 CSDN 博客的部分数据，存储到 hdfs 上，利用 MapReduce 对数据进行了离线计算，将解析好的字段存储到 Hive 中，利用 python 开发实现了协同过滤算法和 PangRank 算法。最终此项目在辽宁省计算机作品大赛中获得二等奖，中国大学生计算机作品大赛中获得三等奖。</p></blockquote><ul><li>图书推荐系统</li></ul><blockquote><p>python 爬取了豆瓣图书数据，对数据进行清洗之后，使用基于 Item 和 User 的协同过滤算法对登录用户产生图书推荐，此项目为大三期间为一个网友做的毕业设计。</p></blockquote><h2 id="荣誉奖励"><a href="#荣誉奖励" class="headerlink" title="荣誉奖励"></a>荣誉奖励</h2><ul><li>单项一等奖学金  * 2</li><li>综合二等奖学金  * 2</li><li>单项支援服务标兵</li><li>优秀团干 * 2</li><li>辽宁省ACM优秀志愿者</li><li>校ACM三等奖</li><li>沈阳航空航天大学计算机作品大赛二等奖【网站】</li><li>辽宁省计算机作品大赛二等奖【博客统计分析系统】</li><li>中国大学生作品大赛三等奖【博客统计分析系统】</li></ul><h2 id="工作经历-1"><a href="#工作经历-1" class="headerlink" title="工作经历"></a>工作经历</h2><ul><li>助理辅导员 | 2014.09-2015.07</li></ul><blockquote><p>计算机学院 2014 级新生助理辅导员，协助辅导员进行大一班级的日常管理</p></blockquote><ul><li>活动部部长 | 2014.09-2015.07</li></ul><blockquote><p>爱心联合会活动部部长，负责相关活动的宣传推广与执行</p></blockquote><ul><li>班级团支书 | 2013.09-2017.06</li></ul><blockquote><p>协助辅导员进行班级日常的管理和相关共青团工作的开展</p></blockquote><hr><h1 id="自我评价"><a href="#自我评价" class="headerlink" title="自我评价"></a>自我评价</h1><ul><li>不服输，爱钻研，具有自学能力和解决问题能力。</li><li>喜欢看书，看文章，整理笔记。</li><li>文艺Coder。</li></ul><hr><h1 id="联系我："><a href="#联系我：" class="headerlink" title="联系我："></a>联系我：</h1><ul><li><img src="/assets/img/myweixin.png" height="300" width="220"></li></ul><p>PS：加我微信，拉你进数据与算法交流群，进行头脑风暴</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;h1 id=&quot;个人信息&quot;&gt;&lt;a href=&quot;#个人信息&quot; class=&quot;headerlink&quot; title=&quot;个人信息&quot;&gt;&lt;/a&gt;个人信息&lt;/h1&gt;&lt;p&gt;唯一：&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;strong&gt;Thinkgamer&lt;/strong&gt;&lt;br
      
    
    </summary>
    
    
      <category term="Thinkgamer" scheme="http://thinkgamer.cn/tags/Thinkgamer/"/>
    
  </entry>
  
  <entry>
    <title>商务合作介绍</title>
    <link href="http://thinkgamer.cn/6666/06/06/%E5%95%86%E5%8A%A1%E5%90%88%E4%BD%9C%E4%BB%8B%E7%BB%8D/"/>
    <id>http://thinkgamer.cn/6666/06/06/商务合作介绍/</id>
    <published>6666-06-05T16:00:00.000Z</published>
    <updated>2019-04-13T04:40:15.346Z</updated>
    
    <content type="html"><![CDATA[<hr><center><h1>WelCome To “Thinkgamer 小站”</h1></center><hr><center><h2>合作范围</h2></center><div class="table-container"><table><thead><tr><th style="text-align:center">Web全栈</th><th style="text-align:center">数据服务</th><th style="text-align:center">模型构建</th></tr></thead><tbody><tr><td style="text-align:center">:-:</td><td style="text-align:center">:-:</td><td style="text-align:center">:-: </td></tr><tr><td style="text-align:center">论文算法实现</td><td style="text-align:center">大数据服务</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">:-:</td><td style="text-align:center">:-:</td><td style="text-align:center">:-: </td></tr><tr><td style="text-align:center">跟拍摄影</td><td style="text-align:center">广告接入</td><td style="text-align:center"></td></tr></tbody></table></div><blockquote><p>全网唯一ID：Thinkgamer，左侧”关于我“关注微信公众号”数据与算法联盟“，可在公众号添加我的微信，本人涉猎范围包括：推荐系统，Python，机器学习，Web开发，大数据云计算，ELK。</p></blockquote><h1 id="▶-Web全栈"><a href="#▶-Web全栈" class="headerlink" title="▶ Web全栈"></a>▶ Web全栈</h1><blockquote><p>如果您在创业，苦于没有额外精力管理一个技术团队；如果您在工作，遇到了一些您解决不了的问题；如果您的网站苦于没有运维；如果您的数据需要备份；如果一切有关Web开发运维的问题。您都可以来找我，我虽不是最厉害的，但绝对会为您提供最优质的服务。</p></blockquote><h1 id="▶-数据服务"><a href="#▶-数据服务" class="headerlink" title="▶ 数据服务"></a>▶ 数据服务</h1><blockquote><p>包含但不局限于以下数据相关的服务：</p></blockquote><ul><li>数据采集（一次性和程序开发）</li><li>数据清洗</li><li>数据可视化（不限于Web）</li><li>数据存储方案设计与实现</li><li>… …</li></ul><p>本人曾多次向他人提供数据相关的技术服务，积累了一定的经验，相信能够为您提供全方位的数据服务。</p><h1 id="▶-模型构建"><a href="#▶-模型构建" class="headerlink" title="▶ 模型构建"></a>▶ 模型构建</h1><blockquote><p>根据对方提供的具体业务场景，进行相关模型选择与构建。当然，如果有荣幸参与您的场景选定和数据准备阶段，也是极好的。</p></blockquote><h1 id="▶-论文算法实现"><a href="#▶-论文算法实现" class="headerlink" title="▶ 论文算法实现"></a>▶ 论文算法实现</h1><blockquote><p>如果您是一个马上要毕业的本科或者研究生，如果您苦于论文的立项与项目实现，如果您没有更好的主意，欢迎您来找我，加我的个人微信，为您的毕业保驾护航。</p></blockquote><h1 id="▶-大数据与分布式计算"><a href="#▶-大数据与分布式计算" class="headerlink" title="▶ 大数据与分布式计算"></a>▶ 大数据与分布式计算</h1><blockquote><p>提供大数据相关的服务，包含但不局限于：</p></blockquote><ul><li>大数据分析平台方案设计</li><li>大数据分析平台搭建</li><li>基于平台的数据分析Demo实现</li><li>海量数据的分布式计算处理</li><li>… …</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;欢迎来找我，24小时在线。</p><h1 id="▶-广告接入"><a href="#▶-广告接入" class="headerlink" title="▶ 广告接入"></a>▶ 广告接入</h1><blockquote><p>眼前的黑不是黑，Ta们说的白是什么白，也许一直是我们忘了搭一座桥，到对方的心里瞧一瞧。你的品牌，你的知名度为什么那么低，因为你没有使用我的广告接入，那么问题来了，包含但不局限于以下几种情况的，可以加我微信私聊了：</p></blockquote><ul><li>品牌宣传</li><li>广告位接入</li><li>公众号互相推广</li><li>个人网站/社区主页链接</li><li>… …</li></ul><h1 id="▶-跟拍摄影"><a href="#▶-跟拍摄影" class="headerlink" title="▶ 跟拍摄影"></a>▶ 跟拍摄影</h1><blockquote><p>如果您在旅游途中缺少了一个摄影的小跟班；如果您苦于找不到好的角度拍照；如果您是一个人，苦于没有人照出你的美；如果您的照片需要美化与调整。那么请您来找我，保证为您提供最优质的技术与服务。</p><p>业务涉及：</p></blockquote><ul><li>跟拍摄影</li><li>照片美化与调整</li><li>PS技术服务</li></ul><center><font color="#0099ff" size="8" face="黑体">小本生意&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;诚信经验<br><br>大神勿扰&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;自求多福</font> </center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;center&gt;
&lt;h1&gt;WelCome To “Thinkgamer 小站”&lt;/h1&gt;
&lt;/center&gt;

&lt;hr&gt;
&lt;center&gt;
&lt;h2&gt;合作范围&lt;/h2&gt;
&lt;/center&gt;

&lt;div class=&quot;table-container&quot;&gt;
&lt;table&gt;
&lt;t
      
    
    </summary>
    
    
      <category term="商务合作" scheme="http://thinkgamer.cn/tags/%E5%95%86%E5%8A%A1%E5%90%88%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>线性模型之PLA数学公式推导</title>
    <link href="http://thinkgamer.cn/2019/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E4%B9%8BPLA%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/"/>
    <id>http://thinkgamer.cn/2019/04/16/机器学习/线性模型之PLA数学公式推导/</id>
    <published>2019-04-16T11:13:32.000Z</published>
    <updated>2019-04-16T23:09:04.145Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>感知机（Perceptron）是一种广泛使用的线性分类器，相当于最简单的人工神经网络，只有一个神经元。其全称是PLA（Perceptron Linear Algorithm），线性感知机算法。</p><p>感知机是对生物神经元的简单数学模型，有与生物神经元相对应的部件，比如权重（突触）、偏置（阈值）及激活函数（细胞体），输出值为 +1 或者 -1。</p><script type="math/tex; mode=display">\hat{y} = sgn(w^Tx)</script><p>对于二分类问题，可以使用感知机算法来解决。PLA的原理是逐点解决，首先在超平面上随意取一条分类面，统计分类错误的点，然后随机对某个错误点修正，即变换直线的位置，使该错误点被修正，接着再随机选取另外一个错误点进行修正，分类面不断变化，直到所有点都分类正确了，就得到了最佳分类面。</p><p>利用二维平面进行解释，第一种情况是错误的将正样本（y=1）分类为负样本（y=-1）。此时wx&lt;0，即w与x的夹角大于90度，分类线L的两侧。修正的方法是让夹角变小，修正w值，使二者位于直线同侧。</p><script type="math/tex; mode=display">w:=w+x=w+yx</script><p>修正过程如下：<br><img src="https://img-blog.csdnimg.cn/20190415193607475.jpg" alt="修正过程"></p><p>第二种情况就是错误的将负样本（y=-1）分类为正样本（y=1）。此时，wx&gt;0，即w与x的夹角小于90度，分类线L的同一侧。修正的方法是让夹角变大，修正w值，使二者位于分类线同侧。</p><script type="math/tex; mode=display">w:=w-x=w+yx</script><p>修正过程如下：<br><img src="https://img-blog.csdnimg.cn/20190415193855372.jpg" alt="修正过程"></p><p>经过上边两种情况分析，PLA每次更新参数w的表达式是一致的，掌握了每次w的优化表达式，那么PLA就能不断地将所有错误的分类样本纠正并分类正确。</p><h1 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h1><p>给定N个训练集样本{(x^n, y^n)},n&lt;=N，其中y^n 属于{+1,-1}，感知机试图学习到参数w*，使得对于每个样本(x^n,y^n)<br>有：</p><script type="math/tex; mode=display">y^n w^{*T}x^n>0,\forall n\in [1,N]</script><p>感知机算法是一种错误驱动的在线学习算法，先初始化一个权重向量w&lt;-0（通常是全零向量），然后每次分错一个样本（x,y）时，就用这个样本来更新权重。</p><script type="math/tex; mode=display">w \leftarrow w + yx</script><p>具体的感知机算法伪代码如下（==算法-1==）：<br><img src="https://img-blog.csdnimg.cn/20190415202606770.png" alt="感知机算法伪代码"></p><p>根据感知器的学习策略，可以反推出感知器的损失函数为：</p><script type="math/tex; mode=display">L (w;x,y)=max(0, -yw^Tx)</script><p>采用随机梯度的下降，其每次更新的梯度为：</p><script type="math/tex; mode=display">\frac{\partial L (w;x,y) }{ \partial w}=\begin{cases}0 & \text{ if } y^Tx>0 \\ -yx & \text{ if } y^Tx<0 \end{cases}</script><p>下图给出了感知机参数学习的过程，其中红色实心为正例，蓝色空心点为负例。黑色箭头表示权重向量，红色虚线箭头表示权重的更新方向。</p><p><img src="https://img-blog.csdnimg.cn/20190415203526997.png" alt="感知机参数学习的过程"></p><h1 id="感知机的收敛"><a href="#感知机的收敛" class="headerlink" title="感知机的收敛"></a>感知机的收敛</h1><p>Novikoff证明对于两类问题，如果训练集是线性可分的，那么感知器<br>算法可以在有限次迭代后收敛。然而，如果训练集不是线性分隔的，那么这个算法则不能确保会收敛。</p><p>当数据集是两类线性可分时，对于数据集D={(x^n,y^n)}，n属于N，其中x^n为样本的增广特征向量，y^n属于{-1，+1}，那么存在一个正的常数r(r&gt;0)和权重向量w，并且||w<em>||=1，对所有n都满足(w^</em>)(y^n x^n)&gt;r。</p><p>可以证明如下定理（定理-1）。</p><hr><p>给定一个训练集</p><script type="math/tex; mode=display">D={(x^n,y^n)},n\in {1,N}</script><p>假设R是训练集中最大的特征向量的模</p><script type="math/tex; mode=display">R=\underset{n}{max} \left \| x^n \right \|</script><p>如果训练集D线性可分，感知机学习算法-1的权重更新次数不超过 R^2/ r^2</p><hr><p>证明：<br>感知机算法的权重更新方式为（==公式-1==）：</p><script type="math/tex; mode=display">w_k = w_{k-1}+y^kx^k</script><p>其中x^k, y^k表示第k个错误分类的条件。<br>因为初始权重为0，在第K次更新时感知器的权重向量为（==公式-2==）：</p><script type="math/tex; mode=display">w_k = =\sum_{k=1}^{K}y^kx^k</script><p>分别计算||w||^2的上下界：</p><p><strong>计算其上界</strong>（公式-2）：</p><script type="math/tex; mode=display">\left \| w_k^2 \right \|</script><script type="math/tex; mode=display">= \left\| w_{K_1} + y^K x^K \right \|^2</script><script type="math/tex; mode=display">= \left \|  w_{K-1} \right \| ^2 + \left \| y^Kx^K \right \| ^2+2y^Kw_{K-1}x^K</script><script type="math/tex; mode=display">\leqslant \left \| w_{K-1} \right \|^2 + R^2</script><script type="math/tex; mode=display">\leqslant\left \| w_{K-2} \right \|^2+2R^2</script><script type="math/tex; mode=display">\leqslant KR^2</script><p><strong>计算其下界</strong>（公式-3）：</p><script type="math/tex; mode=display">\left \| w_k^2 \right \|</script><script type="math/tex; mode=display">=\left \| w^* \right \|^2 .\left \| w_K \right \|^2</script><script type="math/tex; mode=display">\geqslant \left \| w^{*T}w_K \right \| ^2</script><script type="math/tex; mode=display">=\left \| w^{*T}\sum_{k=1}^{K}(y^Kx^K) \right \|^2</script><script type="math/tex; mode=display">=\left \| \sum_{k=1}^{K}w^{*T}(y^Kx^K) \right \|^2</script><script type="math/tex; mode=display">\geq K^2r^2</script><p>==附==：两个向量内积的平方一定小于等于这两个向量的模的乘积。</p><p>由公式-2和公式-3得到（公式-4）</p><script type="math/tex; mode=display">K^2r^2 \leq \left \| w_K \right \|^2\leq KR^2</script><p>取最左和最右的两项，进一步得到K^2r^2 &lt;= K^2R^2，然后两边同时除以K，最终得到（公式-5）：</p><script type="math/tex; mode=display">K\leq \frac{R^2}{r^2}</script><p>因此在线性可分的情况下，算法-1会在R^2 / r^2步内收敛。</p><p>虽然感知机线性模型在线性可分的数据上可以保证收敛，但其存在以下不足：</p><ul><li>在数据集线性可分时，感知器虽然可以找到一个超平面把两类数据分开， 但并不能保证能其泛化能力。</li><li>感知器对样本顺序比较敏感。每次迭代的顺序不一致时，找到的分割超平 面也往往不一致。</li><li>如果训练集不是线性可分的，就永远不会收敛。</li></ul><h1 id="参数平均感知机"><a href="#参数平均感知机" class="headerlink" title="参数平均感知机"></a>参数平均感知机</h1><p>根据定理3.1，如果训练数据是线性可分的，那么感知器可以找到一个判别 函数来分割不同类的数据。如果间隔 γ 越大，收敛越快。但是感知器并不能保 证找到的判别函数是最优的(比如泛化能力高)，这样可能导致过拟合。</p><p>感知机的学习到的权重向量和训练样本的顺序相关。在迭代次序上排在后 面的错误样本，比前面的错误样本对最终的权重向量影响更大。比如有 1, 000 个 训练样本，在迭代 100 个样本后，感知器已经学习到一个很好的权重向量。在 接下来的 899 个样本上都预测正确，也没有更新权重向量。但是在最后第 1, 000 个样本时预测错误，并更新了权重。这次更新可能反而使得权重向量变差。</p><p>为了改善这种情况，可以使用“参数平均”的策略提高感知ji的鲁棒性，也叫投票感知机。</p><p>投票感知机记录第k次更新参数之后的权重w_k在之后的训练过程中正确分类样本的次数c_k。这样最后的分类器形式为（公式-6）：</p><script type="math/tex; mode=display">\hat{y} = sgn(\sum_{k=1}^{K}c_ksgn(w_k^Tx))</script><p>其中sgn(.)为符号函数。</p><p>投票感知机虽然提高了模型的泛化能力，但是需要保存K个权重向量。在实际的操作中会带来额外的开销。因此经常会使用一个简化的版本，即平均感知机。其表达式如下（公式-7）：</p><script type="math/tex; mode=display">\hat{y} = sgn(\sum_{k=1}^{K}c_k (w_k^Tx))=sgn( (\sum_{k=1}^{K}c_k w_k )^Tx )=sgn( \bar{w}^Tx)</script><p>其中 </p><script type="math/tex; mode=display">\bar{w}</script><p>为平均的权重向量。</p><p>假设w_{t,n}是在第t轮更新到第n个样本时的权重向量值，平均的权重向量也可以表示为（公式-8）：</p><script type="math/tex; mode=display">\bar{w} = \frac{\sum_{t=1}^{T} \sum_{n=1}^{N}w_{t,n}}{nT}</script><p>这个方法实现简单，只需要在算法-1中增加一个平均向量，并且在处理每一个样本后，进行更新（公式-9）：</p><script type="math/tex; mode=display">\bar{w} = \bar{w}+ w_{t,n}</script><p>但这个方法需要在处理每一个样本时都要更新平均权重，因为</p><script type="math/tex; mode=display"> \bar{w} ,w_{t,n}</script><p>都是稠密向量，因此更新操作比较费时。为了提高迭代速度，有很多改进的办法，让这个更新只需要在错误预测时才进行。下图给了一个改进的平均感知机算法的训练过程（算法-2）。<br><img src="https://img-blog.csdnimg.cn/20190416184043264.png" alt="改进的平均感知机算法的训练过程"></p><h1 id="扩展到多分类"><a href="#扩展到多分类" class="headerlink" title="扩展到多分类"></a>扩展到多分类</h1><p>原始的感知机是二分类模型，但也很容易的扩展到多分类，甚至是更一般的结构化学习问题。</p><p>之前介绍的线性分类模型中，分类函数都是在输入x的特征空间上。为了使得感知机可以处理更加复杂的输出，我们引入一个构建输入输出联合空间上的特征函数，将样本(x,y)对映射到一个特征向量空间。</p><p>在联合特征空间中，我们可以建立一个广义的感知机模型（公式-10）：</p><script type="math/tex; mode=display">\hat{y} = \underset{y\in Gen(x)}{ arg max} w^T \phi(x,y)</script><p>其中w为权重向量，Gen(x)表示输入x所有的输出目标集合。当处理C类分类问题时，Gen(x)={1,….,C}</p><p>在C分类中，一种常用的特征函数（公式-11）</p><script type="math/tex; mode=display">\phi(x,y)</script><p>是y和x的内积，其中y为类别的one-hot向量表示（公式-12）。</p><script type="math/tex; mode=display"> \phi(x,y) = vec(yx^T)\in R^{(d\times C)}</script><p>其中vec是向量化算子。</p><p>给定样本(x,y)，若</p><script type="math/tex; mode=display">x \in R^d</script><p>y为第c维为1的one-hot向量，则：<br><img src="https://img-blog.csdnimg.cn/20190416190108143.png" alt="y为第c维为1的one-hot向量"></p><p>广义感知器算法的训练过程如算法-3所示：</p><p><img src="https://img-blog.csdnimg.cn/20190416190225604.png" alt="广义感知器算法的训练过程"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="线性模型" scheme="http://thinkgamer.cn/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="Logistic Regression" scheme="http://thinkgamer.cn/tags/Logistic-Regression/"/>
    
      <category term="PLA" scheme="http://thinkgamer.cn/tags/PLA/"/>
    
      <category term="感知机" scheme="http://thinkgamer.cn/tags/%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>线性模型篇之softmax数学公式推导</title>
    <link href="http://thinkgamer.cn/2019/04/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%AF%87%E4%B9%8Bsoftmax%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/"/>
    <id>http://thinkgamer.cn/2019/04/07/机器学习/线性模型篇之softmax数学公式推导/</id>
    <published>2019-04-06T23:24:46.000Z</published>
    <updated>2019-04-13T06:01:56.575Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><blockquote><p>Softmax回归也称多项（multinomial）或者多类（multi-class）的Logistic回归，是Logistic回归在多类分类问题上的推广。和逻辑回归一样属于线性模型。</p></blockquote><h1 id="SoftMax回归简介"><a href="#SoftMax回归简介" class="headerlink" title="SoftMax回归简介"></a>SoftMax回归简介</h1><p>对于多类问题，类别标签</p><script type="math/tex; mode=display">y \in {1,2,3,...,C}</script><p>可以用C个取值，给定一个样本x，softmax回归预测的是属于类别c的概率为(公式-1)：</p><script type="math/tex; mode=display">p(y=c|x)=softmax(w_c^Tx)=\frac{exp(w_c^Tx)}{\sum_{c=1}^{C}exp(w_c^Tx)}</script><p>其中w_c是第c类的权重向量。</p><p>softmax回归的决策函数可以表示为(公式-2)：</p><script type="math/tex; mode=display">\hat{y}=  \underset{c=1}{ \overset{C}{arg max} } \ p(y=c|x) =\underset{c=1}{ \overset{C}{arg max} } \ w_c^T x</script><hr><p>softMax与Logistic回归的关系：</p><p>当类别个C=2时，softMax回归的决策函数为(公式-3)：</p><script type="math/tex; mode=display">\hat{y} = \underset{y\in {0,1}}{ arg max } \ w_y^Tx=I(w_1^Tx - w_0^Tx >0 )=I((w_1 - w_0)^Tx >0 )</script><p>其中I(.)是指示函数，对比二分类决策函数(公式-4)</p><script type="math/tex; mode=display">g(f(x,w))=sgn(f(x,w))=\begin{cases} & +1 \text{ if } f(x,w)>0 \\  & -1 \text{ if } f(x,w)<0 \end{cases}</script><p>其中sgn表示符号函数(sign function)，可以发现两类分类中的权重向量w=w1-w0</p><hr><p>向量表示：</p><p>公式-1用向量形式可以写为(公式-5)</p><script type="math/tex; mode=display">\hat{y}=softmax(W^Tx)=\frac{erp(W^Tx)}{1^Texp(W^Tx)}</script><p>其中W=[w_1,w_2,…,w_C]是由C个类的权重向量组成的矩阵，1为全1的向量，</p><script type="math/tex; mode=display">\hat{y}\in  R^C</script><p>为所有类别的预测条件概率组成的向量，第c维的值是第c类的预测条件概率。</p><h1 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h1><p>给定N个训练样本{(x^n, y^n)},n&lt;=N，softmax回归使用交叉熵损失函数来学习最优的参数矩阵W。</p><p>这里用C维的one-hot向量</p><script type="math/tex; mode=display">y \in {0,1} ^C</script><p>来表示类别标签，其向量表示为(公式-6)：</p><script type="math/tex; mode=display">y = [I(1=c),I(2=c),...,I(C=c)]^T</script><p>其中I(.)为指示函数。</p><p>采用交叉熵损失函数，softmax的经验风险函数为(公式-7)：</p><script type="math/tex; mode=display">R(W)=-\frac{1}{N}\sum_{n=1}^{N}\sum_{c=1}^{C}y_c^nlog\hat{y}_c^nR(W)=-\frac{1}{N}\sum_{n=1}^{N} (y^n)^Tlog\hat{y}^n</script><p>其中</p><script type="math/tex; mode=display">\hat{y}^n = softmax(W^Tx^n)</script><p>为样本x^n在每个类别的后验概率。</p><p>==说明：公式-7第一个式变换到第二个式是因为y_c类别中只有一个为1，其余为0，所以将第二个求和去除。==</p><p>风险函数R(W)关于W的梯度为(公式-8)：</p><script type="math/tex; mode=display">\frac{\partial R(W)}{\partial W} = -\frac{1}{N}\sum_{n=1}^{N}x^n(y^n-\hat{y}^n)^T</script><p>==<strong>证明：</strong>==</p><p>计算公式-8中的梯度，关键在于计算每个样本的损失函数</p><script type="math/tex; mode=display">L^n(W)=-(y^n)^Tlog\hat{y}^n</script><p>关于参数W的梯度，其中需要用到两个导数公式为：</p><ul><li>若y=softmax(z)，则</li></ul><script type="math/tex; mode=display">\frac{\partial y}{\partial z}=diag(y)-yy^T</script><ul><li>若</li></ul><script type="math/tex; mode=display">z=W^Tx=[w_1^Tx,w_2^Tx,...,w_C^Tx]^T</script><p>则</p><script type="math/tex; mode=display">\frac{\partial y}{\partial w_c}</script><p>为第c列为x，其余为0的矩阵。</p><script type="math/tex; mode=display">\frac{\partial z}{\partial w_c} = [ \frac{\partial w_1^Tx}{\partial w_c},\frac{\partial w_2^Tx}{\partial w_c},...,\frac{\partial w_C^Tx}{\partial w_c} ]=[0,0,..,x,...,0]=M_c(x)</script><p>根据链式法则，</p><script type="math/tex; mode=display">L^n(W) = -(y^n)^T log\hat{y}^n</script><p>关于w_c的偏导数为(公式-12)：</p><script type="math/tex; mode=display">\frac{\partial L^n(W) }{\partial w_c}= -\frac{ \partial ((y^n)^T log \hat{y}^n) }{\partial w_c}</script><script type="math/tex; mode=display">= -\frac{\partial z^n}{ \partial w_c } \frac{\partial \hat{y}^n}{ \partial z^n }\frac{\partial log \hat{y}^n}{ \partial \hat{y}^n } y^n</script><script type="math/tex; mode=display">=-M_c(x^n)(diag(\hat{y}^n)-\hat{y}^n(\hat{y}^n)^T)(diag(\hat{y}^n))^{-1} y^n</script><script type="math/tex; mode=display">=-M_c(x^n)(I-\hat{y}^n1^T)y^n</script><script type="math/tex; mode=display">=-M_c(x^n)(y^n-\hat{y}^n1^Ty^n)</script><script type="math/tex; mode=display">=-M_c(x^n)(y^n-\hat{y}^n)</script><script type="math/tex; mode=display">=-x^n[y^n-\hat{y}^n]_c</script><p>公式-12也可以表示为非向量形式(公式-13)：</p><script type="math/tex; mode=display">\frac{\partial L^n(W) }{\partial w_c}= -x^n(I(y^n=c)-\hat{y}_c^n)</script><p>其中I(.)为指示函数，根据公式-12可以得到(公式-14)</p><script type="math/tex; mode=display">\frac{\partial L^n(W) }{\partial W} = -x^n(y^n-\hat{y}^n)^T</script><p>采用梯度下降法，softmax回归的训练过程为：初始化W_0 &lt;- 0，然后通过下式进行迭代更新。</p><script type="math/tex; mode=display">W_{t+1} = W_t + \alpha (\frac{1}{N} \sum_{n=1}^{N}x^n(y^n - \hat{y}_{W_t} ^ n)^T)</script><p>其中a是学习率，</p><script type="math/tex; mode=display">\hat{y}_{W_t}^n</script><p>是当参数为W_t时，softmax回归模型的输出。</p><hr><p><strong>注意：</strong></p><blockquote><p>softmax回归中使用的C个权重向量是冗余的，即对所有权重向量都减去一个同样的向量v，不改变其输出结果。因此，softmax往往需要正则化来约束参数。此外，可以利用这个特性来避免计算softmax函数时在数值计算上溢出问题。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="线性模型" scheme="http://thinkgamer.cn/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="SoftMax" scheme="http://thinkgamer.cn/tags/SoftMax/"/>
    
  </entry>
  
  <entry>
    <title>线性模型篇之Logistic Regression数学公式推导</title>
    <link href="http://thinkgamer.cn/2019/04/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%AF%87%E4%B9%8BLogistic%20Regression%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/"/>
    <id>http://thinkgamer.cn/2019/04/02/机器学习/线性模型篇之Logistic Regression数学公式推导/</id>
    <published>2019-04-02T14:31:51.000Z</published>
    <updated>2019-04-13T05:09:02.305Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="两分类与多分类"><a href="#两分类与多分类" class="headerlink" title="两分类与多分类"></a>两分类与多分类</h1><ul><li>两类分类（Binary Classification）<ul><li>类别标签y只有两种取值，通常设为{0，1}</li><li>线性判别函数，即形如 y = w^T*x + b</li><li>分割超平面（hyper plane）,由满足f(w,x)=0的点组成</li><li>决策边界（Decision boundary）、决策平面（Decision surface）：即分分割超平面，决策边界将特征空间一分为二，划分成两个区域，每个区域对应一个类别。</li><li>有向距离（signed distance）</li></ul></li><li>多样分类（Multi-class Classification）<ul><li>分类的类别个数大于2，多分类一般需要多个线性判别函数，但设计这些判别函数有很多方式。eg：<ul><li>一对其余：属于和不属于</li><li>一对一</li><li>argmax（改进的一对其余）：属于每个类别的概率，找概率最大值</li></ul></li><li>参考：<a href="https://blog.csdn.net/Gamer_gyt/article/details/86378882" target="_blank" rel="external">多分类实现方式介绍和在Spark上实现多分类逻辑回归</a></li></ul></li></ul><h1 id="Logistic回归"><a href="#Logistic回归" class="headerlink" title="Logistic回归"></a>Logistic回归</h1><h2 id="LR回归"><a href="#LR回归" class="headerlink" title="LR回归"></a>LR回归</h2><p>Logistic回归（Logistic Regression，LR）是一种常见的处理二分类的线性回归模型。</p><p>为了解决连续的线性回归函数不适合做分类的问题，引入函数g：R^d -&gt; (0,1)来预测类别标签的后验概率p(y=1 | x)</p><p>其中g(.)通常称为激活函数（activation function），其作用是把线性函数的值域从实数区间“挤压”到了（0，1）之间，可以用概率表示。在统计文献中，g(.)的逆函数g(.)^-1也称为联系函数（Link Function）</p><p>在逻辑回归中使用Logistic作为激活函数，标签y=1的后验概率为(公式-1)：</p><script type="math/tex; mode=display">p(y=1 | x) = \sigma (w^T x)</script><script type="math/tex; mode=display">p(y=1 | x)= \frac{1}{1+exp(-w^T x)}</script><p>标签 y=0的后验概率为(公式-2)：</p><script type="math/tex; mode=display">p(y=0 | x) =1-p(y=0 | x)</script><script type="math/tex; mode=display">p(y=0 | x)= \frac{exp(-w^T x)}{1+exp(-w^T x)}</script><p>将公式-1进行等价变换，可得(公式-3)：</p><script type="math/tex; mode=display">w^T x = log \frac{p(y=1 | x)}{1-p(y=1 | x)}</script><script type="math/tex; mode=display">w^T x = log \frac { p(y=1 | x)}{p(y=0|x)}</script><p>其中</p><script type="math/tex; mode=display">\frac { p(y=1 | x)}{p(y=0|x)}</script><p>为样本x正反例后验概率的比例，称为几率（odds），几率的对数称为对数几率（log odds或者logit），公式-3中第一个表达式，左边是线性函数，logistic回归可以看做是预测值为“标签的对数几率”的线性回归模型，因为Logistic回归也称为对数几率回归（Logit Regression）。</p><p>附公式-1到公式-3的推导：</p><script type="math/tex; mode=display">p(y=1 | x)= \frac{1}{1+exp(-w^T x)}</script><script type="math/tex; mode=display">=> exp(-w^Tx) = \frac{1-p(y=1 | x)}{p(y=1 | x)}</script><script type="math/tex; mode=display">=> - w^T x = log \frac{1- p(y=1 | x)}{p(y=1 | x)}</script><script type="math/tex; mode=display">=>  w^T x = log (\frac{1- p(y=1 | x)}{p(y=1 | x)})^{-1}</script><script type="math/tex; mode=display">=> w^T x = log \frac{p(y=1 | x)}{1-p(y=1 | x)}</script><script type="math/tex; mode=display">=> w^T x = log \frac{p(y=1 | x)}{p(y=0 | x)}</script><h2 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h2><p>LR回归采用交叉熵作为损失函数，并使用梯度下降法对参数进行优化。给定N个训练样本{x_i,y_i}，i&lt;=N，使用LR对每个样本进行预测，并用输出x_i的标签为1的后验概率，记为y’_i(x)  (公式-4)</p><script type="math/tex; mode=display">y'_i(x) = \sigma(w^Tx_i),i\in N</script><p>由于y_i属于{0，1}，样本{x_i,y_i}的真实概率可以表示为(公式-5)：</p><script type="math/tex; mode=display">p_r(y_i =1 | x_i) = y_i</script><script type="math/tex; mode=display">p_r(y_i =0 | x_i) = 1- y_i</script><p>使用交叉熵损失函数，其风险函数为(公式-6)：</p><script type="math/tex; mode=display">R(w)= - \frac{1}{N}\sum_{n=1}^{N} (p_r(y_i =1 | x_i) log(y_i') + p_r(y_i =0 | x_i) log(1-y_i') )</script><script type="math/tex; mode=display">= - \frac{1}{N}\sum_{n=1}^{N} ( y_i log(y_i') + (1-y_i') log(1-y_i') )</script><p>风险函数R(w)关于参数w的导数为(公式-7)：</p><script type="math/tex; mode=display">\frac{ \partial R(w)}{ \partial w} = - \frac{1}{N}\sum_{n=1}^{N}( y_i \frac{y_i'(1-y_i')}{y_i'}x_i -(1-y_i)\frac{y_i'(1-y_i')}{1-y_i'}x_i  )</script><script type="math/tex; mode=display">= - \frac{1}{N}\sum_{n=1}^{N}( y_i(1-y_i')x_i -(1-y_i)y_i'x_i)</script><script type="math/tex; mode=display">= - \frac{1}{N}\sum_{n=1}^{N}x_i(y_i-y_i')</script><p>采用梯度下降算法，Logistic的回归训练过程为：初始化w_0 为0，然后通过下式来更新迭代参数(公式-8)。</p><script type="math/tex; mode=display">w_{t+1} \leftarrow w_t + \alpha \frac{1}{N}\sum_{n=1}^{N} x_i(y_i-y_{w_t}')</script><p>其中a是学习率，y_{wt}’是当参数为w_t 时，Logistic回归的输出。</p><p>从公式-6可知，风险函数R(w)是关于参数w的连续可导的凸函数，因此除了梯度下降算法外，Logistic还可以使用高阶的优化算法，比如牛顿法来进行优化。</p><p>说明:</p><ul><li>两个未知数相乘求导：<script type="math/tex; mode=display">(ab)' = a'b + ab'</script></li><li>sigmoid函数求导后为：<script type="math/tex; mode=display">\sigma ' = \sigma (1-\sigma )x</script></li></ul><hr><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/44591359" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/44591359</a></li><li><a href="https://blog.csdn.net/wgdzz/article/details/48816307" target="_blank" rel="external">https://blog.csdn.net/wgdzz/article/details/48816307</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="LR" scheme="http://thinkgamer.cn/tags/LR/"/>
    
      <category term="线性模型" scheme="http://thinkgamer.cn/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="Logistic Regression" scheme="http://thinkgamer.cn/tags/Logistic-Regression/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法分类</title>
    <link href="http://thinkgamer.cn/2019/03/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB/"/>
    <id>http://thinkgamer.cn/2019/03/26/机器学习/机器学习算法分类/</id>
    <published>2019-03-26T09:43:37.000Z</published>
    <updated>2019-04-13T05:10:24.623Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><blockquote><p>机器学习算法可以按照不同的标准进行分类。比如按函数f(X)的不同，机器学习算法可以分为线性模型和非线性模型；按照学习准则的不同，机器学习算法也可以分为统计方法和非统计方法。</p><p>但一般而言，会按照训练样本提供的信息以及反馈方式不同，将机器学习算法分为以下几类，下面将一一细说。</p></blockquote><ul><li>监督学习</li><li>无监督学习</li><li>强化学习</li></ul><h1 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h1><p>如果机器学习的目标是通过建模样本的特征x和标签y之间的关系：</p><script type="math/tex; mode=display">y=f(x,\theta )</script><p>或</p><script type="math/tex; mode=display">p(y|x,\theta)</script><p>并且训练集中的每个样本都有标签，那么这类学习称之为监督学习（Supervised Learning）。根据标签类型的不同，监督学习又可以分为回归和分类两种。</p><h2 id="回归（Regression）"><a href="#回归（Regression）" class="headerlink" title="回归（Regression）"></a>回归（Regression）</h2><p>回归问题中的标签y是连续值（实数或者连续整数）</p><script type="math/tex; mode=display">y=f(x,\theta )</script><p>的输出也是连续值。</p><h2 id="分类（Classification）"><a href="#分类（Classification）" class="headerlink" title="分类（Classification）"></a>分类（Classification）</h2><p>分类问题中的标签y是离散的类别，在分类问题中，学习到的模型也成为分类器（Classifier）。分类问题根据其类别的数量又可以分为二分类（Binary Clssification）和多分类（Mutil-class Classification）。</p><h2 id="机构化学习（Structured-Learning）"><a href="#机构化学习（Structured-Learning）" class="headerlink" title="机构化学习（Structured Learning）"></a>机构化学习（Structured Learning）</h2><p>结构化学习的输出对象是结构化的对象，比如序列、树、图等，由于结构化学习的输出空间比较大，因此我们一般定义一个联合特征空间，将x,y映射为该空间中的联合特征向量（x,y）,预测模型可以写为：</p><script type="math/tex; mode=display">\hat{y}=\underset{y \in Gen(x)}{arg max f(\phi (x,y),\theta )}</script><p>其中gen(x)表示x所有可能的输出目标集合。计算arg max的过程也称为解码（decoding）过程，一般通过动态规划的方法来计算。</p><h1 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h1><p>无监督学习（Unsupervised Learning）是指从不包含目标标签的训练样本中自动学习到一些有价值的信息。典型的无监督学习问题有聚类、密度估计、特征学习、降维等。</p><h1 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h1><p>强化学习（Reinforcement Learning）是一类通过交互来学习的机器学习算法。在强化学习中，智能体根据环境的状态作出一个动作，并得到即时或延时的奖励。智能体在和环境的交互中不断学习并调整策略，以取得最大化的期望总回报。</p><p>下表给出了三种机器学习类型</p><p><img src="https://img-blog.csdnimg.cn/20190326174240856.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>监督学习需要每个样本都有标签，而无监督学习则不需要标签。一般而言，监督学习通常大量的有标签数据集，这些数据集是一般都需要由人工进行标注，成本很高。因此，也出现了很多弱监督学习（Weak Supervised Learning）和半监督学习（Semi-Supervised Learning）的方法，希望从大规模的无标注数据中充分挖掘有用的信息，降低对标注样本数量的要求。强化学习和监督学习的不同在于强化学习不需要显式地以“输入/输出对”的方式给出训练样本，是一种在线的学习机制。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="机器学习" scheme="http://thinkgamer.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法" scheme="http://thinkgamer.cn/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>从线性回归看偏差-方差分解（Bias-Variance Decomposition）</title>
    <link href="http://thinkgamer.cn/2019/03/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BB%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9C%8B%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE%E5%88%86%E8%A7%A3%EF%BC%88Bias-Variance%20Decomposition%EF%BC%89/"/>
    <id>http://thinkgamer.cn/2019/03/25/机器学习/从线性回归看偏差-方差分解（Bias-Variance Decomposition）/</id>
    <published>2019-03-25T15:18:55.000Z</published>
    <updated>2019-04-13T05:12:00.678Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>对于数字序列1，3，5，7，？，正常情况下大家脑海里蹦出的是9，但是217314也是其一个解<br>9对应的数学公式为</p><script type="math/tex; mode=display">f(x)=2x-1</script><p>217314对应的数学公式为</p><script type="math/tex; mode=display">f(x)=\frac{18111}{2} x^{4}-90555x^{3}+\frac{633885}{2}x^{2}-452773x+217331</script><p>Python 实现为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; def f(x):</span><br><span class="line">...     return 18111/2 * pow(x,4) -90555 * pow(x,3) + 633885/2 * pow(x,2) -452773 * x +217331</span><br><span class="line">... </span><br><span class="line">&gt;&gt;&gt; f(1)</span><br><span class="line">1.0</span><br><span class="line">&gt;&gt;&gt; f(2)</span><br><span class="line">3.0</span><br><span class="line">&gt;&gt;&gt; f(3)</span><br><span class="line">5.0</span><br><span class="line">&gt;&gt;&gt; f(4)</span><br><span class="line">7.0</span><br><span class="line">&gt;&gt;&gt; f(5)</span><br><span class="line">217341.0</span><br></pre></td></tr></table></figure></p><p>当机器学习模型进行预测的时候，通常都需要把握一个非常微妙的平衡，一方面我们希望模型能够匹配更多的训练数据，相应的增加其复杂度，否则会丢失相关特征的趋势（即模型过拟合）；但是另一方面，我们又不想让模型过分的匹配训练数据，相应的舍弃部分复杂的，因为这样存在过度解析所有异常值和伪规律的风险，导致模型的泛化能力差（即模型欠拟合）。因此在模型的拟合能力和复杂度之前取得一个比较好的权衡，对于一个模型来讲十分重要。而偏差-方差分解（Bias-Variance Decomposition）就是用来指导和分析这种情况的工具。</p><h1 id="偏差和方差定义"><a href="#偏差和方差定义" class="headerlink" title="偏差和方差定义"></a>偏差和方差定义</h1><ul><li>偏差（Bias）：即预测数据偏离真实数据的情况。</li><li>方差（Variance）：描述的是随机变量的离散程度，即随机变量在其期望值附近的波动程度。</li></ul><h1 id="偏差-方差推导过程"><a href="#偏差-方差推导过程" class="headerlink" title="偏差-方差推导过程"></a>偏差-方差推导过程</h1><p>以回归问题为例，假设样本的真实分布为p_r(x,y)，并采用平方损失函数，模型f(x)的期望错误为(公式2.1)：</p><script type="math/tex; mode=display">R(f) = E_{(x,y)\sim p_r{(x,y)}} \left [ (y-f(x))^2 \right ]</script><p>那么最优模型为(公式2.2)：</p><script type="math/tex; mode=display">f^*(x) = E_{y\sim p_r{(y|x)}} \left [ y \right ]</script><p>其中p_r(y|x)为真实的样本分布，f^*(x)为使用平方损失作为优化目标的最优模型，其损失为(公式2.3)：</p><script type="math/tex; mode=display">\varepsilon  = E_{(x,y)\sim p_r{(x,y)}} \left [ (y-f^*(x))^2 \right ]</script><p>损失</p><script type="math/tex; mode=display">\varepsilon</script><p>通常是由于样本分布及其噪声引起的，无法通过优化模型来减少。<br>期望错误可以分解为(公式2.4)：</p><script type="math/tex; mode=display">R(f)</script><script type="math/tex; mode=display">= E_{(x,y)\sim p_r{(x,y)}} \left [ (y- f^*(x) + f^*(x) -f(x))^2 \right ]</script><script type="math/tex; mode=display">= E_{x\sim p_r{(x)}}\left [ (f(x) - f^*(x))^2 \right ] + \varepsilon</script><p>公式2.4中的第一项是机器学习可以优化的真实目标，是当前模型和最优模型之间的差距。</p><p>在实际训练一个模型f(x)时，训练集D是从真实分布p_r(x,y)上独立同分布的采样出来的有限样本集合。不同的训练集会得到不同的模型。令f_D(x)表示在训练集D上学习到的模型，一个机器学习算法（包括模型和优化算法）的能力可以通过模型在不同训练集上的平均性能来体现。</p><p>对于单个样本x，不同训练集D得到的模型f_D(x)和最优模型f^*(x)的上的期望差距为（公式2.5）：</p><script type="math/tex; mode=display">E_D[( f_D(x)-f^*(x) )^2]</script><script type="math/tex; mode=display">=E_D\left [  ( f_D(x)  - E_D[f_D(x)] +E_D[f_D(x)]   -f^*(x) )^2   \right ]</script><script type="math/tex; mode=display">=( E_D[f_D(x)]   -f^*(x) )^2 )  + E_D[(f_D(x)  - E_D[f_D(x)] )^2]</script><p>公式2.5最后一行中的第一项为偏差(bias)，是指一个模型在不同训练集上的平均性能和与最优模型的差异，偏差可以用来衡量一个模型的拟合能力；第二项是方差（variance），是指一个模型在不同训练集上的差异，可以用来衡量一个模型是否容易过拟合。</p><p>集合公式2.4和公式2.5，期望错误可以分解为(公式2.6)：</p><script type="math/tex; mode=display">R(f)= (bias)^2 + variance +  \varepsilon</script><p>其中</p><script type="math/tex; mode=display">(bias)^2 =  E_X[E_D[f_D(x)]   -f^*(x) )^2 ]</script><script type="math/tex; mode=display">variance = E_X [ E_D[(f_D(x)  - E_D[f_D(x)] )^2] ]</script><p>最小化期望错误等价于最小化偏差和方差之和。</p><h1 id="偏差和方差分析"><a href="#偏差和方差分析" class="headerlink" title="偏差和方差分析"></a>偏差和方差分析</h1><p><img src="https://img-blog.csdnimg.cn/20190325231703314.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>上图为机器学习中偏差和方差的四种不同情况。每个图的中心点为最优模型f*(x)，蓝点为不同训练集D 上得到的模型f_D(x)。</p><ul><li>(a)给出了一种理想情况，方差和偏差都比较小</li><li>(b)为高偏差低方差的情况，表示模型的泛化能力很好，但拟合能力不足</li><li>(c)为低偏差高方差的情况，表示模型的拟合能力很好，但泛化能力比较差。当训练数据比较少时会导致过拟合</li><li>(d)为高偏差高方差的情况，是一种最差的情况</li></ul><p>方差一般会随着训练样本的增加而减少。当样本比较多时，方差比较少，我们可以选择能力强的模型来减少偏差。然而在很多机器学习任务上，训练集上往往都比较有限，最优的偏差和最优的方差就无法兼顾。</p><p>随着模型复杂度的增加，模型的拟合能力变强，偏差减少而方差增大，从而导致过拟合。以结构错误最小化为例，我们可以调整正则化系数λ来控制模型的复杂度。当λ变大时，模型复杂度会降低，可以有效地减少方差，避免过拟合，但偏差会上升。当λ过大时，总的期望错误反而会上升。因此，正则化系数λ需要在偏差和方差之间取得比较好的平衡。下图给出了机器学习模型的期望错误、偏差和方差随复杂度的变化情况。最优的模型并不一定是偏差曲线和方差曲线的交点。</p><p><img src="https://img-blog.csdnimg.cn/20190325231720819.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>偏差和方差分解给机器学习模型提供了一种分析途径，但在实际操作中难以直接衡量。一般来说，当一个模型在训练集上的错误率比较高时，说明模型的拟合能力不够，偏差比较高。这种情况可以增加数据特征、提高模型复杂度，减少正则化系数等操作来改进模型。当模型在训练集上的错误率比较低，但验证集上的错误率比较高时，说明模型过拟合，方差比较高。这种情况可以通过降低模型复杂度，加大正则化系数，引入先验等方法来缓解。此外，还有一种有效的降低方差的方法为集成模型，即通过多个高方差模型的平均来降低方差。 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="线性模型" scheme="http://thinkgamer.cn/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="线性回归" scheme="http://thinkgamer.cn/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="方差" scheme="http://thinkgamer.cn/tags/%E6%96%B9%E5%B7%AE/"/>
    
      <category term="偏差" scheme="http://thinkgamer.cn/tags/%E5%81%8F%E5%B7%AE/"/>
    
  </entry>
  
  <entry>
    <title>基于神经网络实现Mnist数据集的多分类</title>
    <link href="http://thinkgamer.cn/2019/03/09/TensorFlow/%E5%9F%BA%E4%BA%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0Mnist%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%A4%9A%E5%88%86%E7%B1%BB/"/>
    <id>http://thinkgamer.cn/2019/03/09/TensorFlow/基于神经网络实现Mnist数据集的多分类/</id>
    <published>2019-03-09T13:19:08.000Z</published>
    <updated>2019-04-13T05:56:37.338Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><blockquote><p>在之前的文章中介绍了基于Logistic Regression实现Mnist数据集的多分类，本篇文章主要介绍基于TensorFlow实现Mnist数据集的多分类。</p></blockquote><p>一个典型的神经网络训练图如下所示：<br><img src="https://img-blog.csdnimg.cn/20190308005540655.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><p>只不过在Mnist数据集是十分类的，起输出由y1和y2换成y1，….，y10。本文实现的神经网络如下所示：</p><p>这是使用的是两层的神经网络，第一层神经元个数是256，第二层为128，最终输出的是10个类别。对应的神经网络结果如下图所示：<br><img src="https://img-blog.csdnimg.cn/20190309205947261.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><p>接着我们创建一个MutilClass类，并初始化相关参数用来实现基于神经网络的多分类函数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"> </span><br><span class="line">class MutilClass:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        # 加载数据集</span><br><span class="line">        self.Mnsit = input_data.read_data_sets(&quot;./data/&quot;, one_hot=True)</span><br><span class="line"> </span><br><span class="line">        # 设置神经网络层参数</span><br><span class="line">        self.n_hidden_1 = 256</span><br><span class="line">        self.n_hidden_2 = 128</span><br><span class="line">        self.n_input = 784</span><br><span class="line">        self.n_classes = 10</span><br><span class="line"> </span><br><span class="line">        self.x = tf.placeholder(dtype=float, shape=[None, self.n_input], name=&quot;x&quot;)</span><br><span class="line">        self.y = tf.placeholder(dtype=float, shape=[None, self.n_classes], name=&quot;y&quot;)</span><br><span class="line">        # random_normal 高斯分布初始化权重</span><br><span class="line">        self.weights = &#123;</span><br><span class="line">            &quot;w1&quot;: tf.Variable(tf.random_normal([self.n_input, self.n_hidden_1],stddev = 0.1)),</span><br><span class="line">            &quot;w2&quot;: tf.Variable(tf.random_normal([self.n_hidden_1, self.n_hidden_2], stddev = 0.1)),</span><br><span class="line">            &quot;out&quot;: tf.Variable(tf.random_normal([self.n_hidden_2, self.n_classes], stddev = 0.1))</span><br><span class="line">        &#125;</span><br><span class="line">        self.bias = &#123;</span><br><span class="line">            &quot;b1&quot;: tf.Variable(tf.random_normal([ self.n_hidden_1 ])),</span><br><span class="line">            &quot;b2&quot;: tf.Variable(tf.random_normal([ self.n_hidden_2 ])),</span><br><span class="line">            &quot;out&quot;: tf.Variable(tf.random_normal([ self.n_classes ]))</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        print(&quot;参数初始化完成！&quot;)</span><br></pre></td></tr></table></figure><p>神经网络首次循环，是根据初始化的参数和偏置，向前传播，经过两层隐层，最终的到一个对应各个类别的概率，然后再根据反向传播，最小化损失函数求解参数，所以这里创建一个前向传播和反向传播的函数，如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 定义一个MLP，前向感知器</span><br><span class="line">def _multilayer_perceptron(self,_X, _weights, _bias):</span><br><span class="line">    layer_1 = tf.nn.sigmoid(tf.add ( tf.matmul(_X, _weights[&quot;w1&quot;]), _bias[&quot;b1&quot;] ) )</span><br><span class="line">    layer_2 = tf.nn.sigmoid(tf.add ( tf.matmul(layer_1, _weights[&quot;w2&quot;]), _bias[&quot;b2&quot;] ) )</span><br><span class="line">    return (tf.matmul( layer_2 ,_weights[&quot;out&quot;] ) + _bias[&quot;out&quot;])</span><br><span class="line"> </span><br><span class="line"># 定义反向传播</span><br><span class="line">def _back_propagation(self):</span><br><span class="line">    pred = self._multilayer_perceptron(self.x, self.weights, self.bias)</span><br><span class="line">    # logits 未归一化的概率</span><br><span class="line">    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=self.y) )</span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer( learning_rate= 0.001).minimize(cost)</span><br><span class="line">    corr = tf.equal(tf.argmax(pred, 1), tf.argmax(self.y, 1) )</span><br><span class="line">    accr =tf.reduce_mean(tf.cast(corr, dtype=float))</span><br><span class="line"> </span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    return init, optimizer,cost, accr</span><br></pre></td></tr></table></figure></p><p>接着就是训练网络了，指定的迭代次数为：100，batch_size：100，其对应的函数未：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># 训练模型</span><br><span class="line">def _train_model(self, _init, _optimizer, _cost, _accr):</span><br><span class="line">    epochs = 100</span><br><span class="line">    batch_size = 100</span><br><span class="line">    display_steps = 1</span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    sess.run(_init)</span><br><span class="line"> </span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        avg_cost = 0</span><br><span class="line">        total_batch = int (self.Mnsit.train.num_examples / batch_size)</span><br><span class="line">        for i in range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = self.Mnsit.train.next_batch(batch_size)</span><br><span class="line">            feeds = &#123;self.x: batch_xs, self.y: batch_ys&#125;</span><br><span class="line">            sess.run(_optimizer, feed_dict=feeds)</span><br><span class="line">            avg_cost += sess.run(_cost, feed_dict=feeds)</span><br><span class="line">        avg_cost = avg_cost / total_batch</span><br><span class="line"> </span><br><span class="line">        if (epoch +1) % display_steps ==0:</span><br><span class="line">            print(&quot;Epoch: &#123;&#125; / &#123;&#125;, cost: &#123;&#125;&quot;.format(epoch, epochs, avg_cost))</span><br><span class="line">            feeds = &#123;self.x: batch_xs, self.y: batch_ys&#125;</span><br><span class="line">            train_acc = sess.run(_accr, feed_dict=feeds)</span><br><span class="line">            print(&quot;Train Accuracy: &#123;&#125;&quot;.format(train_acc))</span><br><span class="line"> </span><br><span class="line">            feeds = &#123;self.x : self.Mnsit.test.images, self.y: self.Mnsit.test.labels&#125;</span><br><span class="line">            test_acc = sess.run(_accr, feed_dict= feeds)</span><br><span class="line">            print(&quot;Test Accuracy: &#123;&#125;&quot;.format(test_acc))</span><br><span class="line">            print(&quot;-&quot; * 50)</span><br></pre></td></tr></table></figure><p>创建主函数，进行迭代训练<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    network = MutilClass()</span><br><span class="line">    init, optimizer, cost, accr = network._back_propagation()</span><br><span class="line">    network._train_model(init, optimizer,cost, accr)</span><br></pre></td></tr></table></figure></p><p>最后的迭代结果为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 0 / 100, cost: 2.4407591546665537</span><br><span class="line">Train Accuracy: 0.12999999523162842</span><br><span class="line">Test Accuracy: 0.12960000336170197</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Epoch: 1 / 100, cost: 2.290777679356662</span><br><span class="line">Train Accuracy: 0.12999999523162842</span><br><span class="line">Test Accuracy: 0.1469999998807907</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Epoch: 2 / 100, cost: 2.2774649468335237</span><br><span class="line">Train Accuracy: 0.17000000178813934</span><br><span class="line">Test Accuracy: 0.21799999475479126</span><br><span class="line">--------------------------------------------------</span><br><span class="line"> </span><br><span class="line">.......</span><br><span class="line"> </span><br><span class="line">--------------------------------------------------</span><br><span class="line">Epoch: 98 / 100, cost: 0.7186844098567963</span><br><span class="line">Train Accuracy: 0.8299999833106995</span><br><span class="line">Test Accuracy: 0.8371999859809875</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Epoch: 99 / 100, cost: 0.7124480505423112</span><br><span class="line">Train Accuracy: 0.8100000023841858</span><br><span class="line">Test Accuracy: 0.8377000093460083</span><br><span class="line">--------------------------------------------------</span><br></pre></td></tr></table></figure></p><p>从结果中可以看出，cost是一直在减少，训练集和测试集评估模型的准确率也在一直提高。当然我们也可以通过调节epoch，batch_size来重新训练模型。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="深度学习" scheme="http://thinkgamer.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="TensorFlow" scheme="http://thinkgamer.cn/tags/TensorFlow/"/>
    
      <category term="LR" scheme="http://thinkgamer.cn/tags/LR/"/>
    
  </entry>
  
  <entry>
    <title>机器学习在微博信息流推荐中的应用实践</title>
    <link href="http://thinkgamer.cn/2019/03/05/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9C%A8%E5%BE%AE%E5%8D%9A%E4%BF%A1%E6%81%AF%E6%B5%81%E6%8E%A8%E8%8D%90%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5/"/>
    <id>http://thinkgamer.cn/2019/03/05/推荐系统/机器学习在微博信息流推荐中的应用实践/</id>
    <published>2019-03-05T00:03:35.000Z</published>
    <updated>2019-04-13T05:53:28.074Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><p>本文分为四部分介绍机器学习在微博信息流中的应用实践，分别为：微博信息流推荐场景介绍，内容理解与用户画像，大规模推荐系统实践和总结展望。</p><h1 id="微博信息流推荐场景介绍"><a href="#微博信息流推荐场景介绍" class="headerlink" title="微博信息流推荐场景介绍"></a>微博信息流推荐场景介绍</h1><blockquote><p>微博的feed流内容形态各异，有视频，图片，文字，长文，问答等，其用户量也很大，2018年Q2统计DAU（日活）为1.9亿，MAU（月活）为4.3亿，这么庞大的用户量，如何做好首页feed流的个性化推荐就显得格外重要。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/20190305162718873.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305162740161.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><h1 id="内容理解与用户画像"><a href="#内容理解与用户画像" class="headerlink" title="内容理解与用户画像"></a>内容理解与用户画像</h1><blockquote><p>由于个性化推荐是给用户推荐其感兴趣的内容，所以对于微博的内容理解和用户画像部分就显得格外重要。内容理解即通过文本内容理解和视觉理解技术，对微博内容进行细粒度表征，即形成每篇微博内容的表征向量。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/20190305162850198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305162905934.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><blockquote><p>用户画像即基于用户的发博内容，行为数据，自填信息等进行深度挖掘，精准分析刻画用户，从而在进行微博内容推送时能够实现其个性化。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/20190305162934416.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><h1 id="大规模推荐系统实践"><a href="#大规模推荐系统实践" class="headerlink" title="大规模推荐系统实践"></a>大规模推荐系统实践</h1><blockquote><p>目前推荐架构的实现思路都是先从海量原始数据中，依据用户画像，召回用户偏好的数据，在利用排序算法对其进行排序，最终选择top K返回给用户。微博推荐亦是如此。其整体的流程图如下所示：</p><p>物料召回：即从候选物料集合中粗筛物料，作为进行模型的待排序物料。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/20190305163036955.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><p><img src="https://img-blog.csdnimg.cn/20190305163059488.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><p><img src="https://img-blog.csdnimg.cn/20190305163120489.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><blockquote><p>算法排序则是结合相关特征对物料召回的内容进行预估排序，其特征主要分为：用户特征，内容特征，环境特征，组合特征和上下文特征等。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/20190305163427457.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163455977.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163515432.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163530356.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163645420.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163705966.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163719819.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163749462.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163805882.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163818878.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163833250.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><h1 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a>总结与展望</h1><ul><li><p>总结</p><ul><li>业务和数据决定了模型算法的应用场景</li><li>模型算法殊途同归</li><li>工程能力和算法架构是基本保障</li></ul></li><li><p>展望</p><ul><li>采用多模型融合，能更好的对非结构化内容进行表征</li><li>更多的融合网络结构适用于CTR预估场景</li></ul></li></ul><hr><p>完整内容请阅读原文：<a href="https://blog.csdn.net/Gamer_gyt/article/details/88164127" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/88164127</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="深度学习" scheme="http://thinkgamer.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="推荐系统" scheme="http://thinkgamer.cn/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="机器学习" scheme="http://thinkgamer.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow实现Mnist数据集的多分类逻辑回归模型</title>
    <link href="http://thinkgamer.cn/2019/02/27/TensorFlow/TensorFlow%E5%AE%9E%E7%8E%B0Mnist%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%A4%9A%E5%88%86%E7%B1%BB%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
    <id>http://thinkgamer.cn/2019/02/27/TensorFlow/TensorFlow实现Mnist数据集的多分类逻辑回归模型/</id>
    <published>2019-02-27T05:15:55.000Z</published>
    <updated>2019-04-13T05:46:04.119Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><blockquote><p>多分类逻辑回归基于逻辑回归（Logistic Regression，LR）和softMax实现，其在多分类分类任务中应用广泛，本篇文章基于tf实现多分类逻辑回归，使用的数据集为Mnist。</p></blockquote><p>多分类逻辑回归的基础概要和在Spark上的实现可参考：</p><ul><li>多分类逻辑回归（Multinomial Logistic Regression）</li><li>多分类实现方式介绍和在Spark上实现多分类逻辑回归（Multinomial Logistic Regression）</li></ul><p>本篇文章涉及到的tf相关接口函数及释义如下：</p><h2 id="tf-nn-softmax"><a href="#tf-nn-softmax" class="headerlink" title="tf.nn.softmax"></a>tf.nn.softmax</h2><p>Softmax 在机器学习和深度学习中有着非常广泛的应用。尤其在处理多分类（C &gt; 2）问题，分类器最后的输出单元需要Softmax 函数进行数值处理。关于Softmax 函数的定义如下所示：</p><p>…</p><hr><p>完整内容请阅读原文：<a href="https://blog.csdn.net/Gamer_gyt/article/details/87970776" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/87970776</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="深度学习" scheme="http://thinkgamer.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="TensorFlow" scheme="http://thinkgamer.cn/tags/TensorFlow/"/>
    
      <category term="LR" scheme="http://thinkgamer.cn/tags/LR/"/>
    
  </entry>
  
  <entry>
    <title>深度学习中的epochs、batch_size、iterations详解</title>
    <link href="http://thinkgamer.cn/2019/02/26/TensorFlow/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84epochs%E3%80%81batch_size%E3%80%81iterations%E8%AF%A6%E8%A7%A3/"/>
    <id>http://thinkgamer.cn/2019/02/26/TensorFlow/深度学习中的epochs、batch_size、iterations详解/</id>
    <published>2019-02-25T16:32:29.000Z</published>
    <updated>2019-04-13T05:38:21.475Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><blockquote><p>深度学习中涉及到很多参数，如果对于一些参数不了解，那么去看任何一个框架都会有难度，在TensorFlow中有一些模型训练的基本参数，这些参数是训练模型的前提，也在一定程度上影响着模型的最终效果。下面主要介绍几个参数。</p></blockquote><ul><li>batch_size</li><li>iterations</li><li>epochs</li></ul><h1 id="batch-size"><a href="#batch-size" class="headerlink" title="batch_size"></a>batch_size</h1><p>深度学习的优化算法，其实就是梯度下降，在之前的文章中我们也介绍过梯度下降，这里就不详细说明。梯度下降分为三种：</p><ul><li>批量梯度下降算法（BGD，Batch gradient descent algorithm）</li><li>随机梯度下降算法（SGD，Stochastic gradient descent algorithm）</li><li>小批量梯度下降算法（MBGD，Mini-batch gradient descent algorithm）</li></ul><p>批量梯度下降算法，每一次计算都需要遍历全部数据集，更新梯度，计算开销大，花费时间长，不支持在线学习。</p><p>随机梯度下降算法，每次随机选取一条数据，求梯度更新参数，这种方法计算速度快，但是收敛性能不太好，可能在最优点附近晃来晃去，hit不到最优点。两次参数的更新也有可能互相抵消掉，造成目标函数震荡的比较剧烈。</p><p>为了克服两种方法的缺点，现在一般采用的是一种折中手段，mini-batch gradient decent，小批的梯度下降，这种方法把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也不是很大。</p><p>tf框架中的batch_size指的就是更新梯度中使用的样本数。当然这里如果把batch_size设置为数据集的长度，就成了批量梯度下降算法，batch_size设置为1就是随机梯度下降算法。</p><h1 id="iterations"><a href="#iterations" class="headerlink" title="iterations"></a>iterations</h1><p>迭代次数，每次迭代更新一次网络结构的参数。</p><p>迭代是重复反馈的动作，神经网络中我们希望通过迭代进行多次的训练以到达所需的目标或结果。</p><p>每一次迭代得到的结果都会被作为下一次迭代的初始值。</p><p>一个迭代 = 一个（batch_size）数据正向通过（forward）+ 一个（batch_size）数据反向（backward）</p><p><img src="https://img-blog.csdnimg.cn/2019022523170956.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="神经网络"></p><p>前向传播：构建由（x1,x2,x3）得到Y（hwb(x)）的表达式</p><p>反向传播：基于给定的损失函数，求解参数的过程</p><h1 id="epochs"><a href="#epochs" class="headerlink" title="epochs"></a>epochs</h1><p>epochs被定义为前向和反向传播中所有批次的单次训练迭代。这意味着1个周期是整个输入数据的单次前向和反向传递。</p><p>简单说，epochs指的就是训练过程中数据将被“轮”多少次</p><p>例如在某次模型训练过程中，总的样本数是10000，batch_size=100，epochs=10，其对应的伪代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data = </span><br><span class="line">batch_size = 100</span><br><span class="line">for i in range(epochs):</span><br><span class="line">    for j in range(int(data_length / batch_size - 1)):</span><br><span class="line">        x_data = data[begin:end, ]</span><br><span class="line">        y_data = data[begin:end, ]</span><br><span class="line">        mode.train(x_data, y_data)</span><br><span class="line">        begin += batch_size</span><br><span class="line">        end += batch_size</span><br></pre></td></tr></table></figure></p><p>其中iterations = data_length / batch_size</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="深度学习" scheme="http://thinkgamer.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="tensorFlow" scheme="http://thinkgamer.cn/tags/tensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Spark排序算法系列之（MLLib、ML）GBTs使用方式介绍</title>
    <link href="http://thinkgamer.cn/2019/01/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Spark%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%E4%B9%8B%EF%BC%88MLLib%E3%80%81ML%EF%BC%89GBTs%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F%E4%BB%8B%E7%BB%8D/"/>
    <id>http://thinkgamer.cn/2019/01/29/机器学习/Spark排序算法系列之（MLLib、ML）GBTs使用方式介绍/</id>
    <published>2019-01-29T13:16:35.000Z</published>
    <updated>2019-04-13T05:43:53.236Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>【Spark排序算法系列】主要介绍的是目前推荐系统或者广告点击方面用的比较广的几种算法，和他们在Spark中的应用实现，本篇文章主要介绍GBDT算法，本系列还包括（持续更新）：</p><ul><li>Spark排序算法系列之LR（逻辑回归）</li><li>Spark排序算法系列之模型融合（GBDT+LR）</li><li>Spark排序算法系列之XGBoost</li><li>Spark排序算法系列之FTRL（Follow-the-regularized-Leader）</li><li>Spark排序算法系列之FM与FFM</li></ul><p>在本篇文章中你可以学到：</p><ul><li>Spark MLLib包中的GBDT使用方式</li><li>模型的通过保存、加载、预测</li><li>PipeLine</li><li>ML包中的GBDT</li></ul><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>LR因为其容易并行最早应用到推荐排序中的，但学习能力有限，需要大量的特征工程来增加模型的学习能力。但大量的特征工程耗时耗力，且不一定带来效果的提升，因此在如何能有效的发现特征组合，来缩短LR特征实验周期的背景下，GBDT被应用了起来。GBDT模型全称是Gradient Boosting Decision Tree，梯度提升决策树。是属于Boosing算法中的一种，关于Boosting的介绍可以参考文章集成学习（Ensemble Learning)</p><p>关于GBDT算法理解可参考：</p><ul><li>Spark排序算法系列之GBTs基础——梯度上升和梯度下降</li><li>梯度提升决策树GBDT（Gradient Boosting Decision Tree）</li></ul><p>其实相信很多人对Spark 机器学习包（ml和mllib）中的GBDT傻傻分不清楚，这里我们先来捋一捋。Spark中的GBDT较GBTs——梯度提升树，因为其是基于决策树（Decision Tree，DT）实现的，所以叫GBDT。Spark 中的GBDT算法存在于ml包和mllib包中，mllib是基于RDD的，ml包则是针对DataFrame的，ml包中的GBDT分为分类和回归，在实际使用过程中，需要根据具体情况进行衡量和选择。由于在实际生产环境中使用基于RDD的较多，所以下面将着重介绍下MLLib包中的GBTs，ML包中的将进行简单说明。</p><hr><p>完整内容请阅读原文：<a href="https://blog.csdn.net/Gamer_gyt/article/details/86695837" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/86695837</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="Spark" scheme="http://thinkgamer.cn/tags/Spark/"/>
    
      <category term="机器学习" scheme="http://thinkgamer.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>资源分享：从数理统计到DL、RL，还不快来！</title>
    <link href="http://thinkgamer.cn/2019/01/28/NLP/%E8%B5%84%E6%BA%90%E5%88%86%E4%BA%AB%EF%BC%9A%E4%BB%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E5%88%B0DL%E3%80%81RL%EF%BC%8C%E8%BF%98%E4%B8%8D%E5%BF%AB%E6%9D%A5/"/>
    <id>http://thinkgamer.cn/2019/01/28/NLP/资源分享：从数理统计到DL、RL，还不快来/</id>
    <published>2019-01-27T19:16:19.000Z</published>
    <updated>2019-04-13T05:41:18.266Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><p>之前在自己的年度总结里写到：19年的目标就是技术沉淀与突破。技术突破不仅包含现有技术的总结和反思，更是对未知技术的探索和求知，希望19年能够更上一层楼。</p><p>这个repo是我一直维护和整理的一个技术资料分享的repo，是我包括群友一块整理的一个免费技术资料分享的库，不仅包含了机器学习，数据挖掘，深度学习，还包含了大数据，数理统计，强化学习等，希望在技术这条路上你能跑的更快。</p><p>repo：<a href="https://github.com/Thinkgamer/books" target="_blank" rel="external">https://github.com/Thinkgamer/books</a></p><p>先来张图片镇楼！！！<br><img src="https://img-blog.csdnimg.cn/20190128031446639.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==,size_16,color_FFFFFF,t_70"></p><h1 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h1><p>建立该Repo的目的有两个：</p><ul><li>本人在各个平台共享书籍，进行一个统一管理</li><li>分享给各个搞技术的朋友，知识无私藏之说</li></ul><hr><h1 id="What"><a href="#What" class="headerlink" title="What"></a>What</h1><p>该Repo会涉及包含以下类别书籍：</p><ul><li>机器学习</li><li>数据挖掘</li><li>深度学习</li><li>NLP</li><li>云计算</li><li>统计学概率论</li><li>收藏的论文</li><li>杂乱无章</li></ul><hr><h1 id="List"><a href="#List" class="headerlink" title="List"></a>List</h1><p>注明：排名无先后顺序</p><h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><ul><li>scikit-learn 中英文</li><li>机器学习-周志华</li><li>机器学习实战</li><li>机器学习导论</li><li>集体智慧编程中文版</li><li>[英文版]叶斯思维：统计建模的Python学习法</li></ul><h2 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h2><ul><li>python数据分析与挖掘实战</li><li>利用python进行数据分析</li><li>面向程序员的数据挖掘指南</li><li>数据挖掘：概念与技术（中文第三版）</li><li>数据挖掘应用20个案例分析</li><li>数据挖掘与数据化运营实战_思路_方法_技巧与应用_完整版</li></ul><h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><ul><li>神经网络与机器学习（加）Simon Haykin</li><li>TensorFlow实战-黄文坚</li><li>深度学习 中文版</li><li>神经网络与深度学习</li></ul><h2 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h2><ul><li>模式识别与机器学习 中文版</li><li>NLP汉语自然语言处理原理与实践</li><li>PYTHON自然语言处理中文版</li></ul><h2 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h2><ul><li>推荐系统实践</li><li>learning-to-rank.pdf </li><li>Recommender Systems Handbook.pdf</li><li>Context-Aware-Recommender-Systems-chapter-7.pdf</li></ul><h2 id="云计算"><a href="#云计算" class="headerlink" title="云计算"></a>云计算</h2><ul><li>《快学Scala》</li><li>Learning PySpark.pdf</li><li>SparkMLlib机器学习</li><li>Spark快速大数据分析</li><li>数据算法  Hadoop Spark大数据处理技巧</li></ul><h2 id="统计学与概率论"><a href="#统计学与概率论" class="headerlink" title="统计学与概率论"></a>统计学与概率论</h2><ul><li>《概率论与数理统计》浙大版（第四版）教材</li><li>《概率论与数理统计习题全解指南》.浙大版（第四版）</li><li>数理统计与数据分析原书第3版</li><li>应用商务统计分析 王汉生(2008).pdf</li></ul><h2 id="收藏的论文"><a href="#收藏的论文" class="headerlink" title="收藏的论文"></a>收藏的论文</h2><ul><li>平滑系数自适应的二次指数平滑模型及其应用</li><li>The Structure of Collaborative Tagging Systems</li><li>FM</li><li>FFM</li><li>DeepFFM</li><li>Focal Loss for Dense Object Detection</li><li>Attentive Group Recommendation.pdf</li><li>Real-time Personalization using Embeddings for Search.pdf</li></ul><h2 id="杂乱无章"><a href="#杂乱无章" class="headerlink" title="杂乱无章"></a>杂乱无章</h2><ul><li>阿里技术之瞳-p260</li><li>数据敏感性测试</li><li>正则表达式经典实例.（美）高瓦特斯，（美）利维森</li><li>阿里广告中的机器学习平台.pdf</li><li>广告数据上的大规模机器学习.pdf</li><li>绿盟大数据安全分析平台 产品白皮书.pdf</li><li>Xdef2013-基于机器学习和NetFPGA的智能高速入侵防御系统.ppt</li><li>04-程佳-推荐广告机器学习实践</li><li>A Gentle Introduction to Gradient Boosting.pdf</li><li>GBDT算法原理与系统设计简介.pdf</li><li>[微博] 机器学习在微博信息流推荐应用实践.pdf</li><li>[知乎] 首页信息流系统的框架及机器学习技术在推荐策略中的应用.pdf</li></ul><h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><ul><li>强化学习在阿里的技术演进与业务创新.pdf</li></ul><h2 id="技术集锦"><a href="#技术集锦" class="headerlink" title="技术集锦"></a>技术集锦</h2><ul><li>AAAI2018.pdf</li><li>数字经济下的算法力量.pdf</li><li>2018美团点评算法系列.pdf</li><li>Learning To Rank在个性化电商搜索中的应用</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="资源" scheme="http://thinkgamer.cn/tags/%E8%B5%84%E6%BA%90/"/>
    
      <category term="DL" scheme="http://thinkgamer.cn/tags/DL/"/>
    
      <category term="RL" scheme="http://thinkgamer.cn/tags/RL/"/>
    
      <category term="ML" scheme="http://thinkgamer.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>多分类实现方式介绍和在Spark上实现多分类逻辑回归</title>
    <link href="http://thinkgamer.cn/2019/01/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%88%86%E7%B1%BB%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%9C%A8Spark%E4%B8%8A%E5%AE%9E%E7%8E%B0%E5%A4%9A%E5%88%86%E7%B1%BB%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://thinkgamer.cn/2019/01/12/机器学习/多分类实现方式介绍和在Spark上实现多分类逻辑回归/</id>
    <published>2019-01-12T14:06:02.000Z</published>
    <updated>2019-04-13T05:38:13.219Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>在之前的文章中介绍了多分类逻辑回归算法的数据原理，参考文章链接</p><p>CSDN文章链接：<a href="https://blog.csdn.net/Gamer_gyt/article/details/85209496" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/85209496</a><br>公众号：多分类逻辑回归（Multinomial Logistic Regression）<br>该篇文章介绍一下Spark中多分类算法，主要包括的技术点如下</p><ul><li><p>多分类实现方式</p><ul><li>一对一 （One V One）</li><li>一对其余（One V Remaining）</li><li>多对多 （More V More）</li></ul></li><li><p>Spark中的多分类实现</p><h1 id="多分类实现方式"><a href="#多分类实现方式" class="headerlink" title="多分类实现方式"></a>多分类实现方式</h1><h2 id="一对一"><a href="#一对一" class="headerlink" title="一对一"></a>一对一</h2><p>假设某个分类中有N个类别，将这N个类别两两配对（继而转化为二分类问题），这样可以得到 N（N-1）/ 2个二分类器，这样训练模型时需要训练 N（N-1）/ 2个模型，预测时将样本输送到这些模型中，最终统计出现次数较多的类别结果作为最终类别。</p></li></ul><p>假设现在有三个类别：类别A，类别B，类别C，类别D。一对一实现多分类如下图所示：<br><img src="https://img-blog.csdnimg.cn/20190112220044121.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==,size_16,color_FFFFFF,t_70"></p><hr><p>完整内容请阅读原文：<a href="https://blog.csdn.net/Gamer_gyt/article/details/86378882" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/86378882</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="Spark" scheme="http://thinkgamer.cn/tags/Spark/"/>
    
      <category term="LR" scheme="http://thinkgamer.cn/tags/LR/"/>
    
      <category term="机器学习" scheme="http://thinkgamer.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Hive Join 分析和优化</title>
    <link href="http://thinkgamer.cn/2019/01/03/Spark/Hive%20Join%20%E5%88%86%E6%9E%90%E5%92%8C%E4%BC%98%E5%8C%96/"/>
    <id>http://thinkgamer.cn/2019/01/03/Spark/Hive Join 分析和优化/</id>
    <published>2019-01-03T05:34:48.000Z</published>
    <updated>2019-04-13T05:33:15.758Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>Sku对应品牌进行关联，大表对应非大表（这里的非大表并不能用小表来定义）</p><h1 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h1><p>进行表左关联时，最后一个reduce任务卡到99%，运行时间很长，发生了严重的数据倾斜。</p><p>什么是数据倾斜？数据倾斜主要表现在，map /reduce程序执行时，reduce节点大部分执行完毕，但是有一个或者几个reduce节点运行很慢，导致整个程序的处理时间很长，这是因为某一个key的条数比其他key多很多（有时是百倍或者千倍之多），这条key所在的reduce节点所处理的数据量比其他节点就大很多，从而导致某几个节点迟迟运行不完。</p><p>完整内容请阅读原文：<a href="https://blog.csdn.net/Gamer_gyt/article/details/85690885" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/85690885</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="Spark" scheme="http://thinkgamer.cn/tags/Spark/"/>
    
      <category term="Hive" scheme="http://thinkgamer.cn/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 特征工程:feature_column</title>
    <link href="http://thinkgamer.cn/2019/01/03/TensorFlow/TensorFlow%20%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B:%20feature_column/"/>
    <id>http://thinkgamer.cn/2019/01/03/TensorFlow/TensorFlow 特征工程: feature_column/</id>
    <published>2019-01-03T05:18:16.000Z</published>
    <updated>2019-04-13T05:28:32.930Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="特征工程-feature-column"><a href="#特征工程-feature-column" class="headerlink" title="特征工程: feature_column"></a>特征工程: feature_column</h1><p>在使用很多模型的时候，都需要对输入的数据进行必要的特征工程处理。最典型的就是:one-hot处理，还有hash分桶等处理。为了方便处理这些特征，tensorflow提供了一些列的特征工程方法来方便使用.</p><h2 id="公共的import"><a href="#公共的import" class="headerlink" title="公共的import"></a>公共的import</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.python.estimator.inputs import numpy_io</span><br><span class="line">import numpy as np</span><br><span class="line">import collections</span><br><span class="line">from tensorflow.python.framework import errors</span><br><span class="line">from tensorflow.python.platform import test</span><br><span class="line">from tensorflow.python.training import coordinator</span><br><span class="line">from tensorflow import feature_column</span><br><span class="line"></span><br><span class="line">from tensorflow.python.feature_column.feature_column import _LazyBuilder</span><br></pre></td></tr></table></figure><h2 id="numeric-column"><a href="#numeric-column" class="headerlink" title="numeric_column"></a>numeric_column</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">numeric_column(</span><br><span class="line">    key,</span><br><span class="line">    shape=(1,),</span><br><span class="line">    default_value=None,</span><br><span class="line">    dtype=tf.float32,</span><br><span class="line">    normalizer_fn=None</span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>key：特征的名字。也就是对应的列名称</li><li>shape：该key所对应的特征的shape. 默认是1，但是比如one-hot类型的，shape就不是1，而是实际的维度。总之，这里是key所对应的维度，不一定是1</li><li>default_value：如果不存在使用的默认值</li><li>normalizer_fn：对该特征下的所有数据进行转换。如果需要进行normalize，那么就是使用normalize的函数.这里不仅仅局限于normalize，也可以是任何的转换方法，比如取对数，取指数，这仅仅是一种变换方法</li></ul><p>完整内容请阅读：<a href="https://blog.csdn.net/Gamer_gyt/article/details/85689840" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/85689840</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="深度学习" scheme="http://thinkgamer.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="tensorFlow" scheme="http://thinkgamer.cn/tags/tensorFlow/"/>
    
      <category term="特征过程" scheme="http://thinkgamer.cn/tags/%E7%89%B9%E5%BE%81%E8%BF%87%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>集成学习（Ensemble Learning)</title>
    <link href="http://thinkgamer.cn/2019/01/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%EF%BC%88Ensemble%20Learning)/"/>
    <id>http://thinkgamer.cn/2019/01/03/机器学习/集成学习（Ensemble Learning)/</id>
    <published>2019-01-03T05:14:38.000Z</published>
    <updated>2019-04-13T05:24:56.867Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><blockquote><p>在机器学习的有监督学习算法中，我们的目标是学习出一个稳定的且在各个方面表现都较好的模型，但实际情况往往不这么理想，有时我们只能得到多个有偏好的模型（弱监督模型，在某些方面表现的比较好）。集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。</p></blockquote><p>集成学习在各个规模的数据集上都有很好的策略。</p><ul><li>数据集大：划分成多个小数据集，学习多个模型进行组合</li><li>数据集小：利用Bootstrap方法进行抽样，得到多个数据集，分别训练多个模型再进行组合</li></ul><p>这篇博客介绍一下集成学习的几类：Bagging，Boosting以及Stacking。</p><h1 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h1><p>Bagging是bootstrap aggregating的简写。先说一下bootstrap，bootstrap也称为自助法，它是一种有放回的抽样方法，目的为了得到统计量的分布以及置信区间。具体步骤如下</p><ul><li>采用重抽样方法（有放回抽样）从原始样本中抽取一定数量的样本</li><li>根据抽出的样本计算想要得到的统计量T</li><li>重复上述N次（一般大于1000），得到N个统计量T</li><li>根据这N个统计量，即可计算出统计量的置信区间</li></ul><p>在Bagging方法中，利用bootstrap方法从整体数据集中采取有放回抽样得到N个数据集，在每个数据集上学习出一个模型，最后的预测结果利用N个模型的输出得到，具体地：分类问题采用N个模型预测投票的方式，回归问题采用N个模型预测平均的方式。</p><p>例如随机森林（Random Forest）就属于Bagging。随机森林简单地来说就是用随机的方式建立一个森林，森林由很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。</p><p>在我们学习每一棵决策树的时候就需要用到Bootstrap方法。在随机森林中，有两个随机采样的过程：对输入数据的行（数据的数量）与列（数据的特征）都进行采样。对于行采样，采用有放回的方式，若有N个数据，则采样出N个数据（可能有重复），这样在训练的时候每一棵树都不是全部的样本，相对而言不容易出现overfitting；接着进行列采样从M个feature中选择出m个（m&lt;&lt;M）。最近进行决策树的学习。</p><p>预测的时候，随机森林中的每一棵树的都对输入进行预测，最后进行投票，哪个类别多，输入样本就属于哪个类别。这就相当于前面说的，每一个分类器（每一棵树）都比较弱，但组合到一起（投票）就比较强了。</p><h1 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h1><p>提升方法（Boosting）是一种可以用来减小监督学习中偏差的机器学习算法。主要也是学习一系列弱分类器，并将其组合为一个强分类器。Boosting中有代表性的是AdaBoost（Adaptive boosting）算法：刚开始训练时对每一个训练例赋相等的权重，然后用该算法对训练集训练t轮，每次训练后，对训练失败的训练例赋以较大的权重，也就是让学习算法在每次学习以后更注意学错的样本，从而得到多个预测函数。具体可以参考《统计学习方法》。</p><h1 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h1><p>Stacking方法是指训练一个模型用于组合其他各个模型。首先我们先训练多个不同的模型，然后把之前训练的各个模型的输出为输入来训练一个模型，以得到一个最终的输出。理论上，Stacking可以表示上面提到的两种Ensemble方法，只要我们采用合适的模型组合策略即可。但在实际中，我们通常使用logistic回归作为组合策略。</p><p>如下图，先在整个训练数据集上通过bootstrap抽样得到各个训练集合，得到一系列分类模型，称之为Tier 1分类器（可以采用交叉验证的方式学习），然后将输出用于训练Tier 2 分类器。</p><p>完整内容请阅读： <a href="https://blog.csdn.net/Gamer_gyt/article/details/85689424" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/85689424</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="机器学习" scheme="http://thinkgamer.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="集成学习" scheme="http://thinkgamer.cn/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>基于TF-IDF算法的短标题关键词提取</title>
    <link href="http://thinkgamer.cn/2019/01/03/NLP/%E5%9F%BA%E4%BA%8ETF-IDF%E7%AE%97%E6%B3%95%E7%9A%84%E7%9F%AD%E6%A0%87%E9%A2%98%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96/"/>
    <id>http://thinkgamer.cn/2019/01/03/NLP/基于TF-IDF算法的短标题关键词提取/</id>
    <published>2019-01-02T19:27:48.000Z</published>
    <updated>2019-04-13T05:31:29.638Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="TF-IDF算法介绍"><a href="#TF-IDF算法介绍" class="headerlink" title="TF-IDF算法介绍"></a>TF-IDF算法介绍</h1><p>TF-IDF（Term Frequency–InverseDocument Frequency）是一种用于资讯检索与文本挖掘的常用加权技术。TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。</p><p>完整内容请阅读：<a href="https://blog.csdn.net/Gamer_gyt/article/details/85690389" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/85690389</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="NLP" scheme="http://thinkgamer.cn/tags/NLP/"/>
    
      <category term="TF_IDF" scheme="http://thinkgamer.cn/tags/TF-IDF/"/>
    
      <category term="短标题" scheme="http://thinkgamer.cn/tags/%E7%9F%AD%E6%A0%87%E9%A2%98/"/>
    
      <category term="关键词提取" scheme="http://thinkgamer.cn/tags/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96/"/>
    
  </entry>
  
  <entry>
    <title>【内附PDF资料】Python实现下载图片并生产PDF文件</title>
    <link href="http://thinkgamer.cn/2019/01/02/Python/%E3%80%90%E5%86%85%E9%99%84PDF%E8%B5%84%E6%96%99%E3%80%91Python%E5%AE%9E%E7%8E%B0%E4%B8%8B%E8%BD%BD%E5%9B%BE%E7%89%87%E5%B9%B6%E7%94%9F%E4%BA%A7PDF%E6%96%87%E4%BB%B6/"/>
    <id>http://thinkgamer.cn/2019/01/02/Python/【内附PDF资料】Python实现下载图片并生产PDF文件/</id>
    <published>2019-01-02T12:28:22.000Z</published>
    <updated>2019-04-13T05:22:04.809Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><blockquote><p>2018AICon大会中的一些PPT，官方没有提供完整的PDF文件，而是一张张图片，不方便下载和后续阅读，这里使用Python爬取相关演讲的图片，并生产PDF文件</p></blockquote><h1 id="下载函数"><a href="#下载函数" class="headerlink" title="下载函数"></a>下载函数</h1><p>创建下载图片函数，使用的是Python的urllib库，代码如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 下载图片</span><br><span class="line">def save_img(img_url,file_path=&quot;./img/&quot;):</span><br><span class="line">    if not os.path.exists(file_path):</span><br><span class="line">        print(&quot;保存图片的文件不存在，创建该目录&quot;)</span><br><span class="line">        os.mkdir(file_path)</span><br><span class="line">    # 图片后缀</span><br><span class="line">    file_suffix = os.path.splitext(img_url)[1]</span><br><span class="line">    file_name = str ( int(img_url.split(&quot;-&quot;)[1].split(&quot;.&quot;)[0]) )</span><br><span class="line">    # 拼接图片名（包含路径）</span><br><span class="line">    filename = &apos;&#123;&#125;&#123;&#125;&#123;&#125;&apos;.format(file_path,file_name,file_suffix)</span><br><span class="line">    # urllib</span><br><span class="line">    urllib.request.urlretrieve(img_url, filename=filename)</span><br></pre></td></tr></table></figure><h1 id="生成PDF函数"><a href="#生成PDF函数" class="headerlink" title="生成PDF函数"></a>生成PDF函数</h1><p>创建图片生成PDF文件的函数，使用的是Python的reportlab库，代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 生成pdf</span><br><span class="line">def convert_img_to_pdf(img_path,pdf_path):</span><br><span class="line">    pages = 0</span><br><span class="line">    (w, h) = landscape(portrait(A4))</span><br><span class="line">    can = canvas.Canvas(pdf_path, pagesize=landscape(portrait(A4)))</span><br><span class="line">    # 获取img_path下文件，并进行排序</span><br><span class="line">    files = os.listdir(img_path)</span><br><span class="line">    files.sort(key=lambda x: int(x[:-4]))</span><br><span class="line">    # 遍历每个文件</span><br><span class="line">    for f_name in files:</span><br><span class="line">        # 拼装成完整的file路径</span><br><span class="line">        file_path = img_path + os.sep + str(f_name)</span><br><span class="line">        can.drawImage(file_path, 0, 0, w, h)</span><br><span class="line">        can.showPage()</span><br><span class="line">        pages = pages + 1</span><br><span class="line">    can.save()</span><br></pre></td></tr></table></figure></p><p>需要注意的是，pagesize不能直接指定值portrait(A4)，因为一张图片会完整的嵌套在一页A4纸张上，极其不美观，这里需要将A4的大小进行转置，pagesize=landscape(portrait(A4))</p><h1 id="定义要生成的PDF相关信息"><a href="#定义要生成的PDF相关信息" class="headerlink" title="定义要生成的PDF相关信息"></a>定义要生成的PDF相关信息</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">_list = (</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;pdf_name&quot;: &quot;FFM及DeepFFM模型在推荐系统的探索及实践.pdf&quot;,</span><br><span class="line">        &quot;img_path&quot;: &quot;./img_1/&quot;,</span><br><span class="line">        &quot;page&quot;: 52,</span><br><span class="line">        &quot;id&quot;: &quot;3670025915&quot;</span><br><span class="line">    &#125;,&#123;</span><br><span class="line">        &quot;pdf_name&quot;: &quot;知乎推荐系统的实践及重构之路.pdf&quot;,</span><br><span class="line">        &quot;img_path&quot;: &quot;./img_2/&quot;,</span><br><span class="line">        &quot;page&quot;: 38,</span><br><span class="line">        &quot;id&quot;: &quot;4291192513&quot;</span><br><span class="line">    &#125;,&#123;</span><br><span class="line">        &quot;pdf_name&quot;: &quot;深度树匹配——下一代推荐技术的探索和实践.pdf&quot;,</span><br><span class="line">        &quot;img_path&quot;: &quot;./img_3/&quot;,</span><br><span class="line">        &quot;page&quot;: 33,</span><br><span class="line">        &quot;id&quot;: &quot;3621355867&quot;</span><br><span class="line">    &#125;,&#123;</span><br><span class="line">        &quot;pdf_name&quot;: &quot;基于知识的搜索推荐技术及应用.pdf&quot;,</span><br><span class="line">        &quot;img_path&quot;: &quot;./img_4/&quot;,</span><br><span class="line">        &quot;page&quot;: 31,</span><br><span class="line">        &quot;id&quot;: &quot;436657700&quot;</span><br><span class="line">    &#125;,&#123;</span><br><span class="line">        &quot;pdf_name&quot;: &quot;瓜子二手车个性化推荐的挑战与应对.pdf&quot;,</span><br><span class="line">        &quot;img_path&quot;: &quot;./img_5/&quot;,</span><br><span class="line">        &quot;page&quot;: 40,</span><br><span class="line">        &quot;id&quot;: &quot;1433077746&quot;</span><br><span class="line">    &#125;,&#123;</span><br><span class="line">        &quot;pdf_name&quot;: &quot;机器学习在苏宁搜索平台中的实践.pdf&quot;,</span><br><span class="line">        &quot;img_path&quot;: &quot;./img_6/&quot;,</span><br><span class="line">        &quot;page&quot;: 55,</span><br><span class="line">        &quot;id&quot;: &quot;1155556309&quot;</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h1 id="生成PDF"><a href="#生成PDF" class="headerlink" title="生成PDF"></a>生成PDF</h1><p>遍历要生成PDF的每个信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">for one in _list:</span><br><span class="line">    print(one)</span><br><span class="line">    for i in range(1,one[&quot;page&quot;]+1):</span><br><span class="line">        img_path = one[&quot;img_path&quot;]</span><br><span class="line">        pdf_path = one[&quot;pdf_name&quot;]</span><br><span class="line">        if i &lt; 10:</span><br><span class="line">            img_url = &quot;https://static001.geekbang.org/con/37/pdf/&quot; + one[&quot;id&quot;] + &quot;/image/page-00&quot; + str(i) + &quot;.jpg&quot;</span><br><span class="line">        else:</span><br><span class="line">            img_url = &quot;https://static001.geekbang.org/con/37/pdf/&quot; + one[&quot;id&quot;] + &quot;/image/page-0&quot; + str(i) + &quot;.jpg&quot;</span><br><span class="line">        print(img_url)</span><br><span class="line">        # 下载图片</span><br><span class="line">        save_img(img_url,img_path)</span><br><span class="line">        # 合成pdf</span><br><span class="line">        convert_img_to_pdf(img_path, pdf_path)</span><br><span class="line">    print(&quot;Image转PDF完成！&quot;)</span><br></pre></td></tr></table></figure></p><p>PDF文件资料获取方式：百度网盘</p><p>链接：<a href="https://pan.baidu.com/s/1MC_bH2x5jjsP1LvBB5cZCA" target="_blank" rel="external">https://pan.baidu.com/s/1MC_bH2x5jjsP1LvBB5cZCA</a></p><p>提取码：58zq</p><hr><p>原文链接：<a href="https://blog.csdn.net/Gamer_gyt/article/details/85637436" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/85637436</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="Python" scheme="http://thinkgamer.cn/tags/Python/"/>
    
      <category term="资料" scheme="http://thinkgamer.cn/tags/%E8%B5%84%E6%96%99/"/>
    
  </entry>
  
  <entry>
    <title>多分类逻辑回归（Multinomial Logistic Regression）</title>
    <link href="http://thinkgamer.cn/2018/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%88%86%E7%B1%BB%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Multinomial%20Logistic%20Regression%EF%BC%89/"/>
    <id>http://thinkgamer.cn/2018/12/21/机器学习/多分类逻辑回归（Multinomial Logistic Regression）/</id>
    <published>2018-12-21T07:53:10.000Z</published>
    <updated>2019-04-13T05:18:32.057Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>分类从结果的数量上可以简单的划分为：</p><ul><li>二分类（Binary Classification）</li><li>多分类（Multinomial  Classification）。</li></ul><p>其中二分类是最常见且使用最多的分类场景，解决二分类的算法有很多，比如：</p><ul><li>基本的KNN、贝叶斯、SVM</li><li>Online Ranking中用来做二分类的包括FM、FFM、GBDT、LR、XGBoost等</li></ul><p>多分类中比如：</p><ul><li>改进版的KNN、改进版的贝叶斯、改进版的SVM等</li><li>多类别的逻辑回归<br>啰嗦了这么多，其实就是为了说这个多分类的逻辑回归。</li></ul><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>在统计学里，多类别逻辑回归是一个将逻辑回归一般化成多类别问题得到的分类方法。用更加专业的话来说，它就是一个用来预测一个具有类别分布的因变量不同可能结果的概率的模型。 </p><p>另外，多类别逻辑回归也有很多其它的名字，包括polytomous LR，multiclass LR，softmax regression，multinomial logit，maximum entropy classifier，conditional maximum entropy model。 </p><p>在多类别逻辑回归中，因变量是根据一系列自变量（就是我们所说的特征、观测变量）来预测得到的。具体来说，就是通过将自变量和相应参数进行线性组合之后，使用某种概率模型来计算预测因变量中得到某个结果的概率，而自变量对应的参数是通过训练数据计算得到的，有时我们将这些参数成为回归系数。</p><h2 id="模型分析"><a href="#模型分析" class="headerlink" title="模型分析"></a>模型分析</h2><p>原文：<a href="https://blog.csdn.net/Gamer_gyt/article/details/85209496" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/85209496</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="LR" scheme="http://thinkgamer.cn/tags/LR/"/>
    
      <category term="多分类" scheme="http://thinkgamer.cn/tags/%E5%A4%9A%E5%88%86%E7%B1%BB/"/>
    
      <category term="逻辑回归" scheme="http://thinkgamer.cn/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
      <category term="Logistic Regression" scheme="http://thinkgamer.cn/tags/Logistic-Regression/"/>
    
  </entry>
  
</feed>
