<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>文艺与Code | Thinkgamer的博客</title>
  <icon>https://www.gravatar.com/avatar/1b9c8afc3fc1dc6be26316835c6f4fc4</icon>
  <subtitle>CTR/DL/ML/RL</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://thinkgamer.cn/"/>
  <updated>2019-04-13T04:40:32.345Z</updated>
  <id>http://thinkgamer.cn/</id>
  
  <author>
    <name>Thinkgamer</name>
    <email>thinkgamer@163.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Thinkgamer&#39;s 简历</title>
    <link href="http://thinkgamer.cn/8888/08/08/%E5%85%B3%E4%BA%8E%E6%88%91/"/>
    <id>http://thinkgamer.cn/8888/08/08/关于我/</id>
    <published>8888-08-08T00:08:08.000Z</published>
    <updated>2019-04-13T04:40:32.345Z</updated>
    
    <content type="html"><![CDATA[<hr><h1 id="个人信息"><a href="#个人信息" class="headerlink" title="个人信息"></a>个人信息</h1><p>唯一：&nbsp;&nbsp;&nbsp;&nbsp;<strong>Thinkgamer</strong><br>姓名：&nbsp;&nbsp;&nbsp;&nbsp;<strong>高阳团</strong><br>家乡：&nbsp;&nbsp;&nbsp;&nbsp;<strong>河南-郑州</strong><br>电话：&nbsp;&nbsp;&nbsp;&nbsp;<strong>17600977634</strong><br>邮箱：&nbsp;&nbsp;&nbsp;&nbsp;<strong>thinkgamer@163.com</strong><br>毕业：&nbsp;&nbsp;&nbsp;&nbsp;<strong>沈阳航空航天大学-计算机学院-软件工程</strong><br>就职：&nbsp;&nbsp;&nbsp;&nbsp;<strong>京东商城 | 算法工程师</strong></p><hr><h1 id="技术园地："><a href="#技术园地：" class="headerlink" title="技术园地："></a>技术园地：</h1><ul><li>CSDN：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a></li><li>Github：<a href="https://github.com/Thinkgamer" target="_blank" rel="external">https://github.com/Thinkgamer</a></li><li>知乎: <a href="https://www.zhihu.com/people/thinkgamer/activities" target="_blank" rel="external">https://www.zhihu.com/people/thinkgamer/activities</a></li><li>公众号：数据与算法联盟<br><img src="/assets/img/gongzhonghao.jpg" weight="250px" height="250px"></li></ul><hr><h1 id="工作经历"><a href="#工作经历" class="headerlink" title="工作经历"></a>工作经历</h1><h2 id="2017-12-25～至今-京东商城"><a href="#2017-12-25～至今-京东商城" class="headerlink" title="2017-12-25～至今 | 京东商城"></a>2017-12-25～至今 | 京东商城</h2><ul><li>个性化消息Push</li></ul><blockquote><p>简称“种草”。开发种草整个联动方案，负责相关资源安排沟通，充分理解业务，个性化消息push模型开发训练。</p><p>深度学习模型调研，基于公司内部平台，上线基于tensorflow-serving的深度学习模型，效果较ML模型提升显著。</p></blockquote><ul><li>Plus会员个性化推荐</li></ul><blockquote><p>理解内部推荐架构原理，开发方案，资源安排，个性化推荐模型开发训练，相关CTR预估模型研究与离线测试。</p></blockquote><ul><li>商品价格段模型</li></ul><blockquote><p>基于KMeans构建商品价格段模型。</p></blockquote><ul><li>商品质量分模型</li></ul><blockquote><p>基于线性模型构建商品质量分模型，用户推荐架构中的召回粗排。 </p></blockquote><ul><li>特征监控模型</li></ul><blockquote><p>数据和特征决定了机器学习的上限,而模型和算法只是逼近这个上限。特征对于模型来说极其重要，因为对于特征的监控十分有必要，该模型支持使用者自定义监控指标和监控字段，能够有效的减少出现问题时的排查时间提高效果，并进行预警。</p></blockquote><ul><li>基础数据开发<ul><li>业务内特征开发</li><li>全站特征开发</li><li>召回数据开发</li></ul></li></ul><h2 id="2016-10～2017-12-北京万维星辰科技有限公司"><a href="#2016-10～2017-12-北京万维星辰科技有限公司" class="headerlink" title="2016-10～2017-12 | 北京万维星辰科技有限公司"></a>2016-10～2017-12 | 北京万维星辰科技有限公司</h2><ul><li>搭建基于 Hadoop 和 ELK 技术栈的日志分析系统</li></ul><blockquote><p>参与设计了基于 ELK 的日志分析系统，提出并搭建了 Hadoop 数据备份系统，研究了 ELK 周边的<br>开源产品 ，学习并使用 rails 实现 es 数据的快照备份。</p></blockquote><ul><li>异常检测算法研究与实现</li></ul><blockquote><p>1：根据合作方提供的 wlan 上网数据，对用户进行肖像刻画，从而对后入数据进行异常值估计。</p><p>2：研究基于指数平滑和线性回归的异常值检测，并使用 python 的 elasticsearch 进行实现。</p></blockquote><ul><li>中彩/德州银行日志审计项目</li></ul><blockquote><p>利用公司的日志分析系统对中国福利彩票和德州银行的日志进行分析，并形成安全事件，提出相应的整改和解决意见，形成月度报告。</p></blockquote><h2 id="2016-07～2016-09-北京广联达软件有限公司"><a href="#2016-07～2016-09-北京广联达软件有限公司" class="headerlink" title="2016-07～2016-09 | 北京广联达软件有限公司"></a>2016-07～2016-09 | 北京广联达软件有限公司</h2><blockquote><p>实习以课题形式（课题为：基于质量数据的数据分析平台搭建）展开，利用 Hadoop 等开源组件搭建了 5 台分布式系统，包含 Hadoop，Hive，Spark，Zookeeper，Sqoop 和 Hbase，在该平台上完成了豆瓣影评数据分析 Demo</p></blockquote><hr><h1 id="大学经历"><a href="#大学经历" class="headerlink" title="大学经历"></a>大学经历</h1><h2 id="项目经历"><a href="#项目经历" class="headerlink" title="项目经历"></a>项目经历</h2><ul><li>基于 Hadoop 和机器学习的博客统计分析平台</li></ul><blockquote><p>采用 Django 作为 Web 开发基础，Python 爬取了 CSDN 博客的部分数据，存储到 hdfs 上，利用 MapReduce 对数据进行了离线计算，将解析好的字段存储到 Hive 中，利用 python 开发实现了协同过滤算法和 PangRank 算法。最终此项目在辽宁省计算机作品大赛中获得二等奖，中国大学生计算机作品大赛中获得三等奖。</p></blockquote><ul><li>图书推荐系统</li></ul><blockquote><p>python 爬取了豆瓣图书数据，对数据进行清洗之后，使用基于 Item 和 User 的协同过滤算法对登录用户产生图书推荐，此项目为大三期间为一个网友做的毕业设计。</p></blockquote><h2 id="荣誉奖励"><a href="#荣誉奖励" class="headerlink" title="荣誉奖励"></a>荣誉奖励</h2><ul><li>单项一等奖学金  * 2</li><li>综合二等奖学金  * 2</li><li>单项支援服务标兵</li><li>优秀团干 * 2</li><li>辽宁省ACM优秀志愿者</li><li>校ACM三等奖</li><li>沈阳航空航天大学计算机作品大赛二等奖【网站】</li><li>辽宁省计算机作品大赛二等奖【博客统计分析系统】</li><li>中国大学生作品大赛三等奖【博客统计分析系统】</li></ul><h2 id="工作经历-1"><a href="#工作经历-1" class="headerlink" title="工作经历"></a>工作经历</h2><ul><li>助理辅导员 | 2014.09-2015.07</li></ul><blockquote><p>计算机学院 2014 级新生助理辅导员，协助辅导员进行大一班级的日常管理</p></blockquote><ul><li>活动部部长 | 2014.09-2015.07</li></ul><blockquote><p>爱心联合会活动部部长，负责相关活动的宣传推广与执行</p></blockquote><ul><li>班级团支书 | 2013.09-2017.06</li></ul><blockquote><p>协助辅导员进行班级日常的管理和相关共青团工作的开展</p></blockquote><hr><h1 id="自我评价"><a href="#自我评价" class="headerlink" title="自我评价"></a>自我评价</h1><ul><li>不服输，爱钻研，具有一定程度自学能力和解决问题能力。</li><li>喜欢看书，看文章，整理笔记。</li><li>自我的文艺青年。</li></ul><hr><h1 id="联系我："><a href="#联系我：" class="headerlink" title="联系我："></a>联系我：</h1><ul><li><img src="/assets/img/myweixin.png" height="300" width="220"></li></ul><p>【加我微信，拉你进数据与算法交流群，每天都有技术讨论】</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;h1 id=&quot;个人信息&quot;&gt;&lt;a href=&quot;#个人信息&quot; class=&quot;headerlink&quot; title=&quot;个人信息&quot;&gt;&lt;/a&gt;个人信息&lt;/h1&gt;&lt;p&gt;唯一：&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;strong&gt;Thinkgamer&lt;/strong&gt;&lt;br
      
    
    </summary>
    
    
      <category term="Thinkgamer" scheme="http://thinkgamer.cn/tags/Thinkgamer/"/>
    
  </entry>
  
  <entry>
    <title>商务合作介绍</title>
    <link href="http://thinkgamer.cn/6666/06/06/%E5%95%86%E5%8A%A1%E5%90%88%E4%BD%9C%E4%BB%8B%E7%BB%8D/"/>
    <id>http://thinkgamer.cn/6666/06/06/商务合作介绍/</id>
    <published>6666-06-05T16:00:00.000Z</published>
    <updated>2019-04-13T04:40:15.346Z</updated>
    
    <content type="html"><![CDATA[<hr><center><h1>WelCome To “Thinkgamer 小站”</h1></center><hr><center><h2>合作范围</h2></center><div class="table-container"><table><thead><tr><th style="text-align:center">Web全栈</th><th style="text-align:center">数据服务</th><th style="text-align:center">模型构建</th></tr></thead><tbody><tr><td style="text-align:center">:-:</td><td style="text-align:center">:-:</td><td style="text-align:center">:-: </td></tr><tr><td style="text-align:center">论文算法实现</td><td style="text-align:center">大数据服务</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">:-:</td><td style="text-align:center">:-:</td><td style="text-align:center">:-: </td></tr><tr><td style="text-align:center">跟拍摄影</td><td style="text-align:center">广告接入</td><td style="text-align:center"></td></tr></tbody></table></div><blockquote><p>全网唯一ID：Thinkgamer，左侧”关于我“关注微信公众号”数据与算法联盟“，可在公众号添加我的微信，本人涉猎范围包括：推荐系统，Python，机器学习，Web开发，大数据云计算，ELK。</p></blockquote><h1 id="▶-Web全栈"><a href="#▶-Web全栈" class="headerlink" title="▶ Web全栈"></a>▶ Web全栈</h1><blockquote><p>如果您在创业，苦于没有额外精力管理一个技术团队；如果您在工作，遇到了一些您解决不了的问题；如果您的网站苦于没有运维；如果您的数据需要备份；如果一切有关Web开发运维的问题。您都可以来找我，我虽不是最厉害的，但绝对会为您提供最优质的服务。</p></blockquote><h1 id="▶-数据服务"><a href="#▶-数据服务" class="headerlink" title="▶ 数据服务"></a>▶ 数据服务</h1><blockquote><p>包含但不局限于以下数据相关的服务：</p></blockquote><ul><li>数据采集（一次性和程序开发）</li><li>数据清洗</li><li>数据可视化（不限于Web）</li><li>数据存储方案设计与实现</li><li>… …</li></ul><p>本人曾多次向他人提供数据相关的技术服务，积累了一定的经验，相信能够为您提供全方位的数据服务。</p><h1 id="▶-模型构建"><a href="#▶-模型构建" class="headerlink" title="▶ 模型构建"></a>▶ 模型构建</h1><blockquote><p>根据对方提供的具体业务场景，进行相关模型选择与构建。当然，如果有荣幸参与您的场景选定和数据准备阶段，也是极好的。</p></blockquote><h1 id="▶-论文算法实现"><a href="#▶-论文算法实现" class="headerlink" title="▶ 论文算法实现"></a>▶ 论文算法实现</h1><blockquote><p>如果您是一个马上要毕业的本科或者研究生，如果您苦于论文的立项与项目实现，如果您没有更好的主意，欢迎您来找我，加我的个人微信，为您的毕业保驾护航。</p></blockquote><h1 id="▶-大数据与分布式计算"><a href="#▶-大数据与分布式计算" class="headerlink" title="▶ 大数据与分布式计算"></a>▶ 大数据与分布式计算</h1><blockquote><p>提供大数据相关的服务，包含但不局限于：</p></blockquote><ul><li>大数据分析平台方案设计</li><li>大数据分析平台搭建</li><li>基于平台的数据分析Demo实现</li><li>海量数据的分布式计算处理</li><li>… …</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;欢迎来找我，24小时在线。</p><h1 id="▶-广告接入"><a href="#▶-广告接入" class="headerlink" title="▶ 广告接入"></a>▶ 广告接入</h1><blockquote><p>眼前的黑不是黑，Ta们说的白是什么白，也许一直是我们忘了搭一座桥，到对方的心里瞧一瞧。你的品牌，你的知名度为什么那么低，因为你没有使用我的广告接入，那么问题来了，包含但不局限于以下几种情况的，可以加我微信私聊了：</p></blockquote><ul><li>品牌宣传</li><li>广告位接入</li><li>公众号互相推广</li><li>个人网站/社区主页链接</li><li>… …</li></ul><h1 id="▶-跟拍摄影"><a href="#▶-跟拍摄影" class="headerlink" title="▶ 跟拍摄影"></a>▶ 跟拍摄影</h1><blockquote><p>如果您在旅游途中缺少了一个摄影的小跟班；如果您苦于找不到好的角度拍照；如果您是一个人，苦于没有人照出你的美；如果您的照片需要美化与调整。那么请您来找我，保证为您提供最优质的技术与服务。</p><p>业务涉及：</p></blockquote><ul><li>跟拍摄影</li><li>照片美化与调整</li><li>PS技术服务</li></ul><center><font color="#0099ff" size="8" face="黑体">小本生意&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;诚信经验<br><br>大神勿扰&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;自求多福</font> </center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;center&gt;
&lt;h1&gt;WelCome To “Thinkgamer 小站”&lt;/h1&gt;
&lt;/center&gt;

&lt;hr&gt;
&lt;center&gt;
&lt;h2&gt;合作范围&lt;/h2&gt;
&lt;/center&gt;

&lt;div class=&quot;table-container&quot;&gt;
&lt;table&gt;
&lt;t
      
    
    </summary>
    
    
      <category term="商务合作" scheme="http://thinkgamer.cn/tags/%E5%95%86%E5%8A%A1%E5%90%88%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>点击率预估中的FM算法和FFM算法</title>
    <link href="http://thinkgamer.cn/2018/07/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%82%B9%E5%87%BB%E7%8E%87%E9%A2%84%E4%BC%B0%E4%B8%AD%E7%9A%84FM&amp;FFM%E7%AE%97%E6%B3%95/"/>
    <id>http://thinkgamer.cn/2018/07/13/机器学习/点击率预估中的FM&amp;FFM算法/</id>
    <published>2018-07-13T08:45:14.000Z</published>
    <updated>2019-04-13T04:36:17.794Z</updated>
    
    <content type="html"><![CDATA[<p>特征决定了所有算法效果的上限，而不同的算法只是离这个上限的距离不同而已</p><h1 id="CTR方法概览"><a href="#CTR方法概览" class="headerlink" title="CTR方法概览"></a>CTR方法概览</h1><ol><li>广义线性模型+人工特征组合（LR+FeatureEngineering）</li><li>非线性模型（GBDT，FM，FFM，DNN，MLR）</li><li>广义线性模型+非线性模型组合特征（模型融合，常见的是LR+GBDT）</li></ol><p>其中 2（非线性模型）又可以分为：<br>      ○ 矩阵分解类 （FM，FFM）<br>      ○ 回归类        （GBDT，MLR）<br>      ○ 神经网络类 （DNN）</p><hr><p>转载请注明出处：<a href="https://blog.csdn.net/gamer_gyt/article/details/81038913" target="_blank" rel="external">https://blog.csdn.net/gamer_gyt/article/details/81038913</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="FM算法"><a href="#FM算法" class="headerlink" title="FM算法"></a>FM算法</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>FM算法（Factorization Machine）一般翻译为“因子分解机”，2010年，它由当时还在日本大阪大学的Steffen Rendle提出。此算法的主要作用是可以把所有特征进行高阶组合，减少人工参与特征组合的工作，工程师可以将精力集中在模型参数调优。FM只需要线性时间复杂度，可以应用于大规模机器学习。经过部分数据集试验，此算法在稀疏数据集合上的效果要明显好于SVM。</p><h2 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h2><p>假设一个广告分类问题，根据用户和广告位相关的特征，预测用户是否点击了广告。</p><div class="table-container"><table><thead><tr><th>Clicked?</th><th>Country</th><th>Day</th><th>Ad_type</th></tr></thead><tbody><tr><td>1</td><td>USA</td><td>26/11/15</td><td>Movie</td></tr><tr><td>0</td><td>China</td><td>1/7/14</td><td>Game</td></tr><tr><td>1</td><td>China</td><td>19/2/15</td><td>Game</td></tr></tbody></table></div><p>“Clicked?”是label，Country、Day、Ad_type是特征。由于三种特征都是categorical类型的，需要经过独热编码（One-Hot Encoding）转换成数值型特征。</p><div class="table-container"><table><thead><tr><th>Clicked?</th><th>Country=USA</th><th>Country=China</th><th>Day=26/11/15</th><th>Day=1/7/14</th><th>Day=19/2/15</th><th>Ad_type=Movie</th><th>Ad_type=Game</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td></tr><tr><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td></tr><tr><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td></tr></tbody></table></div><p>经过one-hot编码之后，特征变得稀疏，上边实例中每个样本有7维的特征，但平均仅有3维为非0值，在电商场景中预测sku是否被点击的过程中，特征往往要比上例中多的多，可见数据稀疏性是工业环境中不可避免的问题。另外一个问题就是特征在经过one-hot编码之后，维度会变得非常大，比如说三级品类有3k个，那么sku的所属三级品类特征经过编码之后就会变成3k维，特征空间就会剧增。</p><p>依旧分析上边的例子，特征在经过关联之后，与label之间的相关性就会增加。例如“USA”与“Thanksgiving”、“China”与“Chinese New Year”这样的关联特征，对用户的点击有着正向的影响。换句话说，来自“China”的用户很可能会在“Chinese New Year”有大量的浏览、购买行为，而在“Thanksgiving”却不会有特别的消费行为。这种关联特征与label的正向相关性在实际问题中是普遍存在的，如“化妆品”类商品与“女”性，“球类运动配件”的商品与“男”性，“电影票”的商品与“电影”品类偏好等。因此，引入两个特征的组合是非常有意义的。</p><p>综上，FM所解决的问题是<br>1)：特征稀疏<br>2)：特征组合</p><h2 id="模型形式"><a href="#模型形式" class="headerlink" title="模型形式"></a>模型形式</h2><p>对于特征集合x=（x1,x2,x3,x4,….,xn）和标签y，希望得到x和y的对应关系，最简单的是建立线性回归模型，<br><img src="https://bdn.135editor.com/files/users/360/3608534/201807/fBKAMR8S_7Dba.png"></p><p>但是，一般线性模型无法学习到高阶组合特征，所以会将特征进行高阶组合，这里以二阶为例(理论上，FM可以组合任意高阶，但是由于计算复杂度，实际中常用二阶，后面也主要介绍二阶组合的内容)。模型形式为，</p><p><img src="https://bdn.135editor.com/files/users/360/3608534/201807/cOBUyUJj_TmqU.png"></p><p>其中n代表样本的特征数量，x_i是第i个特征，w_0，w_i，w_ij是模型参数<br>从公式(2)中可以看出，组合特征参数一共有n(n-1)/2个，任意两个参数之间都是独立的，在数据特别稀疏的情况下，二次项参数的训练是十分困难的，原因为每个wij的训练都需要大量的非零x_i，x_j样本，由于样本稀疏，满足条件的样本很少，就会导致参数w_ij不准确，最终将影响模型的性能。<br>如何解决二次项参数的训练？可以利用矩阵分解的思路。<br>在model-based的协同过滤中，一个rating矩阵可以分解为user矩阵和item矩阵，每个user和item都可以采用一个隐向量表示。比如在下图中的例子中，我们把每个user表示成一个二维向量，同时把每个item表示成一个二维向量，两个向量的点积就是矩阵中user对item的打分。</p><p><img src="https://bdn.135editor.com/files/users/360/3608534/201807/UhsawK7r_t2gO.png"></p><p>所有的二次项参数 w_ij 可以组成一个对称矩阵W，那么这个矩阵可以分解为W=V^T * V，V的第j列便是第j维特征的隐向量，换句话说就是每个参数w_ij=<v_i,v_j>，这里的v_i和v_j是分别是第i，j个特征的隐向量，这就是FM的核心思想，因此FM的模型方程为（二阶形式）</v_i,v_j></p><p><img src="https://bdn.135editor.com/files/users/360/3608534/201807/ta3RTSJ7_P6vQ.png"></p><p>其中，V_i是第i维特征的隐向量，&lt;<em>,</em>&gt;表示两个向量内积，隐向量的长度k（k&lt;&lt;n），包含k个描述特征的因子，这样二次项的参数便从n^2 减少到了nk，远少于多项式模型的参数数量。<br>另外，参数因子化使得 x_h，x_i的参数和 x_i，x_j 的参数不再是相互独立的，因此我们可以在样本稀疏的情况下相对合理地估计FM的二次项参数。具体来说，x_h，x_i 和 x_i，x_j 的系数分别为 ⟨v_h,v_i⟩ 和 ⟨v_i,v_j⟩，它们之间有共同项 vi。也就是说，所有包含“xi的非零组合特征”（存在某个 j≠i，使得 x_i x_j≠0）的样本都可以用来学习隐向量 v_i，这很大程度上避免了数据稀疏性造成的影响。而在多项式模型中，w_hi 和 w_ij 是相互独立的。<br>显而易见，公式(3)是一个通用的拟合方程，可以采用不同的损失函数用于解决回归、二元分类等问题，比如可以采用MSE（Mean Square Error）损失函数来求解回归问题，也可以采用Hinge/Cross-Entropy损失（铰链损失，互熵损失）来求解分类问题。当然，在进行二元分类时，FM的输出需要经过sigmoid变换，这与Logistic回归是一样的。直观上看，FM的复杂度是 O(kn^2)。但是，通过公式(3)的等式，FM的二次项可以化简，其复杂度可以优化到 O(kn)。由此可见，FM可以在线性时间对新样本作出预测。</p><p><img src="https://bdn.135editor.com/files/users/360/3608534/201807/ZvytpfVA_wCpX.png"></p><p>从(3)—&gt;(4)推导如下：</p><p><img src="https://bdn.135editor.com/files/users/360/3608534/201807/MS5AAkAm_kPrI.png"></p><p>解读第（1）步到第（2）步，这里用A表示系数矩阵V的上三角元素，B表示对角线上的交叉项系数。由于系数矩阵V是一个对称阵，所以下三角与上三角相等，有下式成立：</p><p><img src="https://bdn.135editor.com/files/users/360/3608534/201807/RpSGrOmQ_AfmL.png"><br>如果用随机梯度下降（Stochastic Gradient Descent）法学习模型参数。那么，模型各个参数的梯度如下：</p><p><img src="https://bdn.135editor.com/files/users/360/3608534/201807/T8MAnmvn_apWg.png"></p><p>其中，v_j,f 是隐向量 v_j 的第 f 个元素。由于 ∑nj=1vj,fxj 只与 f 有关，而与 i 无关，在每次迭代过程中，只需计算一次所有 f 的 ∑nj=1vj,fxj，就能够方便地得到所有 v_i,f 的梯度。显然，计算所有 f 的 ∑nj=1vj,fxj 的复杂度是 O(kn)；已知 ∑nj=1vj,fxj 时，计算每个参数梯度的复杂度是 O(1)；得到梯度后，更新每个参数的复杂度是 O(1)；模型参数一共有 nk+n+1 个。因此，FM参数训练的复杂度也是 O(kn)。综上可知，FM可以在线性时间训练和预测，是一种非常高效的模型。</p><h2 id="FM总结"><a href="#FM总结" class="headerlink" title="FM总结"></a>FM总结</h2><h3 id="FM降低了交叉项参数学习不充分的影响"><a href="#FM降低了交叉项参数学习不充分的影响" class="headerlink" title="FM降低了交叉项参数学习不充分的影响"></a>FM降低了交叉项参数学习不充分的影响</h3><p>one-hot编码后的样本数据非常稀疏，组合特征更是如此。为了解决交叉项参数学习不充分、导致模型有偏或不稳定的问题。借鉴矩阵分解的思路：每一维特征用k维的隐向量表示，交叉项的参数w_ij用对应特征隐向量的内积表示，即⟨v_i,v_j⟩（也可以理解为平滑技术）。这样参数学习由之前学习交叉项参数w_ij的过程，转变为学习n个单特征对应k维隐向量的过程。<br>很明显，单特征参数（k维隐向量v_i）的学习要比交叉项参数w_ij学习得更充分。示例说明：<br>假如有10w条训练样本，其中出现女性特征的样本数为3w，出现男性特征的样本数为7w，出现汽车特征的样本数为2000，出现化妆品的样本数为1000。特征共现的样本数如下：</p><div class="table-container"><table><thead><tr><th>共现交叉特征</th><th>样本数</th><th>注</th></tr></thead><tbody><tr><td>&lt;女性，汽车&gt;</td><td>500</td><td>同时出现&lt;女性，汽车&gt;的样本数</td></tr><tr><td>&lt;女性，化妆品&gt;</td><td>1000</td><td>同时出现&lt;女性，化妆品&gt;的样本数</td></tr><tr><td>&lt;男性，汽车&gt;</td><td>1500</td><td>同时出现&lt;男性，汽车&gt;的样本数</td></tr><tr><td>&lt;男性，化妆品&gt;</td><td>0</td><td>样本中无此特征组合项</td></tr></tbody></table></div><p>&lt;女性，汽车&gt;的含义是女性看汽车广告。可以看到，单特征对应的样本数远大于组合特征对应的样本数。训练时，单特征参数相比交叉项特征参数会学习地更充分。<br>因此，可以说FM降低了因数据稀疏，导致交叉项参数学习不充分的影响</p><h3 id="FM提升了模型预估能力"><a href="#FM提升了模型预估能力" class="headerlink" title="FM提升了模型预估能力"></a>FM提升了模型预估能力</h3><p>依然看上面的示例，样本中没有&lt;男性，化妆品&gt;交叉特征，即没有男性看化妆品广告的数据。如果用多项式模型来建模，对应的交叉项参数W_男性,化妆品是学不出来的，因为数据中没有对应的共现交叉特征。那么多项式模型就不能对出现的男性看化妆品广告场景给出准确地预估。<br>FM模型是否能得到交叉项参数W_男性,化妆品呢？答案是肯定的。由于FM模型是把交叉项参数用对应的特征隐向量内积表示，这里表示为W_男性,化妆品=⟨v_男性,v_化妆品⟩。<br>用男性特征隐向量v_男性和v_化妆品特征隐向量v化妆品的内积表示交叉项参数 W_男性,化妆品。<br>由于FM学习的参数就是单特征的隐向量，那么男性看化妆品广告的预估结果可以用⟨v_男性,v_化妆品⟩得到。这样，即便训练集中没有出现男性看化妆品广告的样本，FM模型仍然可以用来预估，提升了预估能力。</p><h3 id="FM提升了参数学习效率"><a href="#FM提升了参数学习效率" class="headerlink" title="FM提升了参数学习效率"></a>FM提升了参数学习效率</h3><p>这个显而易见，参数个数由(n^2+n+1)变为(nk+n+1)个，模型训练复杂度也由O(mn^2)变为O(mnk)。m为训练样本数。对于训练样本和特征数而言，都是线性复杂度。<br>此外，就FM模型本身而言，它是在多项式模型基础上对参数的计算做了调整，因此也有人把FM模型称为多项式的广义线性模型，也是恰如其分的。<br>从交互项的角度看，FM仅仅是一个可以表示特征之间交互关系的函数表法式，可以推广到更高阶形式，即将多个互异特征分量之间的关联信息考虑进来。例如在广告业务场景中，如果考虑User-Ad-Context三个维度特征之间的关系，在FM模型中对应的degree为3。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>FM最大特点和优势：FM模型对稀疏数据有更好的学习能力，通过交互项可以学习特征之间的关联关系，并且保证了学习效率和预估能力</p><h1 id="FFM算法"><a href="#FFM算法" class="headerlink" title="FFM算法"></a>FFM算法</h1><h2 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h2><p>FFM（Field-aware Factorization Machine）最初的概念来自Yu-Chin Juan（阮毓钦，毕业于中国台湾大学，现在美国Criteo工作）与其比赛队员，是他们借鉴了来自Michael Jahrer的论文中的field概念提出了FM的升级版模型。</p><h2 id="模型形式-1"><a href="#模型形式-1" class="headerlink" title="模型形式"></a>模型形式</h2><p>通过引入field的概念，FFM把相同性质的特征，归结于同一个field。还是以FM中的广告分类为例，“Day=26/11/15”、“Day=1/7/14”、“Day=19/2/15”这三个特征都是代表日期的，可以放到同一个field中。同理，三级品类有3k个，那么sku的所属三级品类特征经过编码之后就会变成3k维，那么这3k维可以放到一个field中，简单来说，同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个field。<br>在FFM中，每一维特征x_i，针对其他特征的每一种field f_j，都会学习到一个隐向量V_i,f_ j。因此，隐向量不仅与特征有关，还与filed有关。也就是说，“Day=26/11/15”这个特征与“Country”特征和“Ad_type”特征进行关联的时候使用不同的隐向量，这与“Country”和“Ad_type”的内在差异相符，也是FFM中“field-aware”的由来。<br>假设样本的 n 个特征属于 f 个field，那么FFM的二次项有 nf个隐向量。而在FM模型中，每一维特征的隐向量只有一个。FM可以看作FFM的特例，是把所有特征都归属到一个field时的FFM模型。根据FFM的field敏感特性，可以导出其模型方程。</p><p><img src="https://bdn.135editor.com/files/users/360/3608534/201807/jUKaHRqz_pIvY.png"></p><p>其中，f_ j 是第 j 个特征所属的field。如果隐向量的长度为 k，那么FFM的二次参数有 f * kn 个，远多于FM模型的 kn 个。此外，由于隐向量与field相关，FFM二次项并不能化简，其预测复杂度是 O(kn^2)。<br>下面以一个例子简单说明FFM的特征组合方式。输入记录如下<br>User Movie Genre Price<br>YuChin 3Idiots Comedy, Drama $9.99<br>这条记录可以编码成5个特征，其中“Genre=Comedy”和“Genre=Drama”属于同一个field，“Price”是数值型，不用One-Hot编码转换。为了方便说明FFM的样本格式，我们将所有的特征和对应的field映射成整数编号。</p><div class="table-container"><table><thead><tr><th>Field name</th><th>Field index</th><th>Feature name</th><th>Feature index</th></tr></thead><tbody><tr><td>User</td><td>1</td><td>User=YuChin</td><td>1</td></tr><tr><td>Movie</td><td>2</td><td>Movie=3Idiots</td><td>2</td></tr><tr><td>Genre</td><td>3</td><td>Genre=Comedy</td><td>3</td></tr><tr><td>Price</td><td>4</td><td>Genre=Drama</td><td>4</td></tr><tr><td></td><td></td><td>Price</td><td>5</td></tr></tbody></table></div><p>那么，FFM的组合特征有10项，如下图所示。</p><p><img src="https://bdn.135editor.com/files/users/360/3608534/201807/BW3Vpww7_n3ZY.png"></p><p>其中，红色是field编号，蓝色是特征编号，绿色是此样本的特征取值。二次项的系数是通过与特征field相关的隐向量点积得到的，二次项共有 n(n−1)/2 个。</p><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>在DSP的场景中，FFM主要用来预估站内的CTR和CVR，即一个用户对一个商品的潜在点击率和点击后的转化率。<br>CTR和CVR预估模型都是在线下训练，然后用于线上预测。两个模型采用的特征大同小异，主要有三类：<br>  ● -用户相关的特征<br>用户相关的特征包括年龄、性别、职业、兴趣、品类偏好、浏览/购买品类等基本信息，以及用户近期点击量、购买量、消费额等统计信息。<br>  ● 商品相关的特征<br>商品相关的特征包括所属品类、销量、价格、评分、历史CTR/CVR等信息。<br>  ● 用户-商品匹配特征<br>用户-商品匹配特征主要有浏览/购买品类匹配、浏览/购买商家匹配、兴趣偏好匹配等几个维度。<br>为了使用FFM方法，所有的特征必须转换成“field_id:feat_id:value”格式，field_id代表特征所属field的编号，feat_id是特征编号，value是特征的值。数值型的特征比较容易处理，只需分配单独的field编号，如用户评论得分、商品的历史CTR/CVR等。categorical特征需要经过One-Hot编码成数值型，编码产生的所有特征同属于一个field，而特征的值只能是0或1，如用户的性别、年龄段，商品的品类id等。<br>除此之外，还有第三类特征，如用户浏览/购买品类，有多个品类id且用一个数值衡量用户浏览或购买每个品类商品的数量。这类特征按照categorical特征处理，不同的只是特征的值不是0或1，而是代表用户浏览或购买数量的数值。按前述方法得到field_id之后，再对转换后特征顺序编号，得到feat_id，特征的值也可以按照之前的方法获得。<br>CTR、CVR预估样本的类别是按不同方式获取的。CTR预估的正样本是站内点击的用户-商品记录，负样本是展现但未点击的记录；CVR预估的正样本是站内支付（发生转化）的用户-商品记录，负样本是点击但未支付的记录。构建出样本数据后，采用FFM训练预估模型，并测试模型的性能。</p><div class="table-container"><table><thead><tr><th></th><th>#(field)</th><th>#(feature)</th><th>AUC</th><th>Logloss</th></tr></thead><tbody><tr><td>站内CTR</td><td>39</td><td>2456</td><td>0.77</td><td>0.38</td></tr><tr><td>站内CVR</td><td>67</td><td>2441</td><td>0.92</td><td>0.13</td></tr></tbody></table></div><p>由于模型是按天训练的，每天的性能指标可能会有些波动，但变化幅度不是很大。这个表的结果说明，站内CTR/CVR预估模型是非常有效的。<br>在训练FFM的过程中，有许多小细节值得特别关注。<br>第一，样本归一化。FFM默认是进行样本数据的归一化，即 pa.norm 为真；若此参数设置为假，很容易造成数据inf溢出，进而引起梯度计算的nan错误。因此，样本层面的数据是推荐进行归一化的。<br>第二，特征归一化。CTR/CVR模型采用了多种类型的源特征，包括数值型和categorical类型等。但是，categorical类编码后的特征取值只有0或1，较大的数值型特征会造成样本归一化后categorical类生成特征的值非常小，没有区分性。例如，一条用户-商品记录，用户为“男”性，商品的销量是5000个（假设其它特征的值为零），那么归一化后特征“sex=male”（性别为男）的值略小于0.0002，而“volume”（销量）的值近似为1。特征“sex=male”在这个样本中的作用几乎可以忽略不计，这是相当不合理的。因此，将源数值型特征的值归一化到 [0,1] 是非常必要的。<br>第三，省略零值特征。从FFM模型的表达式(8)可以看出，零值特征对模型完全没有贡献。包含零值特征的一次项和组合项均为零，对于训练模型参数或者目标值预估是没有作用的。因此，可以省去零值特征，提高FFM模型训练和预测的速度，这也是稀疏样本采用FFM的显著优势。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="http://bourneli.github.io/ml/fm/2017/07/02/fm-remove-combine-features-by-yourself.html" target="_blank" rel="external">http://bourneli.github.io/ml/fm/2017/07/02/fm-remove-combine-features-by-yourself.html</a></li><li><a href="https://tech.meituan.com/deep_understanding_of_ffm_principles_and_practices.html" target="_blank" rel="external">https://tech.meituan.com/deep_understanding_of_ffm_principles_and_practices.html</a></li><li><a href="http://www.52caml.com/head_first_ml/ml-chapter9-factorization-family/" target="_blank" rel="external">http://www.52caml.com/head_first_ml/ml-chapter9-factorization-family/</a></li><li><a href="https://cloud.tencent.com/developer/article/1104673?fromSource=waitui" target="_blank" rel="external">https://cloud.tencent.com/developer/article/1104673?fromSource=waitui</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;特征决定了所有算法效果的上限，而不同的算法只是离这个上限的距离不同而已&lt;/p&gt;
&lt;h1 id=&quot;CTR方法概览&quot;&gt;&lt;a href=&quot;#CTR方法概览&quot; class=&quot;headerlink&quot; title=&quot;CTR方法概览&quot;&gt;&lt;/a&gt;CTR方法概览&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;广
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="推荐系统" scheme="http://thinkgamer.cn/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="FM" scheme="http://thinkgamer.cn/tags/FM/"/>
    
      <category term="FFM" scheme="http://thinkgamer.cn/tags/FFM/"/>
    
  </entry>
  
  <entry>
    <title>回归分析之逻辑回归-Logistic Regression</title>
    <link href="http://thinkgamer.cn/2018/04/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90%E4%B9%8B%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-Logistics%20Regression/"/>
    <id>http://thinkgamer.cn/2018/04/28/机器学习/回归分析之逻辑回归-Logistics Regression/</id>
    <published>2018-04-28T15:44:15.000Z</published>
    <updated>2019-04-13T04:36:44.864Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>逻辑回归与线性回归本质上是一样的，都是通过误差函数求解得到最优的系数，在形式上只不过是在线性回归上套了一个逻辑函数。线性回归的相关知识可以参考上边的给出链接文章，与线性回归相比，逻辑回归（Logistic Regression）更适用于因变量为二分变量的模型，Logistic 回归系数可用于估计模型中每个自变量的几率比。</p></blockquote><hr><p>转载请注明出处：<a href="https://blog.csdn.net/gamer_gyt/article/details/80115252" target="_blank" rel="external">https://blog.csdn.net/gamer_gyt/article/details/80115252</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><p><a href="http://blog.csdn.net/gamer_gyt/article/details/78008144" target="_blank" rel="external">回归分析之理论篇</a><br><a href="http://blog.csdn.net/gamer_gyt/article/details/78135354" target="_blank" rel="external">回归分析之线性回归（N元线性回归）</a><br><a href="http://blog.csdn.net/gamer_gyt/article/details/78467021" target="_blank" rel="external">回归分析之Sklearn实现电力预测</a></p><h1 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h1><h2 id="数学表达式和图形"><a href="#数学表达式和图形" class="headerlink" title="数学表达式和图形"></a>数学表达式和图形</h2><p>这里选用Sigmoid函数（海维赛德阶跃函数）作为LR的模型函数，是因为在二分类情况下输出为0和1，其函数的数学表达式为：</p><script type="math/tex; mode=display">f(x)=\frac{1}{1+e^{-x}}</script><p>其图形为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import math</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">def sigmoid(x):</span><br><span class="line">  return 1 / (1 + math.exp(-x))</span><br><span class="line"></span><br><span class="line"># python2 中range生成的是一个数组，py3中生成的是一个迭代器，可以使用list进行转换</span><br><span class="line">X = list(range(-9,10))</span><br><span class="line">Y = list(map(sigmoid,X))</span><br><span class="line"></span><br><span class="line">#画图</span><br><span class="line">plt.plot(X,Y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><center>![sigmoid](http://img.blog.csdn.net/20171219192329328?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)</center><p>从上图可以看到sigmoid函数是一个s形的曲线，它的取值在[0, 1]之间，在远离0的地方函数的值会很快接近0/1。</p><h2 id="LR为什么要使用Sigmoid"><a href="#LR为什么要使用Sigmoid" class="headerlink" title="LR为什么要使用Sigmoid"></a>LR为什么要使用Sigmoid</h2><p>首先Sigmoid函数满足LR模型的需要，在自变量小于0时，因变量小于0.5，自变量大于0时，因变量大于0.5。<br>以下一段引用 <a href="https://tech.meituan.com/intro_to_logistic_regression.html" target="_blank" rel="external">Logistic Regression 模型简介</a> 的介绍对逻辑回归是一种概率模型做了阐述。</p><blockquote><p>逻辑回归是一种判别模型，表现为直接对条件概率P(y|x)建模，而不关心背后的数据分布P(x,y)。而高斯贝叶斯模型（Gaussian Naive Bayes）是一种生成模型，先对数据的联合分布建模，再通过贝叶斯公式来计算样本属于各个类别的后验概率，即：</p><script type="math/tex; mode=display">p(y|x) = \frac{P(x|y)P(y)}{\sum{P(x|y)P(y)}}</script><p>通常假设P(x|y)是高斯分布，P(y)是多项式分布，相应的参数都可以通过最大似然估计得到。如果我们考虑二分类问题，通过简单的变化可以得到：</p><script type="math/tex; mode=display">\log\frac{P(y=1|x)}{P(y=0|x)} = \log\frac{P(x|y=1)}{P(x|y=0)} + \log\frac{P(y=1)}{P(y=0)}  \ = -\frac{(x-\mu_1)^2}{2\sigma_1^2} + \frac{(x-\mu_0)^2}{2\sigma_0^2}\ + \theta_0</script><p>如果 σ1=σ0，二次项会抵消，我们得到一个简单的线性关系：</p><script type="math/tex; mode=display">\log\frac{P(y=1|x)}{P(y=0|x)} = \theta^T x</script><p>由上式进一步可以得到：</p><script type="math/tex; mode=display">P(y=1|x) = \frac{e^{\theta^T x}}{1+e^{\theta^T x}} = \frac{1}{1+e^{-\theta^T x}}</script><p>可以看到，这个概率和逻辑回归中的形式是一样的。这种情况下GNB 和 LR 会学习到同一个模型。实际上，在更一般的假设（P(x|y)的分布属于指数分布族）下，我们都可以得到类似的结论。</p></blockquote><p>至于逻辑回归为什么采用Sigmoid函数作为激活函数，因为LR是在伯努利分布和广义线性模型的假设下推导而来的，伯努利分布是属于指数分布簇的。</p><p>具体的推导可以参考 <a href="https://blog.csdn.net/u011467621/article/details/48197943" target="_blank" rel="external">机器学习算法之：指数族分布与广义线性模型</a> 的介绍</p><h1 id="Logistic-Regression-分类器"><a href="#Logistic-Regression-分类器" class="headerlink" title="Logistic Regression 分类器"></a>Logistic Regression 分类器</h1><p>即决策函数。<br>一个机器学习的模型，实际上是把决策函数限定在某一组条件下，这组限定条件就决定了模型的假设空间。当然，我们还希望这组限定条件简单而合理。而逻辑回归模型所做的假设是：</p><script type="math/tex; mode=display">P(y=1|x;\theta) = g(\theta^T x) = \frac{1}{1 + e ^ {-\theta^T * x}}</script><p>这里的 g(h)是上边提到的 sigmoid 函数$\theta$ 表示的是一组参数，$\theta^T $表示参数的转置矩阵，$\theta^T x$其实表示的就是$\theta_{0}x_{0} + \theta_{1}x_{1} + \theta_{2} x_{2}….$，相应的决策函数为：</p><script type="math/tex; mode=display">y^* = 1, \, \textrm{if} \, P(y=1|x) > 0.5</script><p>选择0.5作为阈值是一个一般的做法，实际应用时特定的情况可以选择不同阈值，如果对正例的判别准确性要求高，可以选择阈值大一些，对正例的召回要求高，则可以选择阈值小一些。</p><h1 id="参数求解"><a href="#参数求解" class="headerlink" title="参数求解"></a>参数求解</h1><p>确定好决策函数之后就是求解参数了。</p><p>模型的数学形式确定后，剩下就是如何去求解模型中的参数。统计学中常用的一种方法是最大似然估计，即找到一组参数，使得在这组参数下，我们的数据的似然度（概率）越大。在逻辑回归模型中，似然度可表示为：</p><script type="math/tex; mode=display">L(\theta) = P(D|\theta) = \prod P(y|x;\theta) = \prod g(\theta^T x) ^ y (1-g(\theta^T x))^{1-y}</script><p>取对数可以得到对数似然度：</p><script type="math/tex; mode=display">l(\theta) = \sum {y\log{g(\theta^T x)} + (1-y)\log{(1-g(\theta^T x))}}</script><p>另一方面，在机器学习领域，我们更经常遇到的是损失函数的概念，其衡量的是模型预测错误的程度。常用的损失函数有0-1损失，log损失，hinge损失等。其中log损失在单个数据点上的定义为</p><script type="math/tex; mode=display">-y\log{p(y|x)}-(1-y)\log{1-p(y|x)}</script><p>如果取整个数据集上的平均log损失，我们可以得到</p><script type="math/tex; mode=display">J(\theta) = -\frac{1}{N} l(\theta)</script><p>即在逻辑回归模型中，我们最大化似然函数和最小化log损失函数实际上是等价的。对于该优化问题，存在多种求解方法，这里以梯度下降的为例说明。梯度下降(Gradient Descent)又叫作最速梯度下降，是一种迭代求解的方法，通过在每一步选取使目标函数变化最快的一个方向调整参数的值来逼近最优值。基本步骤如下：</p><ul><li>选择下降方向（梯度方向，$\nabla {J(\theta)}$）</li><li>选择步长，更新参数 $\theta^i = \theta^{i-1} - \alpha^i \nabla {J(\theta^{i-1})}$</li><li>重复以上两步直到满足终止条件</li></ul><p><img src="https://tech.meituan.com/img/lr_intro/gradient_descent.png" alt="此处输入图片的描述"></p><p>其中损失函数的梯度计算方法为：</p><script type="math/tex; mode=display">\frac{\partial{J}}{\partial{\theta}} = -\frac{1}{n}\sum_i (y_i - y_i^*)x_i + \lambda \theta</script><p>沿梯度负方向选择一个较小的步长可以保证损失函数是减小的，另一方面，逻辑回归的损失函数是凸函数（加入正则项后是严格凸函数），可以保证我们找到的局部最优值同时是全局最优。此外，常用的凸优化的方法都可以用于求解该问题。例如共轭梯度下降，牛顿法，LBFGS等。</p><h1 id="分类边界"><a href="#分类边界" class="headerlink" title="分类边界"></a>分类边界</h1><p>知道如何求解参数后，我们来看一下模型得到的最后结果是什么样的。很容易可以从sigmoid函数看出，当$\theta^T x &gt; 0$ 时，y=1，否则 y=0。$\theta^T x = 0$ 是模型隐含的分类平面（在高维空间中，我们说是超平面）。所以说逻辑回归本质上是一个线性模型，但是，这不意味着只有线性可分的数据能通过LR求解，实际上，我们可以通过特征变换的方式把低维空间转换到高维空间，而在低维空间不可分的数据，到高维空间中线性可分的几率会高一些。下面两个图的对比说明了线性分类曲线和非线性分类曲线（通过特征映射）。<br><img src="https://tech.meituan.com/img/lr_intro/decision_boundary_1.png" alt="此处输入图片的描述"><br><img src="https://tech.meituan.com/img/lr_intro/decision_boundary_2.png" alt="此处输入图片的描述"><br>左图是一个线性可分的数据集，右图在原始空间中线性不可分，但是在特征转换 $[x_1, x_2] =&gt; [x_1, x_2, x_1^2, x_2^2, x_1x_2]$ 后的空间是线性可分的，对应的原始空间中分类边界为一条类椭圆曲线。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>逻辑回归的数学模型和求解都相对比较简洁，实现相对简单。通过对特征做离散化和其他映射，逻辑回归也可以处理非线性问题，是一个非常强大的分类器。因此在实际应用中，当我们能够拿到许多低层次的特征时，可以考虑使用逻辑回归来解决我们的问题。</p><hr><h1 id="资料参考"><a href="#资料参考" class="headerlink" title="资料参考"></a>资料参考</h1><p><a href="http://blog.csdn.net/han_xiaoyang/article/details/49123419" target="_blank" rel="external">http://blog.csdn.net/han_xiaoyang/article/details/49123419</a></p><p><a href="https://www.cnblogs.com/sxron/p/5489214.html" target="_blank" rel="external">https://www.cnblogs.com/sxron/p/5489214.html</a></p><p><a href="https://tech.meituan.com/intro_to_logistic_regression.html" target="_blank" rel="external">https://tech.meituan.com/intro_to_logistic_regression.html</a></p><hr><center>    <img src="http://img.blog.csdn.net/20180204165223361?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="30%" height="30%">    <br>打开微信扫一扫，加入数据与算法交流大群</center><center>    <img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">    <br>打开微信扫一扫，关注公众号【数据与算法联盟】</center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;逻辑回归与线性回归本质上是一样的，都是通过误差函数求解得到最优的系数，在形式上只不过是在线性回归上套了一个逻辑函数。线性回归的相关知识可以参考上边的给出链接文章，与线性回归相比，逻辑回归（Logistic Regression）更适用于因变量为二分
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="回归分析" scheme="http://thinkgamer.cn/tags/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90/"/>
    
      <category term="逻辑回归" scheme="http://thinkgamer.cn/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow的MNIST学习</title>
    <link href="http://thinkgamer.cn/2018/04/22/TensorFlow/TensorFlow%E7%9A%84MNIST%E5%AD%A6%E4%B9%A0/"/>
    <id>http://thinkgamer.cn/2018/04/22/TensorFlow/TensorFlow的MNIST学习/</id>
    <published>2018-04-22T07:21:28.000Z</published>
    <updated>2019-04-13T04:39:50.178Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>MNIST是在机器学习领域中的一个经典问题。该问题解决的是把28x28像素的灰度手写数字图片识别为相应的数字，其中数字的范围从0到9.</p></blockquote><hr><p>转载请注明出处：<a href="https://blog.csdn.net/gamer_gyt/article/details/80039242" target="_blank" rel="external">https://blog.csdn.net/gamer_gyt/article/details/80039242</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><div class="table-container"><table><thead><tr><th>数据集</th><th>目的</th></tr></thead><tbody><tr><td>data_sets.train</td><td>55000 组 图片和标签, 用于训练。</td></tr><tr><td>data_sets.validation</td><td>5000 组 图片和标签, 用于迭代验证训练的准确性。</td></tr><tr><td>data_sets.test</td><td>10000 组 图片和标签, 用于最终测试训练的准确性。</td></tr></tbody></table></div><h1 id="数据集简介"><a href="#数据集简介" class="headerlink" title="数据集简介"></a>数据集简介</h1><p>MNIST数据集加载有两种办法，第一是直接从网上下载，第二是下载到本地进行load（跟第一种类似，只不过是事先下载好，从本地进行加载）。从网上下载到本地方式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 加载mnist数据集</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line">print(&quot;load finish&quot;)</span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;,one_hot=True)</span><br><span class="line">print(type(mnist))</span><br></pre></td></tr></table></figure><p>输出为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">load finish</span><br><span class="line">Extracting MNIST_data/train-images-idx3-ubyte.gz</span><br><span class="line">Extracting MNIST_data/train-labels-idx1-ubyte.gz</span><br><span class="line">Extracting MNIST_data/t10k-images-idx3-ubyte.gz</span><br><span class="line">Extracting MNIST_data/t10k-labels-idx1-ubyte.gz</span><br><span class="line">&lt;class &apos;tensorflow.contrib.learn.python.learn.datasets.base.Datasets&apos;&gt;</span><br></pre></td></tr></table></figure></p><hr><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;MNIST 训练集数据条数：&quot; ,mnist.train.num_examples)</span><br><span class="line">print(&quot;MNIST 测试集数据条数：&quot; ,mnist.test.num_examples)</span><br><span class="line"></span><br><span class="line">train_img = mnist.train.images</span><br><span class="line">train_label = mnist.train.labels</span><br><span class="line">print(&quot;训练集类型：&quot;,type(train_img))</span><br><span class="line">print(&quot;训练集维度：&quot;,train_img.shape)</span><br><span class="line"></span><br><span class="line">test_img = mnist.test.images</span><br><span class="line">test_label = mnist.test.labels</span><br><span class="line">print(&quot;测试集类型：&quot;,type(test_img))</span><br><span class="line">print(&quot;测试集维度：&quot;,test_img.shape)</span><br></pre></td></tr></table></figure><p>输出为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">MNIST 训练集数据条数： 55000</span><br><span class="line">MNIST 测试集数据条数： 10000</span><br><span class="line">训练集类型： &lt;class &apos;numpy.ndarray&apos;&gt;</span><br><span class="line">训练集维度： (55000, 784)</span><br><span class="line">测试集类型： &lt;class &apos;numpy.ndarray&apos;&gt;</span><br><span class="line">测试集维度： (10000, 784)</span><br></pre></td></tr></table></figure></p><p>打开当前运行代码的目录，我们会发现一个MNIST_data的文件夹，里边包含的文件如下：<br>文件    | 内容<br>—|—<br>train-images-idx3-ubyte.gz |    训练集图片 - 55000 张 训练图片, 5000 张 验证图片<br>train-labels-idx1-ubyte.gz |    训练集图片对应的数字标签<br>t10k-images-idx3-ubyte.gz |    测试集图片 - 10000 张 图片<br>t10k-labels-idx1-ubyte.gz |    测试集图片对应的数字标签</p><p>使用next_batch函数加载指定条数的数据集<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 关于next_batch函数</span><br><span class="line">batchSize = 100</span><br><span class="line">batch_x,batch_y = mnist.train.next_batch(batch_size=batchSize)</span><br><span class="line">print(batch_x.shape)</span><br><span class="line">print(batch_y.shape)</span><br></pre></td></tr></table></figure></p><p>输出为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(100, 784)</span><br><span class="line">(100, 10)</span><br></pre></td></tr></table></figure></p><hr><center>    <img src="http://img.blog.csdn.net/20180204165223361?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%" height="50%">打开微信扫一扫，加入数据与算法交流大群</center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;MNIST是在机器学习领域中的一个经典问题。该问题解决的是把28x28像素的灰度手写数字图片识别为相应的数字，其中数字的范围从0到9.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;转载请注明出处：&lt;a href=&quot;https://blog.
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="深度学习" scheme="http://thinkgamer.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="TensorFlow" scheme="http://thinkgamer.cn/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow实现线性回归</title>
    <link href="http://thinkgamer.cn/2018/04/22/TensorFlow/TensorFlow%E5%AE%9E%E7%8E%B0%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://thinkgamer.cn/2018/04/22/TensorFlow/TensorFlow实现线性回归/</id>
    <published>2018-04-22T06:27:28.000Z</published>
    <updated>2019-04-13T04:39:48.163Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>TensorFlow写简单的代码是大财小用，需要很繁琐的代码方能实现简单的功能，但对于复杂的机器学习算法和深度神经网络却是十分的简单，下边看一个tf实现线性回归的Demo</p></blockquote><hr><p>转载请注明出处：<a href="https://blog.csdn.net/gamer_gyt/article/details/80038679" target="_blank" rel="external">https://blog.csdn.net/gamer_gyt/article/details/80038679</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><p>首先引入tf包的相关的package，并初始化100个点。这里我们假定线性回归中的w为0.1 b为0.3，并设置一个随机数保证在y=0.1X + 0.3 上下浮动。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># coding: utf-8</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">create by: Thinkgamer</span><br><span class="line">create time: 2018/04/22</span><br><span class="line">desc: 使用tensorflow创建线性回归模型</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">import numpy as np</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"># 随机生成100个点</span><br><span class="line">num_points = 100</span><br><span class="line">vectors_set = list()</span><br><span class="line">for i in range(num_points):</span><br><span class="line">    x1 = np.random.normal(0.00,00.55)</span><br><span class="line">    y1 = x1* 0.1 + 0.3 + np.random.normal(0.0,0.03)</span><br><span class="line">    vectors_set.append([x1,y1])</span><br><span class="line"></span><br><span class="line"># 生成一些样本</span><br><span class="line">x_data = [v[0] for v in vectors_set]</span><br><span class="line">y_data = [v[1] for v in vectors_set]</span><br><span class="line"># print(x_data)</span><br><span class="line"># print(y_data)</span><br><span class="line">plt.scatter(x_data,y_data,c=&apos;r&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>生成的图如下所示：<br><img src="https://img-blog.csdn.net/20180422142414284?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>用刚才生成的数据进行线性回归拟合</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># 构造线性回归模型</span><br><span class="line"></span><br><span class="line"># 生成一维的W矩阵，取值是[-1,1]之间的随机数</span><br><span class="line">W = tf.Variable(tf.random_uniform([1],-1,1),name=&apos;W&apos;)</span><br><span class="line"># 生成一维的b矩阵，初始值为0</span><br><span class="line">b = tf.Variable(tf.zeros([1]),name=&apos;b&apos;)</span><br><span class="line"># 经过计算得出预估值y</span><br><span class="line">y=W*x_data + b</span><br><span class="line"></span><br><span class="line"># 定义损失函数，以预估值y和y_data之间的均方误差作为损失</span><br><span class="line">loss = tf.reduce_mean(tf.square(y - y_data),name=&apos;loss&apos;)</span><br><span class="line"># 采用地图下降算法来优化参数</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(0.5)</span><br><span class="line"># 训练过程就是最小化误差</span><br><span class="line">train = optimizer.minimize(loss,name=&apos;train&apos;)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"># 初始化W 和 b</span><br><span class="line">print(&quot;初始化值：  W = &quot;,sess.run(W), &quot;b= &quot;,sess.run(b))</span><br><span class="line">for step in range(20):</span><br><span class="line">    sess.run(train)</span><br><span class="line">    # 打印出每次训练后的w和b</span><br><span class="line">    print(&quot;第 %s 步：  W = &quot; % step,sess.run(W), &quot;b= &quot;,sess.run(b))</span><br><span class="line">    </span><br><span class="line"># 展示</span><br><span class="line">plt.scatter(x_data,y_data,c=&apos;r&apos;)</span><br><span class="line">plt.plot(x_data,sess.run(W)*x_data+sess.run(b),c=&apos;b&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>对应的输出结果为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">初始化值：  W =  [-0.300848] b=  [0.]</span><br><span class="line">第 0 步：  W =  [-0.17744449] b=  [0.3065566]</span><br><span class="line">第 1 步：  W =  [-0.09421768] b=  [0.30550653]</span><br><span class="line">第 2 步：  W =  [-0.03631891] b=  [0.3047983]</span><br><span class="line">第 3 步：  W =  [0.0039596] b=  [0.3043056]</span><br><span class="line">第 4 步：  W =  [0.0319802] b=  [0.3039629]</span><br><span class="line">第 5 步：  W =  [0.05147332] b=  [0.30372444]</span><br><span class="line">第 6 步：  W =  [0.06503413] b=  [0.30355856]</span><br><span class="line">第 7 步：  W =  [0.07446799] b=  [0.30344316]</span><br><span class="line">第 8 步：  W =  [0.08103085] b=  [0.30336288]</span><br><span class="line">第 9 步：  W =  [0.08559645] b=  [0.30330706]</span><br><span class="line">第 10 步：  W =  [0.0887726] b=  [0.3032682]</span><br><span class="line">第 11 步：  W =  [0.09098216] b=  [0.30324116]</span><br><span class="line">第 12 步：  W =  [0.09251929] b=  [0.30322236]</span><br><span class="line">第 13 步：  W =  [0.09358863] b=  [0.30320928]</span><br><span class="line">第 14 步：  W =  [0.09433253] b=  [0.3032002]</span><br><span class="line">第 15 步：  W =  [0.09485004] b=  [0.30319384]</span><br><span class="line">第 16 步：  W =  [0.09521006] b=  [0.30318946]</span><br><span class="line">第 17 步：  W =  [0.09546052] b=  [0.3031864]</span><br><span class="line">第 18 步：  W =  [0.09563475] b=  [0.30318424]</span><br><span class="line">第 19 步：  W =  [0.09575596] b=  [0.30318278]</span><br></pre></td></tr></table></figure></p><p>生成的图如下：<br><img src="https://img-blog.csdn.net/20180422142454345?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><hr><center>    <img src="http://img.blog.csdn.net/20180204165223361?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%" height="50%">打开微信扫一扫，加入数据与算法交流大群</center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;TensorFlow写简单的代码是大财小用，需要很繁琐的代码方能实现简单的功能，但对于复杂的机器学习算法和深度神经网络却是十分的简单，下边看一个tf实现线性回归的Demo&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;转载请注明出处：&lt;a h
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="深度学习" scheme="http://thinkgamer.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="TensorFlow" scheme="http://thinkgamer.cn/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow安装、变量学习和常用操作</title>
    <link href="http://thinkgamer.cn/2018/04/17/TensorFlow/TensorFlow%E5%AE%89%E8%A3%85%E3%80%81%E5%8F%98%E9%87%8F%E5%AD%A6%E4%B9%A0%E5%92%8C%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/"/>
    <id>http://thinkgamer.cn/2018/04/17/TensorFlow/TensorFlow安装、变量学习和常用操作/</id>
    <published>2018-04-16T16:55:28.000Z</published>
    <updated>2019-04-13T04:39:45.467Z</updated>
    
    <content type="html"><![CDATA[<h1 id="安装、入门"><a href="#安装、入门" class="headerlink" title="安装、入门"></a>安装、入门</h1><p>环境说明：</p><ul><li>deepin 15.4</li><li>python 3.5.4</li><li>tensorflow 1.7.0</li></ul><hr><p>转载请注明出处：<a href="https://blog.csdn.net/gamer_gyt/article/details/79968940" target="_blank" rel="external">https://blog.csdn.net/gamer_gyt/article/details/79968940</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><p>安装：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install https://pypi.python.org/packages/dd/ed/9e6c6c16ff50be054277438669542555a166ed9f95a0dcaacff24fd3153a/tensorflow-1.7.0rc1-cp35-cp35m-manylinux1_x86_64.whl#md5=ec44ad9b0d040caef8ca0fac5b822b0d</span><br></pre></td></tr></table></figure></p><p>测试：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import tensorflow as tf</span><br><span class="line">&gt;&gt;&gt; hello = tf.constant(&quot;hello , Thinkgamer&quot;)</span><br><span class="line">&gt;&gt;&gt; with tf.Session() as sess:</span><br><span class="line">...     print(sess.run(hello))</span><br><span class="line">...     sess.close()</span><br><span class="line">... </span><br><span class="line">2018-03-28 00:50:16.728894: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</span><br><span class="line">b&apos;hello , Thinkgamer&apos;</span><br></pre></td></tr></table></figure></p><p>使用 TensorFlow, 你必须明白 TensorFlow:</p><ul><li>使用图 (graph) 来表示计算任务.</li><li>在被称之为 会话 (Session) 的上下文 (context) 中执行图.</li><li>使用 tensor 表示数据.</li><li>通过 变量 (Variable) 维护状态.</li><li>使用 feed 和 fetch 可以为任意的操作(arbitrary operation) 赋值或者从其中获取数据.</li></ul><p><strong>session和InteractiveSession的区别：</strong></p><p>session：启动图的操作，有run()、close()<br>InteractiveSession：默认会话<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; sess = tf.InteractiveSession()</span><br><span class="line">&gt;&gt;&gt; a = tf.constant(5.0)</span><br><span class="line">&gt;&gt;&gt; b = tf.constant(6.0)</span><br><span class="line">&gt;&gt;&gt; c = a * b</span><br><span class="line">&gt;&gt;&gt; print(c.eval())</span><br><span class="line">30.0</span><br><span class="line">&gt;&gt;&gt; sess.close()</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">&gt;&gt;&gt; sess = tf.Session()</span><br><span class="line">&gt;&gt;&gt; a = tf.constant(5.0)</span><br><span class="line">&gt;&gt;&gt; b = tf.constant(6.0)</span><br><span class="line">&gt;&gt;&gt; c = a * b</span><br><span class="line">&gt;&gt;&gt; sess.run(c)</span><br><span class="line">30.0</span><br><span class="line">&gt;&gt;&gt; sess.close()</span><br></pre></td></tr></table></figure></p><h1 id="Tensorflow之变量"><a href="#Tensorflow之变量" class="headerlink" title="Tensorflow之变量"></a>Tensorflow之变量</h1><h2 id="tensor的理解"><a href="#tensor的理解" class="headerlink" title="tensor的理解"></a>tensor的理解</h2><p>tensor即张量，tf中所有的数据通过数据流进行传输，可以声明任何一个张量，只有当一个张量进行run的时候，这个张量所涉及的tensor便会触发，这一点和spark的DAG很是相似，在同一条链上的某个节点进行触发操作时，该节点之前的所有节点便会参与计算。</p><p>在tensorflow中，张量的维数被描述为“阶”，张量是以list的形式存储的。list有几重中括号，对应的张量就是几阶。如t=[ [1,2,3],[4,5,6],[7,8,9] ]，t就是一个二阶张量。</p><p>我们可以认为，一阶张量，如[1,2,3]，相当于一个向量，二阶张量，如[ [1,2,3],[4,5,6],[7,8,9] ]，相当于一个矩阵。</p><p>对于t=[ [1,2,3],[4,5,6],[7,8,9] ]来说，它的shape==&gt;[3,3]，shape可以理解成：当脱去最外层的一对中括号后，里面有3个小list，然后每个小list里又有3个元素，所以该张量的shape==&gt;[3,3]。</p><p>举几个例子，如[ [1,2,3],[4,5,6] ] 的shape=[2,3](因为当脱去最外层的一对中括号后，里面有2个小list，然后每个小list里又有3个元素，所以该张量的shape==&gt;[2,3]。）</p><p>又如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">    [ </span><br><span class="line">        [ [ 2 ], [ 2 ] ] ,</span><br><span class="line">        [ [ 2 ], [ 2 ] ] , </span><br><span class="line">        [ [ 2 ], [ 2 ] ] </span><br><span class="line">    ] , </span><br><span class="line">    [ </span><br><span class="line">        [ [ 2 ], [ 2 ] ] , </span><br><span class="line">        [ [ 2 ], [ 2 ] ] , </span><br><span class="line">        [ [ 2 ], [ 2 ] ] </span><br><span class="line">    ] ,</span><br><span class="line">    [ </span><br><span class="line">        [ [ 2 ], [ 2 ] ] , </span><br><span class="line">        [ [ 2 ], [ 2 ] ] , </span><br><span class="line">        [ [ 2 ], [ 2 ] ]</span><br><span class="line">    ] ,</span><br><span class="line">    [ </span><br><span class="line">        [ [ 2 ], [ 2 ] ] ,</span><br><span class="line">        [ [ 2 ], [ 2 ] ] ,</span><br><span class="line">        [ [ 2 ], [ 2 ] ] </span><br><span class="line">    ] </span><br><span class="line">]</span><br></pre></td></tr></table></figure></p><p>的shape==&gt;[4,3,2,1] (因为当脱去最外层的一对中括号后，里面有4个第二大的list，每个第二大的list里又有3个第三大的list，每个第三大的list里有2个第四大的list，每个第四大的list里有1个元素，所以该张量的shape==&gt;[4,3,2,1]。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#coding: utf-8</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">create by: thinkgamer</span><br><span class="line">create time: 2018/04/16</span><br><span class="line">description: 关于tensorflow变量的学习</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">a = tf.Variable([ [1,2,3],[4,5,6],[7,8,9] ])</span><br><span class="line">b = tf.Variable([ [1,2,3],[4,5,6] ])</span><br><span class="line">c = tf.Variable([[ [ [ 2 ], [ 2 ] ] ,[ [ 2 ], [ 2 ] ] , [ [ 2 ], [ 2 ] ] ] ,</span><br><span class="line">                 [ [ [ 2 ], [ 2 ] ] ,[ [ 2 ], [ 2 ] ] , [ [ 2 ], [ 2 ] ] ] ,</span><br><span class="line">                 [ [ [ 2 ], [ 2 ] ] ,[ [ 2 ], [ 2 ] ] , [ [ 2 ], [ 2 ] ] ] ,</span><br><span class="line">                 [ [ [ 2 ], [ 2 ] ] ,[ [ 2 ], [ 2 ] ] , [ [ 2 ], [ 2 ] ] ] ])</span><br><span class="line">init_op = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">print(&quot;a shape: &quot;,a.eval)</span><br><span class="line">print(&quot;b shape: &quot;,b.eval)</span><br><span class="line">print(&quot;c shape: &quot;,c.eval)</span><br></pre></td></tr></table></figure><p>打印出的结果如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a shape:  &lt;bound method Variable.eval of &lt;tf.Variable &apos;Variable_19:0&apos; shape=(3, 3) dtype=int32_ref&gt;&gt;</span><br><span class="line">b shape:  &lt;bound method Variable.eval of &lt;tf.Variable &apos;Variable_20:0&apos; shape=(2, 3) dtype=int32_ref&gt;&gt;</span><br><span class="line">c shape:  &lt;bound method Variable.eval of &lt;tf.Variable &apos;Variable_21:0&apos; shape=(4, 3, 2, 1) dtype=int32_ref&gt;&gt;</span><br></pre></td></tr></table></figure></p><h2 id="tf实现乘法"><a href="#tf实现乘法" class="headerlink" title="tf实现乘法"></a>tf实现乘法</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># tf 实现矩阵乘法</span><br><span class="line">val1 = tf.Variable([[1,2]])</span><br><span class="line">val2 = tf.Variable([[1],[2]])</span><br><span class="line">result1 = tf.matmul(val1,val2)</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(result1))</span><br><span class="line">    sess.close()</span><br></pre></td></tr></table></figure><h2 id="tf的常用操作"><a href="#tf的常用操作" class="headerlink" title="tf的常用操作"></a>tf的常用操作</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"># tf的常用操作，建议变量以float32为主,CPU,GPU均支持，否则容易出现一些错误,float32需要从tf中引入</span><br><span class="line">from tensorflow import float32</span><br><span class="line"></span><br><span class="line"># 创建一个shape=（3,4）的tensor</span><br><span class="line">t1 = tf.zeros([3,4],float32)</span><br><span class="line"></span><br><span class="line"># 创建一个shape类似于tensor的tensor，值分为全为0或1</span><br><span class="line">tensor = tf.Variable([ [1,2,3],[4,5,6],[7,8,9] ])</span><br><span class="line">t2 = tf.zeros_like(tensor)</span><br><span class="line">t3 = tf.ones_like(tensor)</span><br><span class="line"></span><br><span class="line"># 创建tensorflow支持的常量</span><br><span class="line">t4 = tf.constant([1,2,3,4])</span><br><span class="line"></span><br><span class="line"># 创建常量，指定值全为-1</span><br><span class="line">t5 = tf.constant(-1.0,shape=[2,3])</span><br><span class="line"></span><br><span class="line"># 创建数组</span><br><span class="line">t6 = tf.linspace(1.0,6.0,3,name=&quot;linspace&quot;)</span><br><span class="line"># start=1, limit=9, delta=3</span><br><span class="line">t7 = tf.range(1,9,3)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(&quot;t1: &quot;,sess.run(t1))</span><br><span class="line">    print(&quot;\ntermsor: &quot;,sess.run(tensor))</span><br><span class="line">    print(&quot;\nt2: &quot;,sess.run(t2))</span><br><span class="line">    print(&quot;\nt3: &quot;,sess.run(t3))</span><br><span class="line">    print(&quot;\nt4: &quot;,sess.run(t4))</span><br><span class="line">    print(&quot;\nt5: &quot;,sess.run(t5))</span><br><span class="line">    print(&quot;\nt6: &quot;,sess.run(t6))</span><br><span class="line">    print(&quot;\nt7: &quot;,sess.run(t7))</span><br><span class="line">sess.close()</span><br><span class="line"></span><br><span class="line">#-----------------------------------------</span><br><span class="line"># tf创建符合指定正态分布的tensor</span><br><span class="line">t8 = tf.random_normal([2,3],mean=-1 ,stddev=4)</span><br><span class="line"></span><br><span class="line"># tf shuffle</span><br><span class="line">t9 = tf.constant([[1,2],[3,4],[5,6]])</span><br><span class="line">t10 = tf.random_shuffle(t9)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(&quot;\nt8:&quot;,sess.run(t8))</span><br><span class="line">print(&quot;\nt9:&quot;,sess.run(t9))</span><br><span class="line">print(&quot;\nt10&quot;,sess.run(t10))</span><br></pre></td></tr></table></figure><p>打印出的值为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">t1:  [[0. 0. 0. 0.]</span><br><span class="line"> [0. 0. 0. 0.]</span><br><span class="line"> [0. 0. 0. 0.]]</span><br><span class="line"></span><br><span class="line">termsor:  [[1 2 3]</span><br><span class="line"> [4 5 6]</span><br><span class="line"> [7 8 9]]</span><br><span class="line"></span><br><span class="line">t2:  [[0 0 0]</span><br><span class="line"> [0 0 0]</span><br><span class="line"> [0 0 0]]</span><br><span class="line"></span><br><span class="line">t3:  [[1 1 1]</span><br><span class="line"> [1 1 1]</span><br><span class="line"> [1 1 1]]</span><br><span class="line"></span><br><span class="line">t4:  [1 2 3 4]</span><br><span class="line"></span><br><span class="line">t5:  [[-1. -1. -1.]</span><br><span class="line"> [-1. -1. -1.]]</span><br><span class="line"></span><br><span class="line">t6:  [1.  3.5 6. ]</span><br><span class="line"></span><br><span class="line">t7:  [1 4 7]</span><br><span class="line"></span><br><span class="line">t8:  [[  0.8365586   5.152416   -3.9977255]</span><br><span class="line"> [ -1.7592524 -12.675492   -5.949805 ]]</span><br><span class="line"></span><br><span class="line">t9:  [[1 2]</span><br><span class="line"> [3 4]</span><br><span class="line"> [5 6]]</span><br><span class="line"></span><br><span class="line">t10:  [[3 4]</span><br><span class="line"> [5 6]</span><br><span class="line"> [1 2]]</span><br><span class="line">In [ ]:</span><br></pre></td></tr></table></figure></p><h2 id="tf实现i"><a href="#tf实现i" class="headerlink" title="tf实现i++"></a>tf实现i++</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># tf 实现i++</span><br><span class="line">state= tf.Variable(1)</span><br><span class="line">new_value = tf.add(state,tf.Variable(1))</span><br><span class="line">update = tf.assign(state,new_value)</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(state))</span><br><span class="line">    for _ in range(3):</span><br><span class="line">        sess.run(update)</span><br><span class="line">        print(sess.run(state))</span><br></pre></td></tr></table></figure><p>打印结果为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td></tr></table></figure></p><h2 id="numpy转tensor"><a href="#numpy转tensor" class="headerlink" title="numpy转tensor"></a>numpy转tensor</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># numpy 转化为 tensor</span><br><span class="line">import numpy as np</span><br><span class="line">np1 = np.zeros((3,3))</span><br><span class="line">ta = tf.convert_to_tensor(np1)</span><br><span class="line">with tf.Session() as tf:</span><br><span class="line">    print(sess.run(ta))</span><br></pre></td></tr></table></figure><h1 id="测试遇到的问题"><a href="#测试遇到的问题" class="headerlink" title="测试遇到的问题"></a>测试遇到的问题</h1><h2 id="1：tensorFlow与python版本不适应"><a href="#1：tensorFlow与python版本不适应" class="headerlink" title="1：tensorFlow与python版本不适应"></a>1：tensorFlow与python版本不适应</h2><p>现象：NameError: name ‘XXX’ is not defined<br>原因是：当前tensorflow版本与python版本不适应。<br>解决办法：重新安装tf<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pip3 install --upgrade tensorflow</span><br><span class="line"></span><br><span class="line">------------</span><br><span class="line">Collecting tensorflow</span><br><span class="line">  Downloading tensorflow-1.6.0-cp35-cp35m-manylinux1_x86_64.whl (45.8MB)</span><br></pre></td></tr></table></figure></p><h2 id="2：变量未初始化"><a href="#2：变量未初始化" class="headerlink" title="2：变量未初始化"></a>2：变量未初始化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable_58</span><br></pre></td></tr></table></figure><p>解决办法：将变量进行初始化<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">tf.Session().run(init)</span><br></pre></td></tr></table></figure></p><hr><center>    <img src="http://img.blog.csdn.net/20180204165223361?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="50%" height="50%">打开微信扫一扫，加入数据与算法交流大群</center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;安装、入门&quot;&gt;&lt;a href=&quot;#安装、入门&quot; class=&quot;headerlink&quot; title=&quot;安装、入门&quot;&gt;&lt;/a&gt;安装、入门&lt;/h1&gt;&lt;p&gt;环境说明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;deepin 15.4&lt;/li&gt;
&lt;li&gt;python 3.5.4&lt;/li&gt;

      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="深度学习" scheme="http://thinkgamer.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="TensorFlow" scheme="http://thinkgamer.cn/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>机器学习中的AUC理解</title>
    <link href="http://thinkgamer.cn/2018/04/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84AUC%E7%90%86%E8%A7%A3/"/>
    <id>http://thinkgamer.cn/2018/04/15/机器学习/机器学习中的AUC理解/</id>
    <published>2018-04-14T19:23:16.000Z</published>
    <updated>2019-04-13T04:37:08.183Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近在做GBDT模型，里边用到胡模型评价方法就是AUC，刚好趁此机会，好好学习一下。</p></blockquote><h1 id="混淆矩阵-Confusion-matrix"><a href="#混淆矩阵-Confusion-matrix" class="headerlink" title="混淆矩阵(Confusion matrix)"></a>混淆矩阵(Confusion matrix)</h1><p>混淆矩阵是理解大多数评价指标的基础，毫无疑问也是理解AUC的基础。丰富的资料介绍着混淆矩阵的概念，下边用一个实例讲解什么是混淆矩阵</p><hr><p>转载请注明出处：<a href="https://blog.csdn.net/gamer_gyt/article/details/79945987" target="_blank" rel="external">https://blog.csdn.net/gamer_gyt/article/details/79945987</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><p>如有100个样本数据，这些数据分成2类，每类50个。分类结束后得到的混淆矩阵为：<br><img src="https://img-blog.csdn.net/20180415002706538?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>说明：<br>40个为0类别的，预测正确，60个事实是0类别的给预测为1类别的<br>40个为1类别的，预测正确，60个事实是1类别的给预测为0类别的</p><p>其对应的混淆矩阵如下：<br><img src="https://img-blog.csdn.net/2018041500331853?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>混淆矩阵包含四部分的信息：</p><ul><li>True negative(TN)，称为真阴率，表明实际是负样本预测成负样本的样本数</li><li>False positive(FP)，称为假阳率，表明实际是负样本预测成正样本的样本数</li><li>False negative(FN)，称为假阴率，表明实际是正样本预测成负样本的样本数</li><li>True positive(TP)，称为真阳率，表明实际是正样本预测成正样本的样本数</li></ul><p>从上边的图可以分析出，对角线带True的为判断对了，斜对角线带False的为判断错了。</p><p>像常见的准确率，精准率，召回率，F1-score，AUC都是建立在混淆矩阵上的。</p><p>准确率（Accuracy）：判断正确的占总的数目的比例【（TN+TP）/100=(40+40)/100=0.8】<br>精准率（precision）：判断正确的正类占进行判断的数目的比例（针对判别结果而言的，表示预测为正的数据中有多少是真的正确）【TP/(TP+FP) = 40/(40+60 )=0.4】<br>召回率（recall）: 判断正确正类占应该应该判断正确的正类的比例（针对原样本而言，表示样本中的正例有多少被判断正确了）【TP/(TP+FN)=40/(40+60)=0.4】<br>F1-Measure：精确值和召回率的调和均值【2<em>R</em>R/(P+R)=2<em>0.4</em>0.4/(0.4+0.4)=1】</p><h1 id="AUC-amp-ROC"><a href="#AUC-amp-ROC" class="headerlink" title="AUC &amp; ROC"></a>AUC &amp; ROC</h1><p>AUC是一个模型评价指标，只能用于二分类模型的评价，对于二分类模型，还有损失函数（logloss），正确率（accuracy），准确率（precision），但相比之下AUC和logloss要比accuracy和precision用的多，原因是因为很多的机器学习模型计算结果都是概率的形式，那么对于概率而言，我们就需要去设定一个阈值来判定分类，那么这个阈值的设定就会对我们的正确率和准确率造成一定成都的影响。</p><p>AUC(Area under Curve)，表面上意思是曲线下边的面积，这么这条曲线是什么？——ROC曲线（receiver operating characteristic curve，接收者操作特征曲线）。</p><p>接下来分析下面这张图（图片来自百度百科）：<br><img src="https://img-blog.csdn.net/20180415015350726?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>X轴是假阳率：FP/(FP+TN)<br>Y轴是真阳性：TP(TP+FN)<br>ROC曲线给出的是当阈值(分类器必须提供每个样例被判为阳性或者阴性的可信程度值)变化时假阳率和真阳率的变化情况，左下角的点所对应的是将所有样例判为反例的情况，而右上角的点对应的则是将所有样例判断为正例的情况。<br>ROC曲线不但可以用于比较分类器，还可以基于成本效益分析来做出决策。在理想情况下，最佳的分类器应该尽可能地处于左上角，这就意味着分类器在假阳率很低的同时获得了很高的真阳率。</p><h1 id="AUC计算"><a href="#AUC计算" class="headerlink" title="AUC计算"></a>AUC计算</h1><p>以下部分引用 <a href="https://blog.csdn.net/guohecang/article/details/52858019" target="_blank" rel="external">ROC曲线与AUC计算</a> 中的举例</p><hr><p>假设有6次展示记录，有两次被点击了，得到一个展示序列（1:1,2:0,3:1,4:0,5:0,6:0），前面的表示序号，后面的表示点击（1）或没有点击（0）。<br>然后在这6次展示的时候都通过model算出了点击的概率序列。</p><p>下面看三种情况。</p><p>一、如果概率的序列是（1:0.9,2:0.7,3:0.8,4:0.6,5:0.5,6:0.4）。与原来的序列一起，得到序列（从概率从高到低排）<br>1 0.9<br>1 0.8<br>0 0.7<br>0 0.6<br>0 0.5<br>0 0.4<br>绘制的步骤是：<br>1）把概率序列从高到低排序，得到顺序（1:0.9,3:0.8,2:0.7,4:0.6,5:0.5,6:0.4）；<br>2）从概率最大开始取一个点作为正类，取到点1，计算得到TPR=0.5，FPR=0.0；<br>3）从概率最大开始，再取一个点作为正类，取到点3，计算得到TPR=1.0，FPR=0.0；<br>4）再从最大开始取一个点作为正类，取到点2，计算得到TPR=1.0，FPR=0.25;<br>5）以此类推，得到6对TPR和FPR。<br>然后把这6对数据组成6个点(0,0.5),(0,1.0),(0.25,1),(0.5,1),(0.75,1),(1.0,1.0)。<br>这6个点在二维坐标系中能绘出来。<br><img src="https://img-blog.csdn.net/20180415024426171?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>看看图中，那个就是ROC曲线。</p><p>二、如果概率的序列是（1:0.9,2:0.8,3:0.7,4:0.6,5:0.5,6:0.4）。与原来的序列一起，得到序列（从概率从高到低排）<br>1 0.9<br>0 0.8<br>1 0.7<br>0 0.6<br>0 0.5<br>0 0.4<br>绘制的步骤是：<br>6）把概率序列从高到低排序，得到顺序（1:0.9,2:0.8,3:0.7,4:0.6,5:0.5,6:0.4）；<br>7）从概率最大开始取一个点作为正类，取到点1，计算得到TPR=0.5，FPR=0.0；<br>8）从概率最大开始，再取一个点作为正类，取到点2，计算得到TPR=0.5，FPR=0.25；<br>9）再从最大开始取一个点作为正类，取到点3，计算得到TPR=1.0，FPR=0.25;<br>10）     以此类推，得到6对TPR和FPR。<br>然后把这6对数据组成6个点(0,0.5),(0.25,0.5),(0.25,1),(0.5,1),(0.75,1),(1.0,1.0)。<br>这6个点在二维坐标系中能绘出来。<br><img src="https://img-blog.csdn.net/20180415024524722?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>看看图中，那个就是ROC曲线。</p><p>三、如果概率的序列是（1:0.4,2:0.6,3:0.5,4:0.7,5:0.8,6:0.9）。与原来的序列一起，得到序列（从概率从高到低排）<br>0 0.9<br>0 0.8<br>0 0.7<br>0 0.6<br>1 0.5<br>1 0.4<br>绘制的步骤是：<br>11）把概率序列从高到低排序，得到顺序（1:0.4,2:0.6,3:0.5,4:0.7,5:0.8,6:0.9）；<br>12）从概率最大开始取一个点作为正类，取到点6，计算得到TPR=0.0，FPR=0.25；<br>13）从概率最大开始，再取一个点作为正类，取到点5，计算得到TPR=0.0，FPR=0.5；<br>14）再从最大开始取一个点作为正类，取到点4，计算得到TPR=0.0，FPR=0.75;<br>15）以此类推，得到6对TPR和FPR。<br>然后把这6对数据组成6个点(0.25,0.0),(0.5,0.0),(0.75,0.0),(1.0,0.0),(1.0,0.5),(1.0,1.0)。<br>这6个点在二维坐标系中能绘出来。<br><img src="https://img-blog.csdn.net/20180415024654878?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br>看看图中，那个就是ROC曲线</p><p>意义：<br>如上图的例子，总共6个点，2个正样本，4个负样本，取一个正样本和一个负样本的情况总共有8种。</p><p>上面的第一种情况，从上往下取，无论怎么取，正样本的概率总在负样本之上，所以分对的概率为1，AUC=1。再看那个ROC曲线，它的积分是什么？也是1，ROC曲线的积分与AUC相等。</p><p>上面第二种情况，如果取到了样本2和3，那就分错了，其他情况都分对了；所以分对的概率是0.875，AUC=0.875。再看那个ROC曲线，它的积分也是0.875，ROC曲线的积分与AUC相等。</p><p>上面的第三种情况，无论怎么取，都是分错的，所以分对的概率是0，AUC=0.0。再看ROC曲线，它的积分也是0.0，ROC曲线的积分与AUC相等。</p><p>很牛吧，其实AUC的意思是——Area Under roc Curve，就是ROC曲线的积分，也是ROC曲线下面的面积。</p><p>绘制ROC曲线的意义很明显，不断地把可能分错的情况扣除掉，从概率最高往下取的点，每有一个是负样本，就会导致分错排在它下面的所有正样本，所以要把它下面的正样本数扣除掉（1-TPR，剩下的正样本的比例）。总的ROC曲线绘制出来了，AUC就定了，分对的概率也能求出来了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;最近在做GBDT模型，里边用到胡模型评价方法就是AUC，刚好趁此机会，好好学习一下。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;混淆矩阵-Confusion-matrix&quot;&gt;&lt;a href=&quot;#混淆矩阵-Confusion-matrix&quot;
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="机器学习" scheme="http://thinkgamer.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="AUC" scheme="http://thinkgamer.cn/tags/AUC/"/>
    
  </entry>
  
  <entry>
    <title>梯度提升决策树-GBDT（Gradient Boosting Decision Tree）</title>
    <link href="http://thinkgamer.cn/2018/04/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%86%B3%E7%AD%96%E6%A0%91-GBDT%EF%BC%88Gradient%20Boosting%20Decision%20Tree%EF%BC%89/"/>
    <id>http://thinkgamer.cn/2018/04/12/机器学习/梯度提升决策树-GBDT（Gradient Boosting Decision Tree）/</id>
    <published>2018-04-12T15:44:15.000Z</published>
    <updated>2019-04-13T04:37:10.785Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>研究GBDT的背景是业务中使用到了该模型，用于做推荐场景，当然这里就引出了GBDT的一个应用场景-回归，他的另外一个应用场景便是分类，接下来我会从以下几个方面去学习和研究GBDT的相关知识，当然我也是学习者，只是把我理解到的整理出来。本文参考了网上各路大神的笔记，在此感谢！</p></blockquote><hr><p>转载请注明出处：<a href="https://blog.csdn.net/gamer_gyt/article/details/79875130" target="_blank" rel="external">https://blog.csdn.net/gamer_gyt/article/details/79875130</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="Boosting-amp-Bagging"><a href="#Boosting-amp-Bagging" class="headerlink" title="Boosting&amp;Bagging"></a>Boosting&amp;Bagging</h1><p>集成学习方法不是单独的一个机器学习算法，而是通过构建多个机器学习算法来达到一个强学习器。集成学习可以用来进行分类，回归，特征选取和异常点检测等。随机森林算法就是一个典型的集成学习方法，简单的说就是由一个个弱分类器（决策树）来构建一个强分类器，从而达到比较好的分类效果。</p><p>那么如何得到单个的学习器，一般有两种方法：</p><ul><li>同质（对于一个强学习器而言，所用的单个弱学习器都是一样的，比如说用的都是决策树，或者都是神经网络）</li><li>异质（相对于同质而言，对于一个强学习器而言，所用的单个弱学习器不全是一样的，比如说用的决策树和神经网络的组合）</li></ul><p>相对异质而言，同质学习期用的最为广泛，我们平时所讨论的集成学习方法指的就是同质个体学习器，同质个体学习器按照个体学习器之间的依赖关系分为串行（有强依赖关系）和并行（不存在关系或者有很弱的依赖关系），而在串行关系中有代表性的就是boosting系列算法，并行关系中具有代表性的就是bagging和随机森林（random forest）</p><p>Boosting流程图<br><img src="https://img-blog.csdn.net/20180410010257504?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>Bagging流程图<br><img src="https://img-blog.csdn.net/20180410010305827?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"></p><p>上边简单的介绍了集成学习方法和boosting&amp;bagging的区别，那么对于单个学习器采用何种策略才能得到一个强学习器呢？</p><ul><li>平均法（加权（个体学习器性能相差较大），简单（性能相近））</li><li>投票法（绝对多数（超过半数标记。否则拒绝预测），相对多数，加权投票）</li><li>学习法（通过另一个学习器来进行结合，Stacking算法）</li></ul><blockquote><p>Stacking算法：<br>基本思想：先从初始数据集训练出初级学习器，然后生成一个新数据集用于训练次级学习器。在这个新数据集中，初级学习器的输出被当作样例输入特征，而出事样本的标记仍被当作样例标记。<br><br><br>注意点：若直接用初级学习器的训练集来产生次级训练集，则过拟合风险会比较大；一般会通过交叉验证等方式，用训练初级学习器未使用的样本来产生次级学习器的训练样本。</p></blockquote><hr><h1 id="Gradient-Boosting"><a href="#Gradient-Boosting" class="headerlink" title="Gradient Boosting"></a>Gradient Boosting</h1><p>Gradient Boosting是一种Boosting的方法，它主要的思想是，每一次建立模型是在之前建立模型损失函数的梯度下降方向。损失函数是评价模型性能（一般为拟合程度+正则项），认为损失函数越小，性能越好。而让损失函数持续下降，就能使得模型不断改性提升性能，其最好的方法就是使损失函数沿着梯度方向下降（讲道理梯度方向上下降最快）。</p><p>Gradient Boost是一个框架，里面可以套入很多不同的算法。</p><h1 id="分类树-amp-回归树-amp-分类回归树"><a href="#分类树-amp-回归树-amp-分类回归树" class="headerlink" title="分类树&amp;回归树&amp;分类回归树"></a>分类树&amp;回归树&amp;分类回归树</h1><h2 id="分类树"><a href="#分类树" class="headerlink" title="分类树"></a>分类树</h2><p>三种比较常见的分类决策树分支划分方式包括：ID3, C4.5, CART。</p><blockquote><p>以C4.5分类树为例，C4.5分类树在每次分枝时，是穷举每一个feature的每一个阈值，找到使得按照feature&lt;=阈值，和feature&gt;阈值分成的两个分枝的熵最大的阈值(熵最大的概念可理解成尽可能每个分枝的男女比例都远离1:1)，按照该标准分枝得到两个新节点，用同样方法继续分枝直到所有人都被分入性别唯一的叶子节点，或达到预设的终止条件，若最终叶子节点中的性别不唯一，则以多数人的性别作为该叶子节点的性别。</p></blockquote><p>总结：分类树使用信息增益或增益比率来划分节点；每个节点样本的类别情况投票决定测试样本的类别。</p><h2 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h2><blockquote><p>回归树总体流程也是类似，区别在于，回归树的每个节点（不一定是叶子节点）都会得一个预测值，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每一个feature的每个阈值找最好的分割点，但衡量最好的标准不再是最大熵，而是最小化均方差即(每个人的年龄-预测年龄)^2 的总和 / N。也就是被预测出错的人数越多，错的越离谱，均方差就越大，通过最小化均方差能够找到最可靠的分枝依据。分枝直到每个叶子节点上人的年龄都唯一或者达到预设的终止条件(如叶子个数上限)，若最终叶子节点上人的年龄不唯一，则以该节点上所有人的平均年龄做为该叶子节点的预测年龄。</p></blockquote><p>总结：回归树使用最大均方差划分节点；每个节点样本的均值作为测试样本的回归预测值。</p><h2 id="分类回归树"><a href="#分类回归树" class="headerlink" title="分类回归树"></a>分类回归树</h2><blockquote><p>Classification And Regression Trees，即既能做分类任务又能做回归任务，CART也是决策树的一种，是一种二分决策树，但是也可以用来做回归，CART同决策树类似，不同于 ID3 与 C4.5 ,分类树采用基尼指数来选择最优的切分特征，而且每次都是二分。至于怎么利用基尼系数进行最优的特征切分，大家可以参考这篇文章的详细介绍 <a href="https://www.cnblogs.com/ooon/p/5647309.html" target="_blank" rel="external">决策树之 CART</a></p></blockquote><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>机器学习中的损失函数有很多，常见的有</p><ul><li>0-1损失函数（0-1 loss function）<script type="math/tex; mode=display">L(Y,f(X))=\left\{ \begin{aligned}&1,\quad Y\ne f(X)\\& 0,\quad Y=f(X) \end{aligned} \right.</script></li></ul><blockquote><p>该损失函数的意义就是，当预测错误时，损失函数值为1，预测正确时，损失函数值为0。该损失函数不考虑预测值和真实值的误差程度，也就是只要预测错误，预测错误差一点和差很多是一样的。</p></blockquote><ul><li><p>平方损失函数（quadratic loss function）</p><script type="math/tex; mode=display">L(Y,f(X))=(Y-f(X))^2</script><blockquote><p>取预测差距的平方</p></blockquote></li><li><p>绝对值损失函数（absolute loss function）</p><script type="math/tex; mode=display">L(Y,f(X))=|Y-f(X)|</script><blockquote><p>取预测值与真实值的差值绝对值，差距不会被平方放大</p></blockquote></li><li><p>对数损失函数（logarithmic loss function）</p><script type="math/tex; mode=display">L(Y,P(Y|X))=-logP(Y|X)</script><blockquote><p>该损失函数用到了极大似然估计的思想。P(Y|X)通俗的解释就是：在当前模型的基础上，对于样本X，其预测值为Y，也就是预测正确的概率。由于概率之间的同时满足需要使用乘法，为了将其转化为加法，我们将其取对数。最后由于是损失函数，所以预测正确的概率越高，其损失值应该是越小，因此再加个负号取个反。</p></blockquote></li><li><p>全局损失函数 </p><blockquote><p>上面的损失函数仅仅是对于一个样本来说的。而我们的优化目标函数应当是使全局损失函数最小。因此，全局损失函数往往是每个样本的损失函数之和，即： </p><script type="math/tex; mode=display">J(w,b)=\frac{1}{m} \sum_{i=1}^m L(Y,f(X))</script><p>对于平方损失函数，为了求导方便，我们可以在前面乘上一个1/2，和平方项求导后的2抵消，即： </p><script type="math/tex; mode=display">J(w,b)=\frac{1}{2m} \sum_{i=1}^m L(Y,f(X))</script></blockquote></li><li>逻辑回归中的损失函数<blockquote><p>在逻辑回归中，我们采用的是对数损失函数。由于逻辑回归是服从伯努利分布(0-1分布)的，并且逻辑回归返回的sigmoid值是处于(0,1)区间，不会取到0,1两个端点。因此我们能够将其损失函数写成以下形式： </p><script type="math/tex; mode=display">L(\hat y,y)=-(y\log{\hat y}+(1-y)\log(1-\hat y))</script></blockquote></li></ul><h1 id="GBDT思想"><a href="#GBDT思想" class="headerlink" title="GBDT思想"></a>GBDT思想</h1><blockquote><p>以下部分学习于 <a href="https://www.zybuluo.com/yxd/note/611571" target="_blank" rel="external">GBDT算法原理深入解析</a> ，原文作者讲的很好，照搬过来，毕竟笔者不是推导数学公式的料，哈哈</p></blockquote><p>GBDT 可以看成是由K棵树组成的加法模型：</p><script type="math/tex; mode=display">\hat{y}_i=\sum_{k=1}^K f_k(x_i), f_k \in F \tag 0</script><p>其中F为所有树组成的函数空间，以回归任务为例，回归树可以看作为一个把特征向量映射为某个score的函数。该模型的参数为：$\Theta=\{f_1,f_2, \cdots, f_K \}$。于一般的机器学习算法不同的是，加法模型不是学习d维空间中的权重，而是直接学习函数（决策树）集合</p><p>上述加法模型的目标函数定义为：$Obj=\sum_{i=1}^n l(y_i, \hat{y}_i) + \sum_{k=1}^K \Omega(f_k)$，其中$\Omega$表示决策树的复杂度，那么该如何定义树的复杂度呢？比如，可以考虑树的节点数量、树的深度或者叶子节点所对应的分数的L2范数等等。</p><p>如何来学习加法模型呢？</p><p>解这一优化问题，可以用前向分布算法（forward stagewise algorithm）。因为学习的是加法模型，如果能够从前往后，每一步只学习一个基函数及其系数（结构），逐步逼近优化目标函数，那么就可以简化复杂度。这一学习过程称之为Boosting。具体地，我们从一个常量预测开始，每次学习一个新的函数，过程如下： </p><script type="math/tex; mode=display">\begin{split}\hat{y}_i^0 &= 0 \\\hat{y}_i^1 &= f_1(x_i) = \hat{y}_i^0 + f_1(x_i) \\\hat{y}_i^2 &= f_1(x_i) + f_2(x_i) = \hat{y}_i^1 + f_2(x_i) \\& \cdots \\\hat{y}_i^t &= \sum_{k=1}^t f_k(x_i) = \hat{y}_i^{t-1} + f_t(x_i) \\\end{split}</script><p>那么，在每一步如何决定哪一个函数$f$被加入呢？指导原则还是最小化目标函数。<br>在第$t$步，模型对$x_i$的预测为：$\hat{y}_i^t= \hat{y}_i^{t-1} + f_t(x_i)$，其中$f_t(x_i)$为这一轮我们要学习的函数（决策树）。这个时候目标函数可以写为：</p><script type="math/tex; mode=display">\begin{split}Obj^{(t)} &= \sum_{i=1}^nl(y_i, \hat{y}_i^t) + \sum_{i=i}^t \Omega(f_i) \\&=  \sum_{i=1}^n l\left(y_i, \hat{y}_i^{t-1} + f_t(x_i) \right) + \Omega(f_t) + constant\end{split}\tag{1}</script><p>举例说明，假设损失函数为平方损失（square loss），则目标函数为：</p><script type="math/tex; mode=display">\begin{split}Obj^{(t)} &= \sum_{i=1}^n \left(y_i - (\hat{y}_i^{t-1} + f_t(x_i)) \right)^2 + \Omega(f_t) + constant \\&= \sum_{i=1}^n \left[2(\hat{y}_i^{t-1} - y_i)f_t(x_i) + f_t(x_i)^2 \right] + \Omega(f_t) + constant\end{split}\tag{2}</script><p>其中，$(\hat{y}_i^{t-1} - y_i)$称之为残差（residual）。因此，使用平方损失函数时，GBDT算法的每一步在生成决策树时只需要拟合前面的模型的残差。</p><blockquote><p>泰勒公式：设$n$是一个正整数，如果定义在一个包含$a$的区间上的函数$f$在点$a$处$n+1$次可导，那么对于这个区间上的任意$x$都有：$\displaystyle f(x)=\sum _{n=0}^{N}\frac{f^{(n)}(a)}{n!}(x-a)^ n+R_ n(x)$，其中的多项式称为函数在$a$处的泰勒展开式，$R_ n(x)$是泰勒公式的余项且是$(x-a)^ n$的高阶无穷小。</p></blockquote><p>根据泰勒公式把函数$f(x+\Delta x)$在点$x$处二阶展开，可得到如下等式： </p><script type="math/tex; mode=display">f(x+\Delta x) \approx f(x) + f'(x)\Delta x + \frac12 f''(x)\Delta x^2 \tag 3</script><p>由等式(1)可知，目标函数是关于变量$\hat{y}_i^{t-1} + f_t(x_i)$的函数，若把变量$\hat{y}_i^{t-1}$看成是等式(3)中的$x$，把变量$f_t(x_i)$看成是等式(3)中的$\Delta x$，则等式(1)可转化为：</p><script type="math/tex; mode=display">Obj^{(t)} = \sum_{i=1}^n \left[ l(y_i, \hat{y}_i^{t-1}) + g_if_t(x_i) + \frac12h_if_t^2(x_i) \right]  + \Omega(f_t) + constant \tag 4</script><p>其中$g_i$，定义为损失函数的一阶导数，即$g_i=\partial_{\hat{y}^{t-1}}l(y_i,\hat{y}^{t-1})$；$h_i$定义为损失函数的二阶导数，即$h_i=\partial_{\hat{y}^{t-1}}^2l(y_i,\hat{y}^{t-1})$。<br>假设损失函数为平方损失函数，则$g_i=\partial_{\hat{y}^{t-1}}(\hat{y}^{t-1} - y_i)^2 = 2(\hat{y}^{t-1} - y_i)$，$h_i=\partial_{\hat{y}^{t-1}}^2(\hat{y}^{t-1} - y_i)^2 = 2$，把$g_i$和$h_i$代入等式(4)即得等式(2)。<br>由于函数中的常量在函数最小化的过程中不起作用，因此我们可以从等式(4)中移除掉常量项，得： </p><script type="math/tex; mode=display">Obj^{(t)} \approx \sum_{i=1}^n \left[ g_if_t(x_i) + \frac12h_if_t^2(x_i) \right]  + \Omega(f_t) \tag 5</script><p>由于要学习的函数仅仅依赖于目标函数，从等式(5)可以看出只需为学习任务定义好损失函数，并为每个训练样本计算出损失函数的一阶导数和二阶导数，通过在训练样本集上最小化等式(5)即可求得每步要学习的函数$f(x)$，从而根据加法模型等式(0)可得最终要学习的模型。</p><h1 id="GBDT在Scikit-learn中的调用"><a href="#GBDT在Scikit-learn中的调用" class="headerlink" title="GBDT在Scikit-learn中的调用"></a>GBDT在Scikit-learn中的调用</h1><p>关于GBDT在Scikit-learn中的实现原文在 <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html" target="_blank" rel="external">点击查看</a><br>GBDT在sklearn中导入的包不一样，分类是  from sklearn.ensemble import GradientBoostingClassifier，回归是 from sklearn.ensemble import GradientBoostingRegressor</p><h2 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h2><p>GBDT的参数分为boosting类库参数和弱学习器参数，其中有GBDT的弱学习器为CART，所以弱学习器参数基本为决策树的参数，参考<a href="https://www.cnblogs.com/DjangoBlog/p/6201663.html" target="_blank" rel="external">点击阅读</a></p><h3 id="类库参数"><a href="#类库参数" class="headerlink" title="　类库参数"></a>　类库参数</h3><ul><li>loss：损失函数，对于分类模型，有对数似然损失函数”deviance”和指数损失函数”exponential”两者输入选择。默认是对数似然损失函数”deviance”。在原理篇中对这些分类损失函数有详细的介绍。一般来说，推荐使用默认的”deviance”。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法。</li><li>learning_rate：即每个弱学习器的权重缩减系数νν，也称作步长。</li><li>n_estimators：就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是100。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。</li><li>subsample：子采样，取值是[0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样。</li><li>init：即我们的初始化的时候的弱学习器，如果不输入，则用训练集样本来做样本集的初始化分类回归预测。否则用init参数提供的学习器做初始化分类回归预测。一般用在我们对数据有先验知识，或者之前做过一些拟合的时候，如果没有的话就不用管这个参数了。</li><li>verbose：默认是0，代表启用详细输出，若为1，代表偶尔输出进度信息</li><li>warm_start：默认为false</li><li>random_state：如果int，random_state随机数生成器使用的种子；如果randomstate实例，random_state是随机数发生器；如果没有，随机数生成器使用的np.random的randomstate实例。</li><li>presort：默认情况下会在密集的数据上使用，默认是在稀疏数据正常排序。设置对true的稀疏数据将会引起错误。</li></ul><h3 id="决策树参数"><a href="#决策树参数" class="headerlink" title="决策树参数"></a>决策树参数</h3><ul><li>max_depth：决策树最大深度</li><li>criterion：衡量分裂指标的度量方法，支持的是均方误差</li><li>min_samples_split：内部节点再划分所需最小样本数。这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。</li><li>min_samples_leaf：叶子节点最少样本数。这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。</li><li>min_weight_fraction_leaf：叶子节点最小的样本权重和。这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。 默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。</li><li>max_features：划分时考虑的最大特征数。可以使用很多种类型的值，默认是”None”,意味着划分时考虑所有的特征数；如果是”log2”意味着划分时最多考虑log2Nlog2N个特征；如果是”sqrt”或者”auto”意味着划分时最多考虑N−−√N个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。一般来说，如果样本特征数不多，比如小于50，我们用默认的”None”就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。</li><li>max_leaf_nodes：最大叶子节点数。通过限制最大叶子节点数，可以防止过拟合，默认是”None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。</li><li>min_impurity_split：节点划分最小不纯度。这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。一般不推荐改动默认值1e-7。</li><li>min_impurity_decrease：默认值为0。如果分裂</li></ul><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn.datasets import make_hastie_10_2</span><br><span class="line">&gt;&gt;&gt; from sklearn.ensemble import GradientBoostingClassifier</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; X, y = make_hastie_10_2(random_state=0)</span><br><span class="line">&gt;&gt;&gt; X_train, X_test = X[:2000], X[2000:]</span><br><span class="line">&gt;&gt;&gt; y_train, y_test = y[:2000], y[2000:]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,</span><br><span class="line">...     max_depth=1, random_state=0).fit(X_train, y_train)</span><br><span class="line">&gt;&gt;&gt; clf.score(X_test, y_test)                 </span><br><span class="line">0.913...</span><br></pre></td></tr></table></figure><h2 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import numpy as np</span><br><span class="line">&gt;&gt;&gt; from sklearn.metrics import mean_squared_error</span><br><span class="line">&gt;&gt;&gt; from sklearn.datasets import make_friedman1</span><br><span class="line">&gt;&gt;&gt; from sklearn.ensemble import GradientBoostingRegressor</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)</span><br><span class="line">&gt;&gt;&gt; X_train, X_test = X[:200], X[200:]</span><br><span class="line">&gt;&gt;&gt; y_train, y_test = y[:200], y[200:]</span><br><span class="line">&gt;&gt;&gt; est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,</span><br><span class="line">...     max_depth=1, random_state=0, loss=&apos;ls&apos;).fit(X_train, y_train)</span><br><span class="line">&gt;&gt;&gt; mean_squared_error(y_test, est.predict(X_test))    </span><br><span class="line">5.00...</span><br></pre></td></tr></table></figure><h2 id="模型评价"><a href="#模型评价" class="headerlink" title="模型评价"></a>模型评价</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import cross_validation, metrics</span><br><span class="line">metrics.accuracy_score(y.values, y_pred) # 准确度</span><br><span class="line">metrics.roc_auc_score(y, y_predprob)  # AUC大小</span><br></pre></td></tr></table></figure><hr><p>参考资料：</p><ul><li><a href="https://www.cnblogs.com/pinard/p/6131423.html" target="_blank" rel="external">集成学习原理小结</a></li><li><a href="https://blog.csdn.net/weixin_40604987/article/details/79296427" target="_blank" rel="external">Regression Tree 回归树</a></li><li><a href="https://blog.csdn.net/qq547276542/article/details/77980042" target="_blank" rel="external">浅析机器学习中各种损失函数及其含义</a></li></ul><hr><center>    <img src="http://img.blog.csdn.net/20180204165223361?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" width="35%" height="35%">    <br>打开微信扫一扫，加入数据与算法交流大群</center><center>    <img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">    <br>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;研究GBDT的背景是业务中使用到了该模型，用于做推荐场景，当然这里就引出了GBDT的一个应用场景-回归，他的另外一个应用场景便是分类，接下来我会从以下几个方面去学习和研究GBDT的相关知识，当然我也是学习者，只是把我理解到的整理出来。本文参考了网上
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="机器学习" scheme="http://thinkgamer.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="GBDT" scheme="http://thinkgamer.cn/tags/GBDT/"/>
    
  </entry>
  
  <entry>
    <title>Softmax-Regression</title>
    <link href="http://thinkgamer.cn/2018/03/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Softmax-Regression/"/>
    <id>http://thinkgamer.cn/2018/03/28/机器学习/Softmax-Regression/</id>
    <published>2018-03-28T15:44:15.000Z</published>
    <updated>2019-04-13T04:37:32.192Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>在本节中，我们介绍Softmax回归模型，该模型是logistic回归模型在多分类问题上的推广，在多分类问题中，类标签 \textstyle y 可以取两个以上的值。 Softmax回归模型对于诸如MNIST手写数字分类等问题是很有用的，该问题的目的是辨识10个不同的单个数字。Softmax回归是有监督的，不过后面也会介绍它与深度学习/无监督学习方法的结合。（译者注： MNIST 是一个手写数字识别库，由NYU 的Yann LeCun 等人维护。<a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="external">http://yann.lecun.com/exdb/mnist/</a> ）</p><p>回想一下在 logistic 回归中，我们的训练集由 \textstyle m 个已标记的样本构成：$ \{ (x^{(1)}, y^{(1)}), \ldots, (x^{(m)}, y^{(m)}) \} $ ，其中输入特征$ x^{(i)} \in \Re^{n+1} $（我们对符号的约定如下：特征向量<br>$ \textstyle x $ 的维度为 \textstyle n+1，其中 \textstyle x_0 = 1 对应截距项 。） 由于 logistic 回归是针对二分类问题的，因此类标记 $ y^{(i)} \in \{0,1\}$。假设函数(hypothesis function) 如下：</p><script type="math/tex; mode=display">\begin{align}h_\theta(x) = \frac{1}{1+\exp(-\theta^Tx)},\end{align}</script><p>我们将训练模型参数$ \textstyle \theta $，使其能够最小化代价函数 ：</p><script type="math/tex; mode=display">\begin{align}J(\theta) = -\frac{1}{m} \left[ \sum_{i=1}^m y^{(i)} \log h_\theta(x^{(i)}) + (1-y^{(i)}) \log (1-h_\theta(x^{(i)})) \right]\end{align}</script><p>在 softmax回归中，我们解决的是多分类问题（相对于 logistic 回归解决的二分类问题），类标$ \textstyle y $可以取$ \textstyle k $ 个不同的值（而不是 2 个）。因此，对于训练集$ \{ (x^{(1)}, y^{(1)}), \ldots, (x^{(m)}, y^{(m)}) \} $，我们有$ y^{(i)} \in \{1, 2, \ldots, k\} $。（注意此处的类别下标从 1 开始，而不是 0）。例如，在 MNIST 数字识别任务中，我们有$ \textstyle k=10 $个不同的类别。</p><p>对于给定的测试输入$ \textstyle x$，我们想用假设函数针对每一个类别j估算出概率值$ \textstyle p(y=j | x)$。也就是说，我们想估计$ \textstyle x $ 的每一种分类结果出现的概率。因此，我们的假设函数将要输出一个 $ \textstyle k $维的向量（向量元素的和为1）来表示这$ \textstyle k$ 个估计的概率值。 具体地说，我们的假设函数$ \textstyle h_{\theta}(x) $ 形式如下：</p><script type="math/tex; mode=display">\begin{align}h_\theta(x^{(i)}) =\begin{bmatrix}p(y^{(i)} = 1 | x^{(i)}; \theta) \\p(y^{(i)} = 2 | x^{(i)}; \theta) \\\vdots \\p(y^{(i)} = k | x^{(i)}; \theta)\end{bmatrix}=\frac{1}{ \sum_{j=1}^{k}{e^{ \theta_j^T x^{(i)} }} }\begin{bmatrix}e^{ \theta_1^T x^{(i)} } \\e^{ \theta_2^T x^{(i)} } \\\vdots \\e^{ \theta_k^T x^{(i)} } \\\end{bmatrix}\end{align}</script><p>其中$ \theta_1, \theta_2, \ldots, \theta_k \in \Re^{n+1}$ 是模型的参数。请注意$ \frac{1}{ \sum_{j=1}^{k}{e^{ \theta_j^T x^{(i)} }} } $这一项对概率分布进行归一化，使得所有概率之和为 1 。</p><p>为了方便起见，我们同样使用符号$ \textstyle \theta$ 来表示全部的模型参数。在实现Softmax回归时，将$ \textstyle \theta$ 用一个$ \textstyle k \times(n+1) $的矩阵来表示会很方便，该矩阵是将 $ \theta_1, \theta_2, \ldots, \theta_k $ 按行罗列起来得到的，如下所示：</p><script type="math/tex; mode=display">\theta = \begin{bmatrix}\mbox{---} \theta_1^T \mbox{---} \\\mbox{---} \theta_2^T \mbox{---} \\\vdots \\\mbox{---} \theta_k^T \mbox{---} \\\end{bmatrix}</script><h1 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h1><p>现在我们来介绍 softmax 回归算法的代价函数。在下面的公式中，$\textstyle 1\{\cdot\}$ 是示性函数，其取值规则为：<br>$ \textstyle 1\{ 值为真的表达式 \textstyle \}=1 $</p><p>，$ \textstyle 1\{ 值为假的表达式 \textstyle \}=0 $。举例来说，表达式$ \textstyle 1\{2+2=4\} $的值为1 ，$ \textstyle 1\{1+1=5\} $的值为 0。我们的代价函数为：</p><script type="math/tex; mode=display">\begin{align}J(\theta) = - \frac{1}{m} \left[ \sum_{i=1}^{m} \sum_{j=1}^{k}  1\left\{y^{(i)} = j\right\} \log \frac{e^{\theta_j^T x^{(i)}}}{\sum_{l=1}^k e^{ \theta_l^T x^{(i)} }}\right]\end{align}</script><p>值得注意的是，上述公式是logistic回归代价函数的推广。logistic回归代价函数可以改为：</p><script type="math/tex; mode=display">\begin{align}J(\theta) &= -\frac{1}{m} \left[ \sum_{i=1}^m   (1-y^{(i)}) \log (1-h_\theta(x^{(i)})) + y^{(i)} \log h_\theta(x^{(i)}) \right] \\&= - \frac{1}{m} \left[ \sum_{i=1}^{m} \sum_{j=0}^{1} 1\left\{y^{(i)} = j\right\} \log p(y^{(i)} = j | x^{(i)} ; \theta) \right]\end{align}</script><p>可以看到，Softmax代价函数与logistic 代价函数在形式上非常类似，只是在Softmax损失函数中对类标记的 $\textstyle k $个可能值进行了累加。注意在Softmax回归中将 \textstyle x 分类为类别 $\textstyle j $的概率为：</p><script type="math/tex; mode=display">p(y^{(i)} = j | x^{(i)} ; \theta) = \frac{e^{\theta_j^T x^{(i)}}}{\sum_{l=1}^k e^{ \theta_l^T x^{(i)}} }.</script><p>对于$ \textstyle J(\theta)$ 的最小化问题，目前还没有闭式解法。因此，我们使用迭代的优化算法（例如梯度下降法，或 L-BFGS）。经过求导，我们得到梯度公式如下：</p><script type="math/tex; mode=display">\begin{align}\nabla_{\theta_j} J(\theta) = - \frac{1}{m} \sum_{i=1}^{m}{ \left[ x^{(i)} \left( 1\{ y^{(i)} = j\}  - p(y^{(i)} = j | x^{(i)}; \theta) \right) \right]  }\end{align}</script><p>让我们来回顾一下符号 “$ \textstyle \nabla_{\theta_j}$” 的含义。$\textstyle \nabla_{\theta_j} J(\theta)$ 本身是一个向量，它的第$ \textstyle l $个元素$ \textstyle \frac{\partial J(\theta)}{\partial \theta_{jl}} $是$ \textstyle J(\theta)对\textstyle \theta_j $的第 $\textstyle l $个分量的偏导数。</p><p>有了上面的偏导数公式以后，我们就可以将它代入到梯度下降法等算法中，来最小化$ \textstyle J(\theta)$。 例如，在梯度下降法的标准实现中，每一次迭代需要进行如下更新: $\textstyle \theta_j := \theta_j - \alpha \nabla_{\theta_j} J(\theta)(\textstyle j=1,\ldots,k）$。</p><p>当实现 softmax 回归算法时， 我们通常会使用上述代价函数的一个改进版本。具体来说，就是和权重衰减(weight decay)一起使用。我们接下来介绍使用它的动机和细节。</p><h1 id="Softmax回归模型参数化的特点"><a href="#Softmax回归模型参数化的特点" class="headerlink" title="Softmax回归模型参数化的特点"></a>Softmax回归模型参数化的特点</h1><p>Softmax 回归有一个不寻常的特点：它有一个“冗余”的参数集。为了便于阐述这一特点，假设我们从参数向量 $\textstyle \theta_j $中减去了向量 $\textstyle \psi$，这时，每一个$ \textstyle \theta_j $都变成了 $\textstyle \theta_j - \psi(\textstyle j=1, \ldots, k)$。此时假设函数变成了以下的式子：</p><script type="math/tex; mode=display">\begin{align}p(y^{(i)} = j | x^{(i)} ; \theta)&= \frac{e^{(\theta_j-\psi)^T x^{(i)}}}{\sum_{l=1}^k e^{ (\theta_l-\psi)^T x^{(i)}}}  \\&= \frac{e^{\theta_j^T x^{(i)}} e^{-\psi^Tx^{(i)}}}{\sum_{l=1}^k e^{\theta_l^T x^{(i)}} e^{-\psi^Tx^{(i)}}} \\&= \frac{e^{\theta_j^T x^{(i)}}}{\sum_{l=1}^k e^{ \theta_l^T x^{(i)}}}.\end{align}</script><p>换句话说，从$ \textstyle \theta_j $中减去$ \textstyle \psi$ 完全不影响假设函数的预测结果！这表明前面的 softmax 回归模型中存在冗余的参数。更正式一点来说， Softmax 模型被过度参数化了。对于任意一个用于拟合数据的假设函数，可以求出多组参数值，这些参数得到的是完全相同的假设函数$ \textstyle h_\theta$。</p><p>进一步而言，如果参数 $\textstyle (\theta_1, \theta_2,\ldots, \theta_k) $是代价函数$ \textstyle J(\theta)$ 的极小值点，那么 $\textstyle (\theta_1 - \psi, \theta_2 - \psi,\ldots,<br>\theta_k - \psi) $同样也是它的极小值点，其中 $\textstyle \psi$ 可以为任意向量。因此使 $\textstyle J(\theta)$ 最小化的解不是唯一的。（有趣的是，由于$ \textstyle J(\theta)$ 仍然是一个凸函数，因此梯度下降时不会遇到局部最优解的问题。但是 Hessian 矩阵是奇异的/不可逆的，这会直接导致采用牛顿法优化就遇到数值计算的问题）</p><p>注意，当$ \textstyle \psi = \theta_1$ 时，我们总是可以将 $\textstyle \theta_1$替换为$\textstyle \theta_1 - \psi = \vec{0}$（即替换为全零向量），并且这种变换不会影响假设函数。因此我们可以去掉参数向量 $\textstyle \theta_1 $（或者其他 $\textstyle \theta_j $中的任意一个）而不影响假设函数的表达能力。实际上，与其优化全部的$ \textstyle k\times(n+1) $个参数 $\textstyle (\theta_1, \theta_2,\ldots, \theta_k)$ （其中 $\textstyle \theta_j \in \Re^{n+1}）$，我们可以令$ \textstyle \theta_1 =<br>\vec{0}$，只优化剩余$的 \textstyle (k-1)\times(n+1)$ 个参数，这样算法依然能够正常工作。</p><p>在实际应用中，为了使算法实现更简单清楚，往往保留所有参数$ \textstyle (\theta_1, \theta_2,\ldots, \theta_n)$，而不任意地将某一参数设置为 0。但此时我们需要对代价函数做一个改动：加入权重衰减。权重衰减可以解决 softmax 回归的参数冗余所带来的数值问题。</p><h1 id="权重衰减"><a href="#权重衰减" class="headerlink" title="权重衰减"></a>权重衰减</h1><p>我们通过添加一个权重衰减项$\textstyle \frac{\lambda}{2} \sum_{i=1}^k \sum_{j=0}^{n} \theta_{ij}^2$ 来修改代价函数，这个衰减项会惩罚过大的参数值，现在我们的代价函数变为：</p><script type="math/tex; mode=display">\begin{align}J(\theta) = - \frac{1}{m} \left[ \sum_{i=1}^{m} \sum_{j=1}^{k} 1\left\{y^{(i)} = j\right\} \log \frac{e^{\theta_j^T x^{(i)}}}{\sum_{l=1}^k e^{ \theta_l^T x^{(i)} }}  \right]              + \frac{\lambda}{2} \sum_{i=1}^k \sum_{j=0}^n \theta_{ij}^2\end{align}</script><p>有了这个权重衰减项以后 ($\textstyle \lambda &gt; 0$)，代价函数就变成了严格的凸函数，这样就可以保证得到唯一的解了。 此时的 Hessian矩阵变为可逆矩阵，并且因为$\textstyle J(\theta)$是凸函数，梯度下降法和 L-BFGS 等算法可以保证收敛到全局最优解。</p><p>为了使用优化算法，我们需要求得这个新函数$ \textstyle J(\theta)$ 的导数，如下：</p><script type="math/tex; mode=display">\begin{align}\nabla_{\theta_j} J(\theta) = - \frac{1}{m} \sum_{i=1}^{m}{ \left[ x^{(i)} ( 1\{ y^{(i)} = j\}  - p(y^{(i)} = j | x^{(i)}; \theta) ) \right]  } + \lambda \theta_j\end{align}通过最小化 \textstyle J(\theta)，我们就能实现一个可用的 softmax 回归模型。# Softmax回归与Logistic 回归的关系当类别数$ \textstyle k = 2 $时，softmax 回归退化为 logistic 回归。这表明 softmax 回归是 logistic 回归的一般形式。具体地说，当 $\textstyle k = 2 $时，softmax 回归的假设函数为：</script><p>\begin{align}<br>h_\theta(x) &amp;=<br>\frac{1}{ e^{\theta_1^Tx}  + e^{ \theta_2^T x^{(i)} } }<br>\begin{bmatrix}<br>e^{ \theta_1^T x } \\<br>e^{ \theta_2^T x }<br>\end{bmatrix}<br>\end{align}</p><script type="math/tex; mode=display">利用softmax回归参数冗余的特点，我们令$ \textstyle \psi = \theta_1$，并且从两个参数向量中都减去向量$ \textstyle \theta_1$，得到:</script><p>\begin{align}<br>h(x) &amp;=<br>\frac{1}{ e^{\vec{0}^Tx}  + e^{ (\theta_2-\theta_1)^T x^{(i)} } }<br>\begin{bmatrix}<br>e^{ \vec{0}^T x } \\<br>e^{ (\theta_2-\theta_1)^T x }<br>\end{bmatrix} \\<br>&amp;=<br>\begin{bmatrix}<br>\frac{1}{ 1 + e^{ (\theta_2-\theta_1)^T x^{(i)} } } \\<br>\frac{e^{ (\theta_2-\theta_1)^T x }}{ 1 + e^{ (\theta_2-\theta_1)^T x^{(i)} } }<br>\end{bmatrix} \\<br>&amp;=<br>\begin{bmatrix}<br>\frac{1}{ 1  + e^{ (\theta_2-\theta_1)^T x^{(i)} } } \\<br>1 - \frac{1}{ 1  + e^{ (\theta_2-\theta_1)^T x^{(i)} } } \\<br>\end{bmatrix}<br>\end{align}</p><p>$$<br>因此，用$ \textstyle \theta$’来表示$\textstyle \theta_2-\theta_1$，我们就会发现 softmax 回归器预测其中一个类别的概率为 $\textstyle \frac{1}{ 1  + e^{ (\theta’)^T x^{(i)} } }$，另一个类别概率的为$ \textstyle 1 - \frac{1}{ 1 + e^{ (\theta’)^T x^{(i)} } }$，这与 logistic回归是一致的。</p><h1 id="Softmax-回归-vs-k-个二元分类器"><a href="#Softmax-回归-vs-k-个二元分类器" class="headerlink" title="Softmax 回归 vs. k 个二元分类器"></a>Softmax 回归 vs. k 个二元分类器</h1><p>如果你在开发一个音乐分类的应用，需要对k种类型的音乐进行识别，那么是选择使用 softmax 分类器呢，还是使用 logistic 回归算法建立 k 个独立的二元分类器呢？</p><p>这一选择取决于你的类别之间是否互斥，例如，如果你有四个类别的音乐，分别为：古典音乐、乡村音乐、摇滚乐和爵士乐，那么你可以假设每个训练样本只会被打上一个标签（即：一首歌只能属于这四种音乐类型的其中一种），此时你应该使用类别数 k = 4 的softmax回归。（如果在你的数据集中，有的歌曲不属于以上四类的其中任何一类，那么你可以添加一个“其他类”，并将类别数 k 设为5。）</p><p>如果你的四个类别如下：人声音乐、舞曲、影视原声、流行歌曲，那么这些类别之间并不是互斥的。例如：一首歌曲可以来源于影视原声，同时也包含人声 。这种情况下，使用4个二分类的 logistic 回归分类器更为合适。这样，对于每个新的音乐作品 ，我们的算法可以分别判断它是否属于各个类别。</p><p>现在我们来看一个计算视觉领域的例子，你的任务是将图像分到三个不同类别中。(i) 假设这三个类别分别是：室内场景、户外城区场景、户外荒野场景。你会使用sofmax回归还是 3个logistic 回归分类器呢？ (ii) 现在假设这三个类别分别是室内场景、黑白图片、包含人物的图片，你又会选择 softmax 回归还是多个 logistic 回归分类器呢？</p><p>在第一个例子中，三个类别是互斥的，因此更适于选择softmax回归分类器 。而在第二个例子中，建立三个独立的 logistic回归分类器更加合适。</p><h1 id="中英文对照"><a href="#中英文对照" class="headerlink" title="中英文对照"></a>中英文对照</h1><ul><li>Softmax回归 Softmax Regression</li><li>有监督学习 supervised learning</li><li>无监督学习 unsupervised learning</li><li>深度学习 deep learning</li><li>logistic回归 logistic regression</li><li>截距项 intercept term</li><li>二元分类 binary classification</li><li>类型标记 class labels</li><li>估值函数/估计值 hypothesis</li><li>代价函数 cost function</li><li>多元分类 multi-class classification</li><li>权重衰减 weight decay</li></ul><hr><p>原文链接：<a href="http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92" target="_blank" rel="external">http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92</a><br>英文链接：<a href="http://ufldl.stanford.edu/wiki/index.php/Softmax_Regression" target="_blank" rel="external">http://ufldl.stanford.edu/wiki/index.php/Softmax_Regression</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;在本节中，我们介绍Softmax回归模型，该模型是logistic回归模型在多分类问题上的推广，在多分类问题中，类标签 \textstyle
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="机器学习" scheme="http://thinkgamer.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统的一些思考</title>
    <link href="http://thinkgamer.cn/2018/03/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/"/>
    <id>http://thinkgamer.cn/2018/03/26/机器学习/推荐系统的一些思考/</id>
    <published>2018-03-25T21:55:44.000Z</published>
    <updated>2019-04-13T04:36:53.568Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>推荐系统一直以来都是电商网站必不可少的一项，在提升用户转化，增加GMV方面可谓功不可没，那么一个好的推荐算法必然会创造更大的价值，刚好最近听了一个关于推荐算法的讲座，写出来一些思考吧，算是分享一下。</p></blockquote><h1 id="学术界的推荐系统"><a href="#学术界的推荐系统" class="headerlink" title="学术界的推荐系统"></a>学术界的推荐系统</h1><p>其实在大学期间也看过一些推荐的算法，还帮别人实现过关于推荐系统的毕设，但终究都是停留在协同过滤的层面，顶多是加了一些热门推荐来防止冷启动。不得不说，协同过滤打开了我对推荐系统认知的大门，当然在真实环境中这是远远不够的。</p><p>传统的推荐系统无非就是评分和排序两种方案，评分即计算出用户对item的可能评分，根据评分的高低进行排序，排序则不关心具体的评分是多少，只是为了得到一个顺序（其实这一点和推荐系统即为相似）。传统的推荐算法典型的有协同过滤和基于内容的过滤。如果你不明白什么是协同过滤算法可以参考：<a href="https://blog.csdn.net/gamer_gyt/article/details/51346159" target="_blank" rel="external">https://blog.csdn.net/gamer_gyt/article/details/51346159</a> ,如果你不知道协同过滤与基于内容的过滤的区别可参考：<a href="https://www.zhihu.com/question/19971859。" target="_blank" rel="external">https://www.zhihu.com/question/19971859。</a></p><p>大学数学术界关于推荐算法的论文都是对协同过滤的改进，而最终得到一个相对于原先的算法有很大的提升的结果，但是这些都过于理想化了，真实的环境远比实验要复杂的多，网上最有名的推荐系统数据集莫过于那个电影评分数据了，里边只有用户对电影的评分，和电影的一些信息数据。建立在这些数据上的推荐算法其实有点理想化了，他并不能模拟出真实的电商环境，数据的缺乏也是导致协同过滤算法大行其道的原因。</p><h1 id="工业界的推荐系统"><a href="#工业界的推荐系统" class="headerlink" title="工业界的推荐系统"></a>工业界的推荐系统</h1><p>工业界的推荐系统，需要的是明确的价值走向，比如说电商网站的推荐系统是为了增加交易总额，那么在进行推荐的时候是不是应该适当过滤一些极其廉价的商品，是否应该根据用户对不同价格段的需求进行不同价格段商品的推送；如果电商的推荐系统是为了增加用户停留时间，提交CTR，那么推荐系统就不应该考虑过多别的因素，只找到用户最感兴趣的商品或者评论等，当然如何找到用户最感兴趣的也是一个问题，但是有一点不可否认的是这些如果用传统的协同过滤来做是很难满足需求的。这时候就需要开发出新的推荐架构，来适应不同的需求。</p><p>目前工业界用的最多的算法莫过于GBDT，LR，DNN等，但所有的推荐算法都会面临一个海量数据的情况，这个时候的做法便是对数据集进行数据召回，得到用户比较感兴趣的一些数据，然后再根据我们的推荐模型进行素材偏好度排序，过滤掉用户已经购买过的类别数据，继而推送给用户。</p><p>那么如何进行数据召回呢？这就需要一些基础的模型进行数据准备，比如说用户肖像，用户的价格段偏好，用户购买力水平等。根绝已有的用户数据和特征进行数据召回，适度拉取一些新的数据，保证召回结果的多样性，得到召回池数据之后，便是对模型的训练，其实模型本身难度不大，难度大的是如何选取有效的特征来作为模型的输入数据。你组合得到的有效特征越多，对于模型的训练结果就越准确。</p><h1 id="推荐系统的多样性"><a href="#推荐系统的多样性" class="headerlink" title="推荐系统的多样性"></a>推荐系统的多样性</h1><p>如何保证推荐结果的多样性呢，首先我们要先认识到推荐的可能性，比如说电商网页首页的推荐，商品详细页面的推荐，不同年龄下的推荐，推荐的结果和被评估的指标都是不一样的。这个时候不能单一对所有情况下使用同一种算法或者特征，而是要找到能够区分出不同位置，不同年龄的推荐结果的特征，进行模型训练。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;推荐系统一直以来都是电商网站必不可少的一项，在提升用户转化，增加GMV方面可谓功不可没，那么一个好的推荐算法必然会创造更大的价值，刚好最近听了一个关于推荐算法的讲座，写出来一些思考吧，算是分享一下。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="推荐系统" scheme="http://thinkgamer.cn/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="机器学习" scheme="http://thinkgamer.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Scala解析XML</title>
    <link href="http://thinkgamer.cn/2018/02/04/Spark/Scala%E8%A7%A3%E6%9E%90XML/"/>
    <id>http://thinkgamer.cn/2018/02/04/Spark/Scala解析XML/</id>
    <published>2018-02-04T08:32:45.000Z</published>
    <updated>2019-04-13T04:39:32.795Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在使用Spark时，有时候主函数入口参数过多的时候，会特别复杂，这个时候我们可以将相应的参数写在xml文件中，然后只要将xml文件的路径传进去即可，这里的xml路径可以是本地的，也可以是hdfs上的。</p></blockquote><p>scala提供了类似于Xpath的语法来解析xml文件，其中很重要的两个操作符是”\”<br>和 “\\”</p><ul><li>\ ：根据搜索条件得到下一个节点</li><li>\\：根据条件获取所有的节点</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;configure&gt;</span><br><span class="line">    &lt;input&gt;</span><br><span class="line">        &lt;name&gt;app_feature_goods&lt;/name&gt;</span><br><span class="line">        &lt;hdfs&gt;/user/path/to/goods&lt;/hdfs&gt;</span><br><span class="line">    &lt;/input&gt;</span><br><span class="line">    &lt;input&gt;</span><br><span class="line">        &lt;name&gt;app_feature_user&lt;/name&gt;</span><br><span class="line">        &lt;hdfs&gt;/user/path/to/user&lt;/hdfs&gt;</span><br><span class="line">    &lt;/input&gt;</span><br><span class="line">&lt;/configure&gt;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">val input = args(0)</span><br><span class="line">val xml = XML.load(input)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 找到所有的一级节点 input</span><br><span class="line">val input_list = xml\&quot;input&quot;</span><br><span class="line">input_list.foreach(println)</span><br><span class="line"></span><br><span class="line">// 遍历每个一级节点，得到具体的值</span><br><span class="line">for(one &lt;- input_list)&#123;</span><br><span class="line">    println(one\&quot;name&quot;)</span><br><span class="line">    println((one\&quot;name&quot;).text)</span><br><span class="line">    println(one\&quot;hdfs&quot;)</span><br><span class="line">    println((one\&quot;hdfs&quot;).text)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 得到所有的name</span><br><span class="line">val name_list = xml\\&quot;name&quot;</span><br><span class="line">name_list.map(one =&gt; one.text).foreach(println)</span><br><span class="line"></span><br><span class="line">// 获取所有hdfs</span><br><span class="line">val hdfs_list = xml\\&quot;hdfs&quot;</span><br><span class="line">hdfs_list.map(one =&gt; one.text).foreach(println)</span><br><span class="line"></span><br><span class="line">// 获取具有class的值</span><br><span class="line">println(xml\&quot;input&quot;\&quot;name&quot;\\&quot;@class&quot;)</span><br><span class="line"></span><br><span class="line">// 打印出具有class属性的name值和hdfs值</span><br><span class="line">println((xml\\&quot;name&quot;).filter(_.attribute(&quot;class&quot;).exists(_.text.equals(&quot;test&quot;))).text)</span><br><span class="line">println((xml\\&quot;hdfs&quot;).filter(_.attribute(&quot;class&quot;).exists(_.text.equals(&quot;test&quot;))).text)</span><br></pre></td></tr></table></figure><p>打印出的信息为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">&lt;input&gt;</span><br><span class="line">        &lt;name&gt;app_feature_goods&lt;/name&gt;</span><br><span class="line">        &lt;hdfs&gt;/user/path/to/goods&lt;/hdfs&gt;</span><br><span class="line">    &lt;/input&gt;</span><br><span class="line">&lt;input&gt;</span><br><span class="line">        &lt;name&gt;app_feature_user&lt;/name&gt;</span><br><span class="line">        &lt;hdfs&gt;/user/path/to/user&lt;/hdfs&gt;</span><br><span class="line">    &lt;/input&gt;</span><br><span class="line">&lt;input&gt;</span><br><span class="line">        &lt;name class=&quot;test&quot;&gt;app_feature_user_test&lt;/name&gt;</span><br><span class="line">        &lt;hdfs class=&quot;test&quot;&gt;/user/path/to/user_test&lt;/hdfs&gt;</span><br><span class="line">    &lt;/input&gt;</span><br><span class="line">-------------</span><br><span class="line">&lt;name&gt;app_feature_goods&lt;/name&gt;</span><br><span class="line">app_feature_goods</span><br><span class="line">&lt;hdfs&gt;/user/path/to/goods&lt;/hdfs&gt;</span><br><span class="line">/user/path/to/goods</span><br><span class="line">&lt;name&gt;app_feature_user&lt;/name&gt;</span><br><span class="line">app_feature_user</span><br><span class="line">&lt;hdfs&gt;/user/path/to/user&lt;/hdfs&gt;</span><br><span class="line">/user/path/to/user</span><br><span class="line">&lt;name class=&quot;test&quot;&gt;app_feature_user_test&lt;/name&gt;</span><br><span class="line">app_feature_user_test</span><br><span class="line">&lt;hdfs class=&quot;test&quot;&gt;/user/path/to/user_test&lt;/hdfs&gt;</span><br><span class="line">/user/path/to/user_test</span><br><span class="line">-------------</span><br><span class="line">app_feature_goods</span><br><span class="line">app_feature_user</span><br><span class="line">app_feature_user_test</span><br><span class="line">-------------</span><br><span class="line">/user/path/to/goods</span><br><span class="line">/user/path/to/user</span><br><span class="line">/user/path/to/user_test</span><br><span class="line">-------------</span><br><span class="line">test</span><br><span class="line">-------------</span><br><span class="line">app_feature_user_test</span><br><span class="line">/user/path/to/user_test</span><br><span class="line">-------------</span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br></pre></td></tr></table></figure></p><p>当然还存在一种情况就是XML文件存在于hdfs之上，这时候就不能直接load xml文件里，不过可以通过下面一种方法获得<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">var rdd = sc.textFile(xml_path)</span><br><span class="line">val xml = XML.loadString(rdd.collect().mkString(&quot;\n&quot;))</span><br></pre></td></tr></table></figure></p><p>接下来便可以通过上边的方法进行解析了。</p><center>    <img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">    <br>打开微信扫一扫，关注公众号【数据与算法联盟】</center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;在使用Spark时，有时候主函数入口参数过多的时候，会特别复杂，这个时候我们可以将相应的参数写在xml文件中，然后只要将xml文件的路径传进去即可，这里的xml路径可以是本地的，也可以是hdfs上的。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;sc
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="Spark" scheme="http://thinkgamer.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark求统计量的两种方法</title>
    <link href="http://thinkgamer.cn/2018/02/04/Spark/Spark%E6%B1%82%E7%BB%9F%E8%AE%A1%E9%87%8F%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95/"/>
    <id>http://thinkgamer.cn/2018/02/04/Spark/Spark求统计量的两种方法/</id>
    <published>2018-02-04T07:04:27.000Z</published>
    <updated>2019-04-13T04:39:40.684Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Spark对于统计量中的最大值，最小值，平均值和方差（均值）的计算都提供了封装，这里小编知道两种计算方法，整理一下分享给大家</p></blockquote><h1 id="DataFrame形式"><a href="#DataFrame形式" class="headerlink" title="DataFrame形式"></a>DataFrame形式</h1><h2 id="加载Json数据源"><a href="#加载Json数据源" class="headerlink" title="加载Json数据源"></a>加载Json数据源</h2><p>example.json文件格式如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;name&quot;:&quot;thinkgamer&quot;,&quot;age&quot;:23,&quot;math&quot;:78,&quot;chinese&quot;:78,&quot;english&quot;:95&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;think&quot;,&quot;age&quot;:25,&quot;math&quot;:95,&quot;chinese&quot;:88,&quot;english&quot;:93&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;gamer&quot;,&quot;age&quot;:24,&quot;math&quot;:93,&quot;chinese&quot;:68,&quot;english&quot;:88&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// persist(StorageLevel.MEMORY_AND_DISK) 当内存不够时cache到磁盘里</span><br><span class="line">val df = spark.read.json(&quot;/path/to/example.json&quot;).persist(StorageLevel.MEMORY_AND_DISK)</span><br><span class="line">df.show()</span><br><span class="line">df.describe()</span><br></pre></td></tr></table></figure><p>我们便可以看到如下的形式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">+---+-------+-------+----+----------+</span><br><span class="line">|age|chinese|english|math|      name|</span><br><span class="line">+---+-------+-------+----+----------+</span><br><span class="line">| 23|     78|     95|  78|thinkgamer|</span><br><span class="line">| 25|     88|     93|  95|     think|</span><br><span class="line">| 24|     68|     88|  93|     gamer|</span><br><span class="line">+---+-------+-------+----+----------+</span><br><span class="line"></span><br><span class="line">+-------+----+-------+-----------------+-----------------+----------+</span><br><span class="line">|summary| age|chinese|          english|             math|      name|</span><br><span class="line">+-------+----+-------+-----------------+-----------------+----------+</span><br><span class="line">|  count|   3|      3|                3|                3|         3|</span><br><span class="line">|   mean|24.0|   78.0|             92.0|88.66666666666667|      null|</span><br><span class="line">| stddev| 1.0|   10.0|3.605551275463989| 9.29157324317757|      null|</span><br><span class="line">|    min|  23|     68|               88|               78|     gamer|</span><br><span class="line">|    max|  25|     88|               95|               95|thinkgamer|</span><br><span class="line">+-------+----+-------+-----------------+-----------------+----------+</span><br></pre></td></tr></table></figure></p><p>如果是想看某列的通知值的话，可以用下面的方式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.select(&quot;age&quot;).describe().show()</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">+-------+----+</span><br><span class="line">|summary| age|</span><br><span class="line">+-------+----+</span><br><span class="line">|  count|   3|</span><br><span class="line">|   mean|24.0|</span><br><span class="line">| stddev| 1.0|</span><br><span class="line">|    min|  23|</span><br><span class="line">|    max|  25|</span><br><span class="line">+-------+----+</span><br></pre></td></tr></table></figure><h1 id="RDD形式"><a href="#RDD形式" class="headerlink" title="RDD形式"></a>RDD形式</h1><p>假设同样还是上边的数据，只不过现在变成按\t分割的普通文本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">thinkgamer  23  78  78  95</span><br><span class="line">think   25  95  88  93</span><br><span class="line">gamer   24  93  68  88</span><br></pre></td></tr></table></figure></p><p>这里可以将rdd转换成dataframe洗形式，也可以使用rdd计算，转化为df的样例如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val new_data = data_txt</span><br><span class="line">      .map(_.split(&quot;\\s+&quot;))</span><br><span class="line">      .map(one =&gt; Person(one(0),one(1).toInt,one(2).toDouble,one(3).toDouble,one(4).toDouble))</span><br><span class="line">      .toDF()</span><br></pre></td></tr></table></figure></p><p>接下来就是进行和上边df一样的操作了。</p><p>那么对于rdd形式的文件如何操作：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.mllib.linalg.Vectors</span><br><span class="line">import org.apache.spark.mllib.stat.&#123;MultivariateStatisticalSummary, Statistics&#125;</span><br><span class="line"></span><br><span class="line">val data_txt = SparkSC.spark.sparkContext.textFile(input_txt).persist(StorageLevel.MEMORY_AND_DISK)</span><br><span class="line">    val new_data = data_txt</span><br><span class="line">      .map(_.split(&quot;\\s+&quot;))</span><br><span class="line">      .map(one =&gt; Vectors.dense(one(1).toInt,one(2).toDouble,one(3).toDouble,one(4).toDouble))</span><br><span class="line">    val summary: MultivariateStatisticalSummary = Statistics.colStats(new_data)</span><br><span class="line">    </span><br><span class="line">println(&quot;Max:&quot;+summary.max)</span><br><span class="line">println(&quot;Min:&quot;+summary.min)</span><br><span class="line">println(&quot;Count:&quot;+summary.count)</span><br><span class="line">println(&quot;Variance:&quot;+summary.variance)</span><br><span class="line">println(&quot;Mean:&quot;+summary.mean)</span><br><span class="line">println(&quot;NormL1:&quot;+summary.normL1)</span><br><span class="line">println(&quot;Norml2:&quot;+summary.normL2)</span><br></pre></td></tr></table></figure></p><p>输出结果为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Max:[25.0,95.0,88.0,95.0]</span><br><span class="line">Min:[23.0,78.0,68.0,88.0]</span><br><span class="line">Count:3</span><br><span class="line">Variance:[1.0,86.33333333333331,100.0,13.0]</span><br><span class="line">Mean:[24.0,88.66666666666667,78.0,92.0]</span><br><span class="line">NormL1:[72.0,266.0,234.0,276.0]</span><br><span class="line">Norml2:[41.593268686170845,154.1363033162532,135.83813897429545,159.43023552638942]</span><br></pre></td></tr></table></figure></p><p>这里可以得到相关的统计信息，主要区别在于dataframe得到的是标准差，而使用mllib得到的统计值中是方差，但这并不矛盾，两者可以相互转化得到。</p><p>当然如果要求四分位数，可以转化成df，使用sql语句进行查询<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Select PERCENTILE(col,&lt;0.25,0.75&gt;) from tableName;</span><br></pre></td></tr></table></figure></p><h1 id="自己实现"><a href="#自己实现" class="headerlink" title="自己实现"></a>自己实现</h1><p>下面是我自己实现的一个方法，传入的参数是一个rdd，返回的是一个字符串<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">// 计算最大值，最小值，平均值，方差，标准差，四分位数</span><br><span class="line">def getStat(data: RDD[String]):String= &#123;</span><br><span class="line">   val sort_data = data</span><br><span class="line">      .filter(one =&gt; Verify.istoDouble(one))</span><br><span class="line">      .map(_.toDouble)</span><br><span class="line">      .sortBy(line=&gt;line)</span><br><span class="line">      .persist(StorageLevel.MEMORY_AND_DISK)  // 默认是true 升序，false为降序</span><br><span class="line"></span><br><span class="line">   val data_list = sort_data.collect()</span><br><span class="line">   val len = data_list.length</span><br><span class="line">   val min = data_list(0)</span><br><span class="line">   val max = data_list(len-1)</span><br><span class="line">   val mean = sort_data.reduce((a,b) =&gt; a+b) / len</span><br><span class="line">   val variance = sort_data.map(one =&gt; math.pow(one-mean,2)).reduce((a,b)=&gt;a+b)/len</span><br><span class="line">   val stdder = math.sqrt(variance)</span><br><span class="line">   var quant = &quot;&quot;</span><br><span class="line">   if(len&lt;4)&#123;</span><br><span class="line">      val q1 = min</span><br><span class="line">      val q2 = min</span><br><span class="line">      val q3 = max</span><br><span class="line">      quant = q1+&quot;\t&quot;+q2+&quot;\t&quot;+q3</span><br><span class="line">   &#125;else &#123;</span><br><span class="line">      val q1 = data_list((len * 0.25).toInt - 1)</span><br><span class="line">      val q2 = data_list((len * 0.5).toInt - 1)</span><br><span class="line">      val q3 = data_list((len * 0.75).toInt - 1)</span><br><span class="line">      quant = q1+&quot;\t&quot;+q2+&quot;\t&quot;+q3</span><br><span class="line">   &#125;</span><br><span class="line">   max+&quot;\t&quot;+min+&quot;\t&quot;+mean+&quot;\t&quot;+variance+&quot;\t&quot;+stdder+&quot;\t&quot;+quant</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h1 id="本地碰见的一个错误"><a href="#本地碰见的一个错误" class="headerlink" title="本地碰见的一个错误"></a>本地碰见的一个错误</h1><p>1：错误1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scala.Predef$.refArrayOps([Ljava/lang/Object;)Lscala/collection/mutable/Array</span><br></pre></td></tr></table></figure></p><p>原因是Spark中spark-sql_2.11-2.2.1 ，是用scala 2.11版本上编译的，而我的本地的scala版本为2.12.4，所以就错了，可以在<br>里边把相应的scala版本就行修改就行了</p><p>2：错误2<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoSuchMethodError: scala.Product.$init$(Lscala/Product;)V</span><br></pre></td></tr></table></figure></p><p>原因也是因为我下载安装的scala2.12版本，换成scala2.11版本就可以了</p><hr><center>    <img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">    <br>打开微信扫一扫，关注公众号【数据与算法联盟】</center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;Spark对于统计量中的最大值，最小值，平均值和方差（均值）的计算都提供了封装，这里小编知道两种计算方法，整理一下分享给大家&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;DataFrame形式&quot;&gt;&lt;a href=&quot;#DataFrame形式&quot;
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="Spark" scheme="http://thinkgamer.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>记一次百G数据的聚类算法实施过程</title>
    <link href="http://thinkgamer.cn/2018/01/27/Spark/%E8%AE%B0%E4%B8%80%E6%AC%A1%E7%99%BEG%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AE%9E%E6%96%BD%E8%BF%87%E7%A8%8B/"/>
    <id>http://thinkgamer.cn/2018/01/27/Spark/记一次百G数据的聚类算法实施过程/</id>
    <published>2018-01-26T16:08:24.000Z</published>
    <updated>2019-04-13T04:39:30.187Z</updated>
    
    <content type="html"><![CDATA[<p>如题，记一次百G数据的聚类算法实施过程，用的技术都不难，spark和kmeans，我想你会认为这没有什么难度，我接到这个任务的时候也认为没有难度，可是一周之后我发现我错了，数据量100G的确不大，但难度在于我需要对 kmeans 的 train过程执行将近3000次，而且需要高效的完成。那么问题就来了，如何保证高效和准确性。（声明小编对Spark也不是说很熟悉）</p><h1 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h1><p>数据格式为三列，第一列为类别ID，第二列为商品ID，第三列为价格，数据格式如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1000    2000    45.3</span><br><span class="line">1000    2001    121.3</span><br><span class="line">1001    2002    4125.3</span><br><span class="line">1000    2003    225.3</span><br><span class="line">1001    2004    3415.3</span><br><span class="line">1000    2005    12245.3   </span><br><span class="line">...     ...     ....</span><br></pre></td></tr></table></figure></p><p>数据有很多条，数据量为将近100G，存储在hdfs上，第一列品类ID不唯一，每个品类ID下有多个商品ID，商品ID唯一，价格为浮点型数据</p><p>现在要对每个品类下的价格进行聚类，得到1~7个价格level（7level的价格要比6level的价格高，以此类推）</p><h1 id="第一次尝试"><a href="#第一次尝试" class="headerlink" title="第一次尝试"></a>第一次尝试</h1><p>第一次尝试很天真，思路也很正常，如下：<br>1：全量加载数据，形成rdd<br>2：数据split之后，按key进行groupby<br>3：针对每个key（也就是类别ID）进行kmeans聚类和预测，并将结果写入hdfs<br>4：加载每个类别的结果，进行聚合形成最终结果</p><p>那么开始写代码。papapa写了一堆，发现groupBy之后的数据格式是CompactBuffer，转化成spark kmeans train所需要的格式之后，代码卡着不会动，不明所以（我估计是格式没有转正确，不是kmeans 所需要的格式，但是如果不是kmeans 需要的格式，应该会报错呀），后来当我把代码打包，提交到集群上运行时，提示我kmeans train所在的函数中没有指定master url，可是我明明指定了，后来才发现是因为，我在rdd操作过程中能够，嵌套了函数，函数中又重新使用了rdd，也就是说rdd 不能嵌套rdd使用，具体可参考 <a href="https://www.zhihu.com/question/54439266" target="_blank" rel="external">Spark 为什么 不允许 RDD 嵌套-如 RDD[RDD[T]]</a>，而我在本地测试时指的都是local，没有进行报错，至此这条路行不通，也就是说不能按这样的思路执行</p><p>在该思路的基础上进行改进：<br>既然rdd不能嵌套rdd使用，何不先得到所有的类别id，然后在全量数据总filter单个类别id进行kmeans操作呢？</p><p>该代码，测试，伪代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">leibieID_list = XXXXX</span><br><span class="line">leibieID_list.map(one =&gt; kmeans(one,path))</span><br></pre></td></tr></table></figure></p><p>需要注意的是 leibieID_list.map 操作并不是分布式的，而是for 循环，这样3000个类别id运行完，时间可想而知，是极其耗时的，所以这条路也失败了（不是说行不通，是因为耗时）</p><h1 id="第二次尝试"><a href="#第二次尝试" class="headerlink" title="第二次尝试"></a>第二次尝试</h1><p>经过上边的尝试发现不行，那么我想是不是先全量读取数据，然后按照类别ID，将同一个类别ID的数据写到一个文件（或者文件夹下），然后再对之操作</p><p>开始写classify by ID 的代码，这里遇到了问题是如何让同一个类别ID的数据写到一个文件中，上网查了一些资料，可以参考之前整理的笔记</p><p><a href="http://blog.csdn.net/gamer_gyt/article/details/79157055" target="_blank" rel="external"> Spark多路径输出和二次排序</a></p><p>这里边有实现的办法，但是还有一个问题，对全量数据（100G）进行shuffle的时候，由于数据量特别大，也特别占用资源，往往会出现一些内存上的错误。</p><p>这里采用的策略是将全量数据rdd进行random split，然后for循环遍历split之后的rdd，进行saveAsTestFile，保存的目录这样设计<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/path/split=0/</span><br><span class="line">/path/split=1/</span><br><span class="line">/path/split=2/</span><br><span class="line">/path/split=3/</span><br><span class="line">    ...  ...</span><br></pre></td></tr></table></figure></p><p>这样的话，就可以避免大量数据 shuffle 耗费资源的问题了，而且也不影响后续数据的使用，同时这一步也会把类别id提取出来，保存在hdfs上，供下一步使用。</p><p>经历了上一步的数据准备，开始step 2的开发，第二步的思路：<br>加载第一步保存的类别id list文件，分成5份，启动5个spark任务进行train，至此，思路是正确的，但却忽略了一个很严正的问题：数据倾斜</p><p>由于是随机对类别 id 进行分组操作，那么不能保证没组中每个类别id对应的数据条数的大概一致性，也就是存在某个ID 数据条数只有几十条，而有些ID 数据条数千万条，这种情况下就会导致代码在运行过程中，有些task很快运行完了，有些执行了好久也没完事。</p><h1 id="第三次尝试"><a href="#第三次尝试" class="headerlink" title="第三次尝试"></a>第三次尝试</h1><p>有了第二次的经验，想法就是如何将数据条数差不多的分到同一组里，我采用的方法是进行统计，按照10的X次方形式进行分组，比如说<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1~10    1</span><br><span class="line">10~100  2</span><br><span class="line">100~1000    3</span><br><span class="line">....</span><br></pre></td></tr></table></figure></p><p>但是这样也有一个问题，就是这样大概符合正太分布，4、5、6这样的组里数据条数比较多，1、2、3和7、8、9这样的数据条数少，这样就会因为4、5、6组的程序运行时间较长，整体任务运行时间也较长。</p><p>所以这里采用合并和拆分的策略，比如说将1,2,3合并到一组，4、5、6分别拆成两组，7、8、9合成一组，这样就会保证每组运行的时间是差不多的。（实际情况中，要根据数据的分布进行合理的拆分和合并）</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>至此，问题算是最终解决了，相比原先的MR版本，时间缩减了将近8个小时，在整个优化的过程中，其实对于经验足够的开发者来说，可能很快就会解决，但对于我们这些新手，可能就要耗费些时间，涨涨记性了，在整个过程中对spark也算是有进一步的了解了。</p><p>其他的相关笔记：</p><ul><li><p><a href="http://blog.csdn.net/gamer_gyt/article/details/79157055" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt/article/details/79157055</a></p></li><li><p><a href="http://blog.csdn.net/gamer_gyt/article/details/79135118" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt/article/details/79135118</a></p></li></ul><hr><center>    <img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">    <br>打开微信扫一扫，关注公众号【数据与算法联盟】</center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;如题，记一次百G数据的聚类算法实施过程，用的技术都不难，spark和kmeans，我想你会认为这没有什么难度，我接到这个任务的时候也认为没有难度，可是一周之后我发现我错了，数据量100G的确不大，但难度在于我需要对 kmeans 的 train过程执行将近3000次，而且需
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="Spark" scheme="http://thinkgamer.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark多路径输出和二次排序</title>
    <link href="http://thinkgamer.cn/2018/01/25/Spark/Spark%E5%A4%9A%E8%B7%AF%E5%BE%84%E8%BE%93%E5%87%BA%E5%92%8C%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8F/"/>
    <id>http://thinkgamer.cn/2018/01/25/Spark/Spark多路径输出和二次排序/</id>
    <published>2018-01-24T16:22:03.000Z</published>
    <updated>2019-04-13T04:39:35.147Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在实际应用场景中，我们对于Spark往往有各式各样的需求，比如说想MR中的二次排序，Top N，多路劲输出等。那么这篇文章我们就来看下这几个问题。</p></blockquote><h1 id="二次排序"><a href="#二次排序" class="headerlink" title="二次排序"></a>二次排序</h1><p>假设我们的数据是这样的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1   2</span><br><span class="line">1   3</span><br><span class="line">1   1</span><br><span class="line">1   6</span><br><span class="line">1   4</span><br><span class="line">2   5</span><br><span class="line">2   8</span><br><span class="line">2   3</span><br></pre></td></tr></table></figure></p><p>我们想要实现第一列按降序排列，当第一列相同时，第二列按降序排列</p><p>定义一个SecondSortKey类：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class SecondSortKey(val first: Int, val second: Int)</span><br><span class="line">  extends Ordered[SecondSortKey] with Serializable &#123;</span><br><span class="line">  override def compare(that: SecondSortKey): Int = &#123;</span><br><span class="line">    if (this.first - that.first == 0) &#123;</span><br><span class="line">      this.second - that.second</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      this.first - that.first</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>然后这样去使用<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val lines = sc.textFile(&quot;test.txt&quot;)</span><br><span class="line">val pairs = lines.map &#123; x =&gt;</span><br><span class="line">      (new SecondSortKey(x.split(&quot;\\s+&quot;)(0).toInt,</span><br><span class="line">        x.split(&quot;\\s+&quot;)(1).toInt), x)</span><br><span class="line">    &#125;</span><br><span class="line">val sortedPairs = pairs.sortByKey(false);</span><br><span class="line">sortedPairs.map(_._2).foreach(println)</span><br></pre></td></tr></table></figure></p><p>当然这里如果想按第一列升序，当第一列相同时，第二列升序的顺序排列，只需要对SecondSoryKey做如下修改即可<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class SecondSortKey(val first: Int, val second: Int)</span><br><span class="line">  extends Ordered[SecondSortKey] with Serializable &#123;</span><br><span class="line">  override def compare(that: SecondSortKey): Int = &#123;</span><br><span class="line">    if (this.first - that.first !== 0) &#123;</span><br><span class="line">      this.second - that.second</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      this.first - that.first</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>当时使用的使用去掉<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pairs.sortByKey(false)</span><br></pre></td></tr></table></figure></p><p>中的false</p><h1 id="Top-N"><a href="#Top-N" class="headerlink" title="Top N"></a>Top N</h1><p>同样还是上边的数据，假设我们要得到第一列中的前五位<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val lines = sc.textFile(&quot;test.txt&quot;)</span><br><span class="line">val rdd = lines</span><br><span class="line">        .map(x =&gt; x.split(&quot;\\s+&quot;))</span><br><span class="line">        .map(x =&gt; (x(0),x(1)))</span><br><span class="line">        .sortByKey()</span><br><span class="line">rdd.take(N).foreach(println)</span><br></pre></td></tr></table></figure></p><h1 id="多路径输出"><a href="#多路径输出" class="headerlink" title="多路径输出"></a>多路径输出</h1><p>自己在使用的过程中，通过搜索发现了两种方法<br>1：调用saveAsHadoopFile函数并自定义一个OutputFormat类</p><p>自定义RDDMultipleTextOutputFormat类</p><p>RDDMultipleTextOutputFormat类中的generateFileNameForKeyValue函数有三个参数，key和value就是我们RDD的Key和Value，而name参数是每个Reduce的编号。本例中没有使用该参数，而是直接将同一个Key的数据输出到同一个文件中。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.hadoop.mapred.lib.MultipleTextOutputFormat  </span><br><span class="line">  </span><br><span class="line">class RDDMultipleTextOutputFormat extends MultipleTextOutputFormat[Any, Any] &#123;  </span><br><span class="line">  override def generateFileNameForKeyValue(key: Any, value: Any, name: String): String =  </span><br><span class="line">    key.asInstanceOf[String]  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>调用<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize(List((&quot;w&quot;, &quot;www&quot;), (&quot;b&quot;, &quot;blog&quot;), (&quot;c&quot;, &quot;com&quot;), (&quot;w&quot;, &quot;bt&quot;)))  </span><br><span class="line">      .map(value =&gt; (value._1, value._2 + &quot;Test&quot;))  </span><br><span class="line">      .partitionBy(new HashPartitioner(3))  </span><br><span class="line">      .saveAsHadoopFile(&quot;/iteblog&quot;, classOf[String],classOf[String],classOf[RDDMultipleTextOutputFormat])</span><br></pre></td></tr></table></figure></p><p>这里的<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new HashPartitioner(3)</span><br></pre></td></tr></table></figure></p><p>中的3是有key的种类决定的，当然在实际应用场景中，我们可能并不知道有多少k，这个时候就可以通过一个rdd 的 distinct操作来得到唯一key的数目。</p><p>2：使用dataframe<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">people_rdd = sc.parallelize([(1, &quot;alice&quot;), (1, &quot;bob&quot;), (2,&quot;charlie&quot;)])</span><br><span class="line">people_df = people_rdd.toDF([&quot;number&quot;, &quot;name&quot;])</span><br><span class="line">people_df.write.partitionBy(&quot;number&quot;).format(&quot;text&quot;).save(path  )</span><br></pre></td></tr></table></figure></p><p>当然这两种方法都有一个缺陷，就是当数据量特别大的时候，数据在repartition的过程中特别耗费资源，也会容易出现任务failed的情况，小编采用的解决办法是，适当的对原rdd进行split，然后遍历每个rdd，进行multioutput操作</p><p>形似如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.textFile(input)</span><br><span class="line">var split_rdd = rdd.randomSplit(Array(1.0,1.0,1.0,1.0))</span><br><span class="line">for (one &lt;- Array(1,2,3,4))</span><br><span class="line">&#123;</span><br><span class="line">    split_rdd(one)XXXX</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><hr><p>参考：</p><ul><li><a href="https://andone1cc.github.io/2017/03/04/Spark/%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8Ftopn/" target="_blank" rel="external">Spark学习笔记——二次排序，TopN，TopNByGroup</a></li><li><a href="https://www.iteblog.com/archives/1281.html" target="_blank" rel="external">Spark多文件输出(MultipleOutputFormat)</a></li><li><a href="https://code.i-harness.com/en/q/16e22a0" target="_blank" rel="external">scala - Write to multiple outputs by key Spark - one Spark job</a></li><li><a href="https://stackoverflow.com/questions/23995040/write-to-multiple-outputs-by-key-spark-one-spark-job/26051042#26051042" target="_blank" rel="external">Write to multiple outputs by key Spark - one Spark job</a></li></ul><hr><center><img src="http://img.blog.csdn.net/20171231121058707?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"><br>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;在实际应用场景中，我们对于Spark往往有各式各样的需求，比如说想MR中的二次排序，Top N，多路劲输出等。那么这篇文章我们就来看下这几个问题。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;二次排序&quot;&gt;&lt;a href=&quot;#二次排序&quot; cl
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="Spark" scheme="http://thinkgamer.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark提交参数说明和常见优化</title>
    <link href="http://thinkgamer.cn/2018/01/23/Spark/Spark%E6%8F%90%E4%BA%A4%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E%E5%92%8C%E5%B8%B8%E8%A7%81%E4%BC%98%E5%8C%96/"/>
    <id>http://thinkgamer.cn/2018/01/23/Spark/Spark提交参数说明和常见优化/</id>
    <published>2018-01-22T16:30:15.000Z</published>
    <updated>2019-04-13T04:39:38.020Z</updated>
    
    <content type="html"><![CDATA[<p>最近在搞一个价格分类模型，虽说是分类，用的是kmeans算法，求出聚类中心，对每个价格进行级别定级。虽然说起来简单，但做起来却是并没有那么容易，不只是因为数据量大，在执行任务时要不是效率问题就是shuffle报错等。但在这整个过程中对scala编程,Spark rdd 机制，以及海量数据背景下对算法的认知都有很大的提升，这一篇文章主要是总结一些Spark在shell 终端提交jar包任务的时候的相关知识，在后续文章会具体涉及到相关的”实战经历“。</p><h1 id="对Spark的认识"><a href="#对Spark的认识" class="headerlink" title="对Spark的认识"></a>对Spark的认识</h1><p>由于之前接触过Hadoop，对Spark也是了解一些皮毛，但中间隔了好久才重新使用spark，期间也产生过一些错误的认识。</p><p>之前觉得MapReduce耗费时间，写一个同等效果的Spark程序很快就能执行完，很长一段时间自己都是在本地的单机环境进行测试学习，所以这种错误的认知就会更加深刻，但事实却并非如此，MR之所以慢是因为每一次操作数据都写在了磁盘上，大量的IO造成了时间和资源的浪费，但是Spark是基于内存的计算引擎，相比MR，减少的是大量的IO，但并不是说给一个Spark程序足够的资源，就可以为所欲为了，在提交一个spark程序时，不仅要考虑所在资源队列的总体情况，还要考虑代码本身的高效性，要尽量避免大量的shuffle操作和action操作，尽量使用同一个rdd。</p><p>会用spark，会调api和能用好spark是两回事，在进行开发的过程中，不仅要了解运行原理，还要了解业务，将合适的方法和业务场景合适的结合在一起，才能发挥最大的价值。</p><h1 id="spark-submit"><a href="#spark-submit" class="headerlink" title="spark-submit"></a>spark-submit</h1><p>进入spark的home目录，执行以下命令查看帮助<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --help</span><br></pre></td></tr></table></figure></p><p>spark提交任务常见的两种模式<br>1：local/local[K]</p><ul><li>本地使用一个worker线程运行spark程序</li><li>本地使用K个worker线程运行spark程序</li></ul><p>此种模式下适合小批量数据在本地调试代码</p><p>2：yarn-client/yarn-cluster</p><ul><li>yarn-client：以client方式连接到YARN集群，集群的定位由环境变量HADOOP_CONF_DIR定义，该方式driver在client运行。</li><li>yarn-cluster：以cluster方式连接到YARN集群，集群的定位由环境变量HADOOP_CONF_DIR定义，该方式driver也在集群中运行。</li></ul><p>注意：若使用的是本地文件需要在file路径前加：file://</p><p>在提交任务时的几个重要参数</p><ul><li>executor-cores —— 每个executor使用的内核数，默认为1</li><li>num-executors —— 启动executors的数量，默认为2</li><li>executor-memory —— executor内存大小，默认1G</li><li>driver-cores —— driver使用内核数，默认为1</li><li>driver-memory —— driver内存大小，默认512M</li></ul><p>下边给一个提交任务的样式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">  --master local[5]  \</span><br><span class="line">  --driver-cores 2   \</span><br><span class="line">  --driver-memory 8g \</span><br><span class="line">  --executor-cores 4 \</span><br><span class="line">  --num-executors 10 \</span><br><span class="line">  --executor-memory 8g \</span><br><span class="line">  --class PackageName.ClassName XXXX.jar \</span><br><span class="line">  --name &quot;Spark Job Name&quot; \</span><br><span class="line">  InputPath      \</span><br><span class="line">  OutputPath</span><br><span class="line">  </span><br><span class="line">如果这里通过--queue 指定了队列，那么可以免去写--master</span><br></pre></td></tr></table></figure></p><p>以上就是通过spark-submit来提交一个任务</p><h1 id="几个参数的常规设置"><a href="#几个参数的常规设置" class="headerlink" title="几个参数的常规设置"></a>几个参数的常规设置</h1><ul><li><p>executor_cores*num_executors<br>表示的是能够并行执行Task的数目<br>不宜太小或太大！一般不超过总队列 cores 的 25%，比如队列总 cores 400，最大不要超过100，最小不建议低于 40，除非日志量很小。</p></li><li><p>executor_cores<br>不宜为1！否则 work 进程中线程数过少，一般 2~4 为宜。</p></li><li><p>executor_memory<br>一般 6~10g 为宜，最大不超过20G，否则会导致GC代价过高，或资源浪费严重。</p></li><li><p>driver-memory<br>driver 不做任何计算和存储，只是下发任务与yarn资源管理器和task交互，除非你是 spark-shell，否则一般 1-2g</p></li></ul><hr><p>增加每个executor的内存量，增加了内存量以后，对性能的提升，有三点：</p><ul><li>1、如果需要对RDD进行cache，那么更多的内存，就可以缓存更多的数据，将更少的数据写入磁盘，<br>甚至不写入磁盘。减少了磁盘IO。</li><li>2、对于shuffle操作，reduce端，会需要内存来存放拉取的数据并进行聚合。如果内存不够，也会写入磁盘。如果给executor分配更多内存以后，就有更少的数据，需要写入磁盘，甚至不需要写入磁盘。减少了磁盘IO，提升了性能。</li><li>3、对于task的执行，可能会创建很多对象。如果内存比较小，可能会频繁导致JVM堆内存满了，然后频繁GC，垃圾回收，minor GC和full GC。（速度很慢）。内存加大以后，带来更少的GC，垃圾回收，避免了速度变慢，性能提升。</li></ul><h1 id="常规注意事项"><a href="#常规注意事项" class="headerlink" title="常规注意事项"></a>常规注意事项</h1><ul><li>预处理数据，丢掉一些不必要的数据</li><li>增加Task的数量</li><li>过滤掉一些容易导致发生倾斜的key</li><li>避免创建重复的RDD</li><li>尽可能复用一个RDD</li><li>对多次使用的RDD进行持久化</li><li>尽量避免使用shuffle算子</li><li>在要使用groupByKey算子的时候,尽量用reduceByKey或者aggregateByKey算子替代.因为调用groupByKey时候,按照相同的key进行分组,形成RDD[key,Iterable[value]]的形式,此时所有的键值对都将被重新洗牌,移动,对网络数据传输造成理论上的最大影响.</li><li>使用高性能的算子</li></ul><hr><p>参考：<br>1：<a href="http://www.cnblogs.com/haozhengfei/p/e570f24c43fa15f23ebb97929a1b7fe6.html" target="_blank" rel="external">http://www.cnblogs.com/haozhengfei/p/e570f24c43fa15f23ebb97929a1b7fe6.html</a><br>2：<a href="https://www.jianshu.com/p/4c584a3bac7d" target="_blank" rel="external">https://www.jianshu.com/p/4c584a3bac7d</a></p><center>    <img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">    <br>打开微信扫一扫，关注公众号【数据与算法联盟】</center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近在搞一个价格分类模型，虽说是分类，用的是kmeans算法，求出聚类中心，对每个价格进行级别定级。虽然说起来简单，但做起来却是并没有那么容易，不只是因为数据量大，在执行任务时要不是效率问题就是shuffle报错等。但在这整个过程中对scala编程,Spark rdd 机制
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="Spark" scheme="http://thinkgamer.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>用大把的时间仿徨，却用几个瞬间成长</title>
    <link href="http://thinkgamer.cn/2017/12/31/%E9%9A%8F%E6%89%8B%E8%AE%B0/%E7%94%A8%E5%A4%A7%E6%8A%8A%E7%9A%84%E6%97%B6%E9%97%B4%E4%BB%BF%E5%BE%A8%EF%BC%8C%E5%8D%B4%E7%94%A8%E5%87%A0%E4%B8%AA%E7%9E%AC%E9%97%B4%E6%88%90%E9%95%BF/"/>
    <id>http://thinkgamer.cn/2017/12/31/随手记/用大把的时间仿徨，却用几个瞬间成长/</id>
    <published>2017-12-31T03:02:00.000Z</published>
    <updated>2019-04-13T04:38:15.878Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>人总要在特定的阶段去完成特定的事情，然后转身告诉自己，继续往前。</p></blockquote><p>提笔写这篇文章，内心是毫无波澜的，或许是因为曾经的经历让我意识到那一切都是缘的力所能及。以下的内容可能没有章节，没有顺序，但都是心灵能够触及的地方。</p><p>2017年对我个人来讲是比较重要的一年，可以说是完成了自我的一个改变吧，但是脱壳之后，需要的是更加努力的完善自我，这样才能够避免被淘汰。为什么这么说？因为这一年我觉得对我个人影响最大的两件事是：买了首套房（虽然只是付了首付）；在年末之时选择离开，侥幸的入职了京东。一个是为“家庭”做了必要的准备，一个是为“事业”做了点缀。而至于其他的大大小小的事情，则是17年的五味杂粮，并不是那么重要，但却也缺一不可。感谢17年，那些我认识的，或者我不认识的，帮助过我的，或者我帮助过的人。</p><h1 id="过往"><a href="#过往" class="headerlink" title="过往"></a>过往</h1><p>人上了年纪，总爱回忆！</p><p>刚好这两天，“18岁”刷爆了朋友圈和QQ空间，或许这是连腾讯有没有预料到的起死回生或者苟延残喘吧。趁机翻了下我的QQ空间相册，突然发现了，这二十多年来以来，我也是经历颇丰。</p><p>从上海到江苏，从沈阳到长春，从南京到天津，最后到北京，这一条路一走便是4年。<br>从初中到高中，从大学到社会，这一条路一走便是11年。</p><p>过往的这条路上遇见了很多人，碰见很多事，但终究还是走散了，看淡了，不过庆幸的是那些一直还有联系的，我们还能彼此叫出姓名的人，不管是18岁之前，还是18岁之后，我想我们是幸运的，</p><p>年少的我们曾经难免会埋下羞涩的种子，在记忆深处藏着一些不可告人的秘密，直到有一天我们盘膝而坐，三巡酒过，才道出那些现在我们认为可笑的不能再可笑的羞涩，从此，稚嫩青春里的唯一一朵留恋，也该告一段落。</p><p>一杯敬明天，一杯敬过往。灵魂不再无处安放。</p><h1 id="遇见"><a href="#遇见" class="headerlink" title="遇见"></a>遇见</h1><p>人有了目标，便爱胡闹！</p><p>2017年，不管是工作，还是自我学习都收获了挺多知识，感谢万维接受了我这个毫无工作经验的“学生”吧，在这里的确收获了挺多，让我明白没有结合业务的技术，只是向别人吹嘘而毫无创造价值的垃圾，不管你在哪，技术都是为了推动业务的增长。</p><p>2017年，还是习惯性的写一些学习笔记，只不过是频率明显了降了下来，这只能归结于自己变得懒惰了，从开通博客到现在，所有的表层的收获只能通过这些浅显的数据来表达：</p><center><img src="http://img.blog.csdn.net/20171231110350995?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><p>只是没有17年尾的时候访问量达到100W，不过一切随缘吧。</p><p>下边这几张图是最近一个月的访问分析情况，不管怎样，你学习了，也帮助其他人了，这就是成长。</p><center><img src="http://img.blog.csdn.net/20171231111249749?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">    <img src="http://img.blog.csdn.net/20171231111301875?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">    <img src="http://img.blog.csdn.net/20171231111312908?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">    <img src="http://img.blog.csdn.net/20171231111325084?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">    </center><p>2017年，开通了微信公众号【数据与算法联盟】（原名为码农故事多），没有刻意的运营，没有刻意的传播积累用户，一切随缘。当然如果你想加入我们的数据与算法学习交流群的话，欢迎加我的微信，拉你入群，群里有很多大牛，不定时进行“扯淡”。我们的宗旨就是以技术会友，分享与进步！感谢2017年和以往遇见的所有好友！</p><center>    <img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">    <br>长按二维码识别，关注微信公众号【数据与算法联盟】</center><p>2017年，很侥幸的加入了JD这个大家庭，未来的一切都是未知数，但你能做的就是向他人学习，围绕着工作进行深度的学习和成长。</p><h1 id="新生"><a href="#新生" class="headerlink" title="新生"></a>新生</h1><p>人过了18，要努力发芽！</p><p>过去的一年里定了太多的目标，结果大部分都没有实现，哎，分析一下，大部门的目标都是盲目的，没有围绕工作的目标（学习规划吧算是），其实实现起来是有难度的，所以新的一年里调整计划，重新出发。</p><p>2018不会定太多的目标，主要是想在技术和业务层面提升下自己，不管是学习大数据还是算法，都会围绕功能工作和一条主旋律进行展开。</p><p>感谢一路以来，遇见的所有人！</p><hr><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;人总要在特定的阶段去完成特定的事情，然后转身告诉自己，继续往前。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;提笔写这篇文章，内心是毫无波澜的，或许是因为曾经的经历让我意识到那一切都是缘的力所能及。以下的内容可能没有章节，没有顺序，但都是心灵能够触及
      
    
    </summary>
    
      <category term="随手记" scheme="http://thinkgamer.cn/categories/%E9%9A%8F%E6%89%8B%E8%AE%B0/"/>
    
    
      <category term="随手记" scheme="http://thinkgamer.cn/tags/%E9%9A%8F%E6%89%8B%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Hexo-Yilia加入相册功能</title>
    <link href="http://thinkgamer.cn/2017/12/14/%E9%9A%8F%E6%89%8B%E8%AE%B0/Hexo-Yilia%E5%8A%A0%E5%85%A5%E7%9B%B8%E5%86%8C%E5%8A%9F%E8%83%BD/"/>
    <id>http://thinkgamer.cn/2017/12/14/随手记/Hexo-Yilia加入相册功能/</id>
    <published>2017-12-14T09:55:29.000Z</published>
    <updated>2019-04-13T04:38:21.798Z</updated>
    
    <content type="html"><![CDATA[<p>参考：<a href="http://maker997.com/2017/07/01/hexo-Yilia-%E4%B8%BB%E9%A2%98%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0%E7%9B%B8%E5%86%8C%E5%8A%9F%E8%83%BD" target="_blank" rel="external">点击查看</a></p><p>但是其中有一些小问题，自己便重新整理了一下（本文适用于使用github存放照片）</p><h1 id="主页新建相册链接"><a href="#主页新建相册链接" class="headerlink" title="主页新建相册链接"></a>主页新建相册链接</h1><p>主题_config.json文件的menu 中加入 相册和对应的链接<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">themes/yilia/_config.json</span><br><span class="line"></span><br><span class="line">menu:</span><br><span class="line">  主页: /</span><br><span class="line">  ... ...</span><br><span class="line">  相册: /photos</span><br></pre></td></tr></table></figure></p><h1 id="新建目录并拷贝相应文件"><a href="#新建目录并拷贝相应文件" class="headerlink" title="新建目录并拷贝相应文件"></a>新建目录并拷贝相应文件</h1><p>使用的是litten 大神的博客 photos文件夹，对应的路径为：<br><a href="https://github.com/litten/BlogBackup/tree/master/source/photos" target="_blank" rel="external">https://github.com/litten/BlogBackup/tree/master/source/photos</a></p><p>自己的项目根目录下的source文件夹下新建photos文件夹，将下载的几个文件放在该文件夹中，或者不用新建，直接将下载的photos文件夹放在source目录下。</p><h1 id="文件修改"><a href="#文件修改" class="headerlink" title="文件修改"></a>文件修改</h1><ol><li>修改 ins.js 文件的 render()函数<br>这个函数是用来渲染数据的<br>修改图片的路径地址.minSrc 小图的路径. src 大图的路径.修改为自己的图片路径(github的路径)<br>例如我的为：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">var minSrc = &apos;https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/&apos; + data.link[i] + &apos;.min.jpg&apos;;</span><br><span class="line">var src = &apos;https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/photos/&apos; + data.link[i];</span><br></pre></td></tr></table></figure></li></ol><h1 id="生成json"><a href="#生成json" class="headerlink" title="生成json"></a>生成json</h1><p>1：下载相应python工具文件</p><ul><li>tools.py</li><li>ImageProcess.py</li></ul><p>下载地址：<a href="https://github.com/Thinkgamer/GitBlog" target="_blank" rel="external">https://github.com/Thinkgamer/GitBlog</a></p><p>2：新建photos和min_photos文件夹<br>在项目根目录下创建，用来存放照片和压缩后的照片<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir photos</span><br><span class="line">mkdir min_photos</span><br></pre></td></tr></table></figure></p><p>3：py文件和文件夹都放在项目根目录下</p><p>4：生成json<br>执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python tools.py</span><br></pre></td></tr></table></figure></p><p>如果提示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;tools.py&quot;, line 13, in &lt;module&gt;</span><br><span class="line">    from PIL import Image</span><br><span class="line">ImportError: No module named PIL</span><br></pre></td></tr></table></figure></p><p>说明你没有安装pillow，执行以下命令安装即可<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pillow</span><br></pre></td></tr></table></figure></p><p>如果报错：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ValueError: time data &apos;DSC&apos; does not match format &apos;%Y-%m-%d&apos;</span><br></pre></td></tr></table></figure></p><p>说明你照片的命名方式不合格，这里必须命名为以下这样的格式（当然时间是随意的）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2016-10-12_xxx.jpg/png</span><br></pre></td></tr></table></figure></p><p>ok，至此会在min_photos文件夹下生成同名的文件，但是大小会小很多</p><h1 id="本地预览和部署"><a href="#本地预览和部署" class="headerlink" title="本地预览和部署"></a>本地预览和部署</h1><h2 id="本地预览"><a href="#本地预览" class="headerlink" title="本地预览"></a>本地预览</h2><p>项目根目录下执行<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo s</span><br></pre></td></tr></table></figure></p><p>浏览器4000端口访问，按照上边的方式进行配置，正常情况下你是看不到图片的，通过调试可以发现图片的url中后缀变成了 xxx.jpg.jpg，所以我们要去掉一个jpg</p><p>改正方法<br>ins.js/render 函数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">var minSrc = &apos;https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/&apos; + data.link[i] + &apos;.min.jpg&apos;;</span><br><span class="line"></span><br><span class="line">换成</span><br><span class="line"></span><br><span class="line">var minSrc = &apos;https://raw.githubusercontent.com/Thinkgamer/GitBlog/master/min_photos/&apos; + data.link[i];</span><br><span class="line"></span><br><span class="line">注释掉该行：</span><br><span class="line">src += &apos;.jpg&apos;;</span><br></pre></td></tr></table></figure></p><p>到这里没完，路径都对了，但是在浏览器中还是不能看到图片，调试发现，下载大神的photos文件夹的ins.js中有一行代码，饮用了一张图片，默认情况下，在你的项目中，这张图片是不存在的，改正办法就是在对应目录下放一张图片，并修改相应的名字</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">src=&quot;/assets/img/empty.png</span><br></pre></td></tr></table></figure><p>ok，至此刷新浏览器是可以看到图片的，如果还看不到，应该就是浏览器缓存问题了，如果还有问题，可以加我微信进行沟通：gyt13342445911</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;参考：&lt;a href=&quot;http://maker997.com/2017/07/01/hexo-Yilia-%E4%B8%BB%E9%A2%98%E5%A6%82%E4%BD%95%E6%B7%BB%E5%8A%A0%E7%9B%B8%E5%86%8C%E5%8A%9F%E
      
    
    </summary>
    
      <category term="随手记" scheme="http://thinkgamer.cn/categories/%E9%9A%8F%E6%89%8B%E8%AE%B0/"/>
    
    
      <category term="hexo" scheme="http://thinkgamer.cn/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>梯度算法之批量梯度下降，随机梯度下降和小批量梯度下降</title>
    <link href="http://thinkgamer.cn/2017/12/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95%E4%B9%8B%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%8C%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%92%8C%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    <id>http://thinkgamer.cn/2017/12/14/机器学习/梯度算法之批量梯度下降，随机梯度下降和小批量梯度下降/</id>
    <published>2017-12-14T06:40:43.000Z</published>
    <updated>2019-04-13T04:37:13.264Z</updated>
    
    <content type="html"><![CDATA[<p>在机器学习领域，体梯度下降算法分为三种</p><ul><li>批量梯度下降算法（BGD，Batch gradient descent algorithm）</li><li>随机梯度下降算法（SGD，Stochastic gradient descent algorithm）</li><li>小批量梯度下降算法（MBGD，Mini-batch gradient descent algorithm）</li></ul><h1 id="批量梯度下降算法"><a href="#批量梯度下降算法" class="headerlink" title="批量梯度下降算法"></a>批量梯度下降算法</h1><p>BGD是最原始的梯度下降算法，每一次迭代使用全部的样本，即权重的迭代公式中(公式中用$\theta$代替$\theta_i$)，</p><script type="math/tex; mode=display">\jmath (\theta _0,\theta _1,...,\theta _n)=\sum_{i=0}^{m}( h_\theta(x_0,x_1,...,x_n)-y_i )^2</script><script type="math/tex; mode=display">\theta _i = \theta _i - \alpha \frac{\partial \jmath (\theta _1,\theta _2,...,\theta _n)}{\partial \theta _i}</script><script type="math/tex; mode=display">公式(1)</script><p>这里的m代表所有的样本，表示从第一个样本遍历到最后一个样本。</p><p>特点：</p><ul><li>能达到全局最优解，易于并行实现</li><li>当样本数目很多时，训练过程缓慢</li></ul><h1 id="随机梯度下降算法"><a href="#随机梯度下降算法" class="headerlink" title="随机梯度下降算法"></a>随机梯度下降算法</h1><p>SGD的思想是更新每一个参数时都使用一个样本来进行更新，即公式（1）中m为1。每次更新参数都只使用一个样本，进行多次更新。这样在样本量很大的情况下，可能只用到其中的一部分样本就能得到最优解了。<br>但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</p><p>特点：</p><ul><li>训练速度快</li><li>准确度下降，并不是最优解，不易于并行实现</li></ul><h1 id="小批量梯度下降算法"><a href="#小批量梯度下降算法" class="headerlink" title="小批量梯度下降算法"></a>小批量梯度下降算法</h1><p>MBGD的算法思想就是在更新每一参数时都使用一部分样本来进行更新，也就是公式（1）中的m的值大于1小于所有样本的数量。</p><p>相对于随机梯度下降，Mini-batch梯度下降降低了收敛波动性，即降低了参数更新的方差，使得更新更加稳定。相对于批量梯度下降，其提高了每次学习的速度。并且其不用担心内存瓶颈从而可以利用矩阵运算进行高效计算。一般而言每次更新随机选择[50,256]个样本进行学习，但是也要根据具体问题而选择，实践中可以进行多次试验，选择一个更新速度与更次次数都较适合的样本数。mini-batch梯度下降可以保证收敛性，常用于神经网络中。</p><h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><p>在样本量较小的情况下，可以使用批量梯度下降算法，样本量较大的情况或者线上，可以使用随机梯度下降算法或者小批量梯度下降算法。</p><p>在机器学习中的无约束优化算法，除了梯度下降以外，还有前面提到的最小二乘法，此外还有牛顿法和拟牛顿法。</p><p>梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。</p><p>梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快。但是每次迭代的时间比梯度下降法长。</p><h1 id="sklearn中的SGD"><a href="#sklearn中的SGD" class="headerlink" title="sklearn中的SGD"></a>sklearn中的SGD</h1><p>sklearn官网上查了一下，并没有找到BGD和MBGD的相关文档，只是看到可SGD的，感兴趣的可以直接去官网看英文文档，点击SGD查看：<a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html" target="_blank" rel="external">SGD</a>，这也有一个中文的 <a href="http://sklearn.lzjqsdd.com/modules/sgd.html" target="_blank" rel="external">SGD</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">In [1]: from sklearn.linear_model import SGDClassifier</span><br><span class="line"></span><br><span class="line">In [2]: X = [[0., 0.], [1., 1.]]</span><br><span class="line"></span><br><span class="line">In [3]: y = [0, 1]</span><br><span class="line"></span><br><span class="line">In [4]: clf = SGDClassifier(loss=&quot;hinge&quot;, penalty=&quot;l2&quot;)</span><br><span class="line"></span><br><span class="line">In [5]: clf.fit(X, y)</span><br><span class="line">Out[5]: </span><br><span class="line">SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,</span><br><span class="line">       eta0=0.0, fit_intercept=True, l1_ratio=0.15,</span><br><span class="line">       learning_rate=&apos;optimal&apos;, loss=&apos;hinge&apos;, n_iter=5, n_jobs=1,</span><br><span class="line">       penalty=&apos;l2&apos;, power_t=0.5, random_state=None, shuffle=True,</span><br><span class="line">       verbose=0, warm_start=False)</span><br><span class="line"></span><br><span class="line">In [6]:  clf.predict([[2., 2.]])</span><br><span class="line">Out[6]: array([1])</span><br><span class="line"></span><br><span class="line">In [7]: clf.coef_ </span><br><span class="line">Out[7]: array([[ 9.91080278,  9.91080278]])</span><br><span class="line"></span><br><span class="line">In [8]: clf.intercept_ </span><br><span class="line">Out[8]: array([-9.97004991])</span><br></pre></td></tr></table></figure><p>参考：</p><ul><li><a href="https://www.cnblogs.com/pinard/p/5970503.html" target="_blank" rel="external">https://www.cnblogs.com/pinard/p/5970503.html</a></li><li><a href="http://blog.csdn.net/uestc_c2_403/article/details/74910107" target="_blank" rel="external">http://blog.csdn.net/uestc_c2_403/article/details/74910107</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在机器学习领域，体梯度下降算法分为三种&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;批量梯度下降算法（BGD，Batch gradient descent algorithm）&lt;/li&gt;
&lt;li&gt;随机梯度下降算法（SGD，Stochastic gradient descent algorit
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="梯度下降" scheme="http://thinkgamer.cn/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
  </entry>
  
  <entry>
    <title>梯度算法之梯度上升和梯度下降</title>
    <link href="http://thinkgamer.cn/2017/12/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95%E4%B9%8B%E6%A2%AF%E5%BA%A6%E4%B8%8A%E5%8D%87%E5%92%8C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    <id>http://thinkgamer.cn/2017/12/14/机器学习/梯度算法之梯度上升和梯度下降/</id>
    <published>2017-12-14T06:11:11.000Z</published>
    <updated>2019-04-13T04:37:16.080Z</updated>
    
    <content type="html"><![CDATA[<p>第一次看见随机梯度上升算法是看《机器学习实战》这本书，当时也是一知半解，只是大概知道和高等数学中的函数求导有一定的关系。下边我们就好好研究下随机梯度上升（下降）和梯度上升（下降）。</p><h1 id="高数中的导数"><a href="#高数中的导数" class="headerlink" title="高数中的导数"></a>高数中的导数</h1><p>设导数 y = f(x) 在 $ x_0 $的某个邻域内有定义，当自变量从 $ x_0 $ 变成</p><script type="math/tex; mode=display">x_{0} + \Delta x</script><p>函数y=f(x)的增量</p><script type="math/tex; mode=display">\Delta y = f(x_0 + \Delta x) - f(x_0)</script><p>与自变量的增量 $ \Delta x $ 之比：</p><script type="math/tex; mode=display">\frac{ \Delta y }{ \Delta x } = \frac{ f(x_0 + \Delta x)-f(x_0) }{ \Delta x }</script><p>称为f(x)的平均变化率。<br>如 $ \Delta x \rightarrow 0 $ 平均变化率的极限</p><script type="math/tex; mode=display">\lim_{\Delta x \rightarrow 0} \frac{ \Delta y }{ \Delta x } = \lim_{\Delta x  \rightarrow 0} \frac{ f(x_0 + \Delta x)-f(x_0) }{ \Delta x }</script><p>存在，则称极限值为f(x)在$ x_0 $ 处的导数，并说f(x)在$ x_0 $ 处可导或有导数。当平均变化率极限不存在时，就说f(x)在 $ x_0 $ 处不可导或没有导数。</p><p>关于导数的说明</p><p>1）点导数是因变量在$ x_0 $ 处的变化率，它反映了因变量随自变量的变化而变化的快慢成都</p><p>2）如果函数y = f(x)在开区间 I 内的每点都可导，就称f(x)在开区间 I 内可导</p><p>3）对于任一 x 属于 I ，都对应着函数f(x)的一个导数，这个函数叫做原来函数f(x)的导函数</p><p>4）导函数在x1 处 为 0，若 x&lt;1 时，f’(x) &gt; 0 ，这 f(x) 递增，若f’(x)&lt;0 ，f(x)递减</p><p>5）f’(x0) 表示曲线y=f(x)在点 （x0,f($x_0$)）处的切线斜率</p><h1 id="偏导数"><a href="#偏导数" class="headerlink" title="偏导数"></a>偏导数</h1><p>函数z=f(x,y)在点(x0,y0)的某一邻域内有定义，当y固定在y0而x在 $x_0$ 处有增量$ \Delta x $ 时，相应的有函数增量</p><script type="math/tex; mode=display">f(x_0 + \Delta x, y_0) - f(x_0,y_0)</script><p>如果</p><script type="math/tex; mode=display">\lim_{\Delta x\rightarrow 0 } \frac {f(x_0 + \Delta x, y_0) - f(x_0,y_0)}{\Delta x}</script><p>存在，则称z=f(x,y)在点($x_0$,$y_0$)处对x的偏导数，记为：$ f_x(x_0,y_0) $</p><p>如果函数z=f(x,y)在区域D内任一点(x,y)处对x的偏导数都存在，那么这个偏导数就是x,y的函数，它就称为函数z=f(x,y)对自变量x的偏导数，记做</p><script type="math/tex; mode=display">\frac{ \partial z }{ \partial x } , \frac{ \partial f }{ \partial x } , z_x , f_x(x,y),</script><p>偏导数的概念可以推广到二元以上的函数，如 u = f(x,y,z)在x,y,z处</p><script type="math/tex; mode=display">f_x(x,y,z)=\lim_{\Delta x \rightarrow 0} \frac{f(x + \Delta x,y,z) -f(x,y,z)}{\Delta x}</script><script type="math/tex; mode=display">f_y(x,y,z)=\lim_{\Delta y \rightarrow 0} \frac{f(x,y + \Delta y,z) -f(x,y,z)}{\Delta y}</script><script type="math/tex; mode=display">f_z(x,y,z)=\lim_{\Delta z \rightarrow 0} \frac{f(x,y,z + \Delta z) -f(x,y,z)}{\Delta z}</script><p>可以看出导数与偏导数本质是一致的，都是自变量趋近于0时，函数值的变化与自变量的变化量比值的极限，直观的说，偏导数也就是函数在某一点沿坐标轴正方向的变化率。</p><p>区别：<br>导数指的是一元函数中，函数y=f(x)某一点沿x轴正方向的的变化率；<br>偏导数指的是多元函数中，函数y=f(x,y,z)在某一点沿某一坐标轴正方向的变化率。</p><p>偏导数的几何意义：<br>偏导数$ z = f_x(x_0,y_0)$表示的是曲面被 $ y=y_0 $ 所截得的曲线在点M处的切线$ M_0T_x $对x轴的斜率<br>偏导数$ z = f_y(x_0,y_0)$表示的是曲面被 $ x=x_0 $ 所截得的曲线在点M处的切线$ M_0T_y $对y轴的斜率</p><p>例子：<br>求 $z = x^2 + 3 xy+y^2 $在点(1,2)处的偏导数。</p><script type="math/tex; mode=display">\frac{ \partial z}{\partial x} = 2x +3y</script><script type="math/tex; mode=display">\frac{ \partial z}{\partial y} = 2y +3x</script><p>所以:<br>$z_x(x=1,y=2) = 8$<br>$z_y(x=1,y=2) = 7$</p><h1 id="方向导数"><a href="#方向导数" class="headerlink" title="方向导数"></a>方向导数</h1><script type="math/tex; mode=display">\frac{ \partial }{ \partial l }  f(x_0,x_1,...,x_n) = \lim_{\rho \rightarrow 0} \frac{\Delta y}{ \Delta x } = \lim_{\rho \rightarrow 0} \frac{ f(x_0 + \Delta x_0,...,x_j + \Delta x_j,...,x_n + \Delta x_n)-f(x_0,...,x_j,...,x_n)}{ \rho }</script><script type="math/tex; mode=display">\rho = \sqrt{ (\Delta x_0)^{2} +...+(\Delta x_j)^{2}+...+(\Delta x_n)^{2}}</script><p>前边导数和偏导数的定义中，均是沿坐标轴正方向讨论函数的变化率。那么当讨论函数沿任意方向的变化率时，也就引出了方向导数的定义，即：某一点在某一趋近方向上的导数值。</p><p>通俗的解释是： 我们不仅要知道函数在坐标轴正方向上的变化率（即偏导数），而且还要设法求得函数在其他特定方向上的变化率。而方向导数就是函数在其他特定方向上的变化率。 　</p><h1 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h1><p>与方向导数有一定的关联，在微积分里面，对多元函数的参数求 $ \partial  $ 偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数f(x,y), 分别对x,y求偏导数，求得的梯度向量就是 $<br>( \frac{ \partial f }{ \partial x },\frac{ \partial f }{ \partial y })^T<br>$ ,简称grad f(x,y)或者 $▽f(x,y)$。对于在点$(x_0,y_0)$的具体梯度向量就是$( \frac{ \partial f }{ \partial x_0 },\frac{ \partial f }{ \partial y_0 })^T$.或者$▽f(x_0,y_0)$，如果是3个参数的向量梯度，就是 $( \frac{ \partial f }{ \partial x },\frac{ \partial f }{ \partial y },\frac{ \partial f }{ \partial z })^T$,以此类推。</p><p>那么这个梯度向量求出来有什么意义呢？他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数f(x,y),在点$(x_0,y_0)$，沿着梯度向量的方向就是$( \frac{ \partial f }{ \partial x_0 },\frac{ \partial f }{ \partial y_0 })^T$的方向是f(x,y)增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是 $-( \frac{ \partial f }{ \partial x_0 },\frac{ \partial f }{ \partial y_0 })^T$的方向，梯度减少最快，也就是更加容易找到函数的最小值。</p><p>例如：<br>函数 $f(x,y) = \frac{1}{x^2+y^2} $ ，分别对x，y求偏导数得：</p><script type="math/tex; mode=display"> \frac{ \partial f }{ \partial x}=-\frac{2x}{ (x^2+y^2)^2}</script><script type="math/tex; mode=display"> \frac{ \partial f }{ \partial y}=-\frac{2y}{ (x^2+y^2)^2}</script><p>所以</p><script type="math/tex; mode=display">grad( \frac{1}{x^2+y^2} ) = (-\frac{2x}{ (x^2+y^2)^2} ,-\frac{2y}{ (x^2+y^2)^2})</script><p>函数在某一点的梯度是这样一个向量，它的方向与取得最大方向导数的方向一致，而它的模为方向导数的最大值。 </p><p>注意点：<br>1）梯度是一个向量<br>2）梯度的方向是最大方向导数的方向<br>3）梯度的值是最大方向导数的值</p><h1 id="梯度下降与梯度上升"><a href="#梯度下降与梯度上升" class="headerlink" title="梯度下降与梯度上升"></a>梯度下降与梯度上升</h1><p>在机器学习算法中，在最小化损失函数时，可以通过梯度下降思想来求得最小化的损失函数和对应的参数值，反过来，如果要求最大化的损失函数，可以通过梯度上升思想来求取。</p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><h3 id="关于梯度下降的几个概念"><a href="#关于梯度下降的几个概念" class="headerlink" title="关于梯度下降的几个概念"></a>关于梯度下降的几个概念</h3><p>1）步长（learning rate）：步长决定了在梯度下降迭代过程中，每一步沿梯度负方向前进的长度<br>2）特征（feature）：指的是样本中输入部门，比如样本（x0，y0），（x1，y1），则样本特征为x，样本输出为y<br>3）假设函数（hypothesis function）：在监督学习中，为了拟合输入样本，而使用的假设函数，记为$h_θ(x)$。比如对于样本$（x_i,y_i）(i=1,2,…n)$,可以采用拟合函数如下： $h_θ(x) = θ0+θ1_x$。<br>4）损失函数（loss function）：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于样本（xi,yi）(i=1,2,…n),采用线性回归，损失函数为：</p><script type="math/tex; mode=display">\jmath (\theta _0,\theta _1)=\sum_{i=0}^{m}( h_\theta(x_i)-y_i )^2</script><p>其中$x_i$表示样本特征x的第i个元素，$y_i$表示样本输出y的第i个元素，$h_\theta(x_i)$ 为假设函数。</p><h3 id="梯度下降的代数方法描述"><a href="#梯度下降的代数方法描述" class="headerlink" title="梯度下降的代数方法描述"></a>梯度下降的代数方法描述</h3><ol><li><p>先决条件：确定优化模型的假设函数和损失函数<br>这里假定线性回归的假设函数为$h_\theta(x_1,x_2,…x_n)=\theta_0+\theta_1x_1+…+\theta_nx_n$，其中 $\theta _i(i=0,1,2…n)$ 为模型参数(公式中用$\theta$代替)，$x_i(i=0,1,2…n)$为每个样本的n个特征值。</p><p>则对应选定得损失函数为：</p><script type="math/tex; mode=display">\jmath (\theta _0,\theta _1,...,,\theta _n)=\sum_{i=0}^{m}( h_\theta(x_0,x_1,...,x_n)-y_i )^2</script></li><li><p>算法相关参数的初始化<br>主要是初始化 $ \theta _0,\theta _1…,\theta _n$，算法终止距离 $\varepsilon $ 以及步长 $ \alpha $。在没有任何先验知识的时候，我喜欢将所有的 $\theta$ 初始化为0， 将步长初始化为1。在调优的时候再优化。</p></li><li><p>算法过程</p></li></ol><ul><li><p>1)：确定当前损失函数的梯度，对于$\theta _i $，其梯度表达式为：</p><script type="math/tex; mode=display">\frac{\partial }{\partial \theta _i}\jmath (\theta _1,\theta _2,...,\theta _n)</script></li><li><p>2)：用步长乘以损失函数的梯度，得到当前位置的下降距离，即</p><script type="math/tex; mode=display">\alpha \frac{\partial \jmath (\theta _1,\theta _2,...,\theta _n)}{\partial \theta _i}</script></li><li><p>3)：确定是否所有的$\theta _i$ ，梯度下降的距离都小于 $ \varepsilon $，如果小于$ \varepsilon $，则算法停止，当前所有的 $\theta _i(i=1,2,3,…,n)$ 即为最终结果。否则执行下一步。</p></li><li><p>4)：更新所有的 $\theta$，对于$\theta _i $，其更新表达式如下。更新完毕后继续转入步骤1)。</p><script type="math/tex; mode=display">\theta _i = \theta _i - \alpha \frac{\partial \jmath (\theta _1,\theta _2,...,\theta _n)}{\partial \theta _i}</script><h3 id="梯度下降的矩阵方式描述"><a href="#梯度下降的矩阵方式描述" class="headerlink" title="梯度下降的矩阵方式描述"></a>梯度下降的矩阵方式描述</h3><ol><li>先决条件：确定优化模型的假设函数和损失函数<br>这里假定线性回归的假设函数为$h_\theta(x_1,x_2,…x_n)=\theta_0+\theta_1x_1+…+\theta_nx_n$，其中 $\theta _i(i=0,1,2…n)$ 为模型参数，$x_i(i=0,1,2…n)$为每个样本的n个特征值。<br>假设函数对应的矩阵表示为：$ h_\theta (x) = X \theta $，假设函数 $h_\theta(x)$ 为mx1的向量，$\theta $ 为nx1的向量，里面有n个代数法的模型参数。X为mxn维的矩阵。m代表样本的个数，n代表样本的特征数。<br>则对应选定得损失函数为：<script type="math/tex; mode=display">\jmath (\theta)=(X \theta −Y)^T (X \theta−Y)</script>其中YY是样本的输出向量，维度为m*1<br><br><br>2.算法相关参数初始化:<br>$\theta$ 向量可以初始化为默认值，或者调优后的值。算法终止距离 $\varepsilon $ ，步长 $\alpha$ 和 “梯度下降的代数方法”描述中一致。<br><br><br>3.算法过程</li></ol></li><li><p>1)：确定当前位置的损失函数的梯度，对于 $ \theta $ 向量,其梯度表达式如下：</p><script type="math/tex; mode=display">\frac{ \partial }{\partial \theta } \jmath (\theta)</script></li><li>2)：用步长乘以损失函数的梯度，得到当前位置下降的距离，即 $\alpha \frac{ \partial }{\partial \theta } \jmath (\theta)$ </li><li>3)：确定 $\theta$ 向量里面的每个值,梯度下降的距离都小于 $\varepsilon$，如果小于 $\varepsilon$ 则算法终止，当前 $\theta$ 向量即为最终结果。否则进入步骤4)</li><li>4)：更新 $\theta$ 向量，其更新表达式如下。更新完毕后继续转入步骤1)<script type="math/tex; mode=display">\theta =\theta - \alpha \frac{ \partial }{\partial \theta } \jmath (\theta)</script></li></ul><h2 id="梯度上升"><a href="#梯度上升" class="headerlink" title="梯度上升"></a>梯度上升</h2><p>梯度上升和梯度下降的分析方式是一致的，只不过把 $ \theta $ 的更新中 减号变为加号。</p><h2 id="梯度下降的算法优化"><a href="#梯度下降的算法优化" class="headerlink" title="梯度下降的算法优化"></a>梯度下降的算法优化</h2><ol><li><p>算法的步长选择。在前面的算法描述中，我提到取步长为1，但是实际上取值取决于数据样本，可以多取一些值，从大到小，分别运行算法，看看迭代效果，如果损失函数在变小，说明取值有效，否则要增大步长。前面说了。步长太大，会导致迭代过快，甚至有可能错过最优解。步长太小，迭代速度太慢，很长时间算法都不能结束。所以算法的步长需要多次运行后才能得到一个较为优的值。</p></li><li><p>算法参数的初始值选择。 初始值不同，获得的最小值也有可能不同，因此梯度下降求得的只是局部最小值；当然如果损失函数是凸函数则一定是最优解。由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。</p></li></ol><p>3.归一化。由于样本不同特征的取值范围不一样，可能导致迭代很慢，为了减少特征取值的影响，可以对特征数据归一化，也就是对于每个特征x，求出它的均值 $\bar{x}$ 和标准差std(x)，然后转化为：</p><script type="math/tex; mode=display">\frac{x - \bar{x}}{std(x)}</script><p>这样特征的新期望为0，新方差为1，迭代次数可以大大加快。</p><hr><p><a href="http://blog.csdn.net/walilk/article/details/50978864" target="_blank" rel="external">http://blog.csdn.net/walilk/article/details/50978864</a></p><p><a href="https://www.zhihu.com/question/24658302" target="_blank" rel="external">https://www.zhihu.com/question/24658302</a></p><p><a href="https://www.cnblogs.com/pinard/p/5970503.html" target="_blank" rel="external">https://www.cnblogs.com/pinard/p/5970503.html</a></p><p><a href="http://www.doc88.com/p-7844239247737.html" target="_blank" rel="external">http://www.doc88.com/p-7844239247737.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;第一次看见随机梯度上升算法是看《机器学习实战》这本书，当时也是一知半解，只是大概知道和高等数学中的函数求导有一定的关系。下边我们就好好研究下随机梯度上升（下降）和梯度上升（下降）。&lt;/p&gt;
&lt;h1 id=&quot;高数中的导数&quot;&gt;&lt;a href=&quot;#高数中的导数&quot; class=&quot;h
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="梯度下降" scheme="http://thinkgamer.cn/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
  </entry>
  
</feed>
