<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>文艺与Code | Thinkgamer的博客</title>
  <icon>https://www.gravatar.com/avatar/1b9c8afc3fc1dc6be26316835c6f4fc4</icon>
  <subtitle>CTR/DL/ML/RL</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://thinkgamer.cn/"/>
  <updated>2019-04-13T04:40:32.345Z</updated>
  <id>http://thinkgamer.cn/</id>
  
  <author>
    <name>Thinkgamer</name>
    <email>thinkgamer@163.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Thinkgamer&#39;s 简历</title>
    <link href="http://thinkgamer.cn/8888/08/08/%E5%85%B3%E4%BA%8E%E6%88%91/"/>
    <id>http://thinkgamer.cn/8888/08/08/关于我/</id>
    <published>8888-08-08T00:08:08.000Z</published>
    <updated>2019-04-13T04:40:32.345Z</updated>
    
    <content type="html"><![CDATA[<hr><h1 id="个人信息"><a href="#个人信息" class="headerlink" title="个人信息"></a>个人信息</h1><p>唯一：&nbsp;&nbsp;&nbsp;&nbsp;<strong>Thinkgamer</strong><br>姓名：&nbsp;&nbsp;&nbsp;&nbsp;<strong>高阳团</strong><br>家乡：&nbsp;&nbsp;&nbsp;&nbsp;<strong>河南-郑州</strong><br>电话：&nbsp;&nbsp;&nbsp;&nbsp;<strong>17600977634</strong><br>邮箱：&nbsp;&nbsp;&nbsp;&nbsp;<strong>thinkgamer@163.com</strong><br>毕业：&nbsp;&nbsp;&nbsp;&nbsp;<strong>沈阳航空航天大学-计算机学院-软件工程</strong><br>就职：&nbsp;&nbsp;&nbsp;&nbsp;<strong>京东商城 | 算法工程师</strong></p><hr><h1 id="技术园地："><a href="#技术园地：" class="headerlink" title="技术园地："></a>技术园地：</h1><ul><li>CSDN：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a></li><li>Github：<a href="https://github.com/Thinkgamer" target="_blank" rel="external">https://github.com/Thinkgamer</a></li><li>知乎: <a href="https://www.zhihu.com/people/thinkgamer/activities" target="_blank" rel="external">https://www.zhihu.com/people/thinkgamer/activities</a></li><li>公众号：数据与算法联盟<br><img src="/assets/img/gongzhonghao.jpg" weight="250px" height="250px"></li></ul><hr><h1 id="工作经历"><a href="#工作经历" class="headerlink" title="工作经历"></a>工作经历</h1><h2 id="2017-12-25～至今-京东商城"><a href="#2017-12-25～至今-京东商城" class="headerlink" title="2017-12-25～至今 | 京东商城"></a>2017-12-25～至今 | 京东商城</h2><ul><li>个性化消息Push</li></ul><blockquote><p>简称“种草”。开发种草整个联动方案，负责相关资源安排沟通，充分理解业务，个性化消息push模型开发训练。</p><p>深度学习模型调研，基于公司内部平台，上线基于tensorflow-serving的深度学习模型，效果较ML模型提升显著。</p></blockquote><ul><li>Plus会员个性化推荐</li></ul><blockquote><p>理解内部推荐架构原理，开发方案，资源安排，个性化推荐模型开发训练，相关CTR预估模型研究与离线测试。</p></blockquote><ul><li>商品价格段模型</li></ul><blockquote><p>基于KMeans构建商品价格段模型。</p></blockquote><ul><li>商品质量分模型</li></ul><blockquote><p>基于线性模型构建商品质量分模型，用户推荐架构中的召回粗排。 </p></blockquote><ul><li>特征监控模型</li></ul><blockquote><p>数据和特征决定了机器学习的上限,而模型和算法只是逼近这个上限。特征对于模型来说极其重要，因为对于特征的监控十分有必要，该模型支持使用者自定义监控指标和监控字段，能够有效的减少出现问题时的排查时间提高效果，并进行预警。</p></blockquote><ul><li>基础数据开发<ul><li>业务内特征开发</li><li>全站特征开发</li><li>召回数据开发</li></ul></li></ul><h2 id="2016-10～2017-12-北京万维星辰科技有限公司"><a href="#2016-10～2017-12-北京万维星辰科技有限公司" class="headerlink" title="2016-10～2017-12 | 北京万维星辰科技有限公司"></a>2016-10～2017-12 | 北京万维星辰科技有限公司</h2><ul><li>搭建基于 Hadoop 和 ELK 技术栈的日志分析系统</li></ul><blockquote><p>参与设计了基于 ELK 的日志分析系统，提出并搭建了 Hadoop 数据备份系统，研究了 ELK 周边的<br>开源产品 ，学习并使用 rails 实现 es 数据的快照备份。</p></blockquote><ul><li>异常检测算法研究与实现</li></ul><blockquote><p>1：根据合作方提供的 wlan 上网数据，对用户进行肖像刻画，从而对后入数据进行异常值估计。</p><p>2：研究基于指数平滑和线性回归的异常值检测，并使用 python 的 elasticsearch 进行实现。</p></blockquote><ul><li>中彩/德州银行日志审计项目</li></ul><blockquote><p>利用公司的日志分析系统对中国福利彩票和德州银行的日志进行分析，并形成安全事件，提出相应的整改和解决意见，形成月度报告。</p></blockquote><h2 id="2016-07～2016-09-北京广联达软件有限公司"><a href="#2016-07～2016-09-北京广联达软件有限公司" class="headerlink" title="2016-07～2016-09 | 北京广联达软件有限公司"></a>2016-07～2016-09 | 北京广联达软件有限公司</h2><blockquote><p>实习以课题形式（课题为：基于质量数据的数据分析平台搭建）展开，利用 Hadoop 等开源组件搭建了 5 台分布式系统，包含 Hadoop，Hive，Spark，Zookeeper，Sqoop 和 Hbase，在该平台上完成了豆瓣影评数据分析 Demo</p></blockquote><hr><h1 id="大学经历"><a href="#大学经历" class="headerlink" title="大学经历"></a>大学经历</h1><h2 id="项目经历"><a href="#项目经历" class="headerlink" title="项目经历"></a>项目经历</h2><ul><li>基于 Hadoop 和机器学习的博客统计分析平台</li></ul><blockquote><p>采用 Django 作为 Web 开发基础，Python 爬取了 CSDN 博客的部分数据，存储到 hdfs 上，利用 MapReduce 对数据进行了离线计算，将解析好的字段存储到 Hive 中，利用 python 开发实现了协同过滤算法和 PangRank 算法。最终此项目在辽宁省计算机作品大赛中获得二等奖，中国大学生计算机作品大赛中获得三等奖。</p></blockquote><ul><li>图书推荐系统</li></ul><blockquote><p>python 爬取了豆瓣图书数据，对数据进行清洗之后，使用基于 Item 和 User 的协同过滤算法对登录用户产生图书推荐，此项目为大三期间为一个网友做的毕业设计。</p></blockquote><h2 id="荣誉奖励"><a href="#荣誉奖励" class="headerlink" title="荣誉奖励"></a>荣誉奖励</h2><ul><li>单项一等奖学金  * 2</li><li>综合二等奖学金  * 2</li><li>单项支援服务标兵</li><li>优秀团干 * 2</li><li>辽宁省ACM优秀志愿者</li><li>校ACM三等奖</li><li>沈阳航空航天大学计算机作品大赛二等奖【网站】</li><li>辽宁省计算机作品大赛二等奖【博客统计分析系统】</li><li>中国大学生作品大赛三等奖【博客统计分析系统】</li></ul><h2 id="工作经历-1"><a href="#工作经历-1" class="headerlink" title="工作经历"></a>工作经历</h2><ul><li>助理辅导员 | 2014.09-2015.07</li></ul><blockquote><p>计算机学院 2014 级新生助理辅导员，协助辅导员进行大一班级的日常管理</p></blockquote><ul><li>活动部部长 | 2014.09-2015.07</li></ul><blockquote><p>爱心联合会活动部部长，负责相关活动的宣传推广与执行</p></blockquote><ul><li>班级团支书 | 2013.09-2017.06</li></ul><blockquote><p>协助辅导员进行班级日常的管理和相关共青团工作的开展</p></blockquote><hr><h1 id="自我评价"><a href="#自我评价" class="headerlink" title="自我评价"></a>自我评价</h1><ul><li>不服输，爱钻研，具有一定程度自学能力和解决问题能力。</li><li>喜欢看书，看文章，整理笔记。</li><li>自我的文艺青年。</li></ul><hr><h1 id="联系我："><a href="#联系我：" class="headerlink" title="联系我："></a>联系我：</h1><ul><li><img src="/assets/img/myweixin.png" height="300" width="220"></li></ul><p>【加我微信，拉你进数据与算法交流群，每天都有技术讨论】</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;h1 id=&quot;个人信息&quot;&gt;&lt;a href=&quot;#个人信息&quot; class=&quot;headerlink&quot; title=&quot;个人信息&quot;&gt;&lt;/a&gt;个人信息&lt;/h1&gt;&lt;p&gt;唯一：&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;strong&gt;Thinkgamer&lt;/strong&gt;&lt;br
      
    
    </summary>
    
    
      <category term="Thinkgamer" scheme="http://thinkgamer.cn/tags/Thinkgamer/"/>
    
  </entry>
  
  <entry>
    <title>商务合作介绍</title>
    <link href="http://thinkgamer.cn/6666/06/06/%E5%95%86%E5%8A%A1%E5%90%88%E4%BD%9C%E4%BB%8B%E7%BB%8D/"/>
    <id>http://thinkgamer.cn/6666/06/06/商务合作介绍/</id>
    <published>6666-06-05T16:00:00.000Z</published>
    <updated>2019-04-13T04:40:15.346Z</updated>
    
    <content type="html"><![CDATA[<hr><center><h1>WelCome To “Thinkgamer 小站”</h1></center><hr><center><h2>合作范围</h2></center><div class="table-container"><table><thead><tr><th style="text-align:center">Web全栈</th><th style="text-align:center">数据服务</th><th style="text-align:center">模型构建</th></tr></thead><tbody><tr><td style="text-align:center">:-:</td><td style="text-align:center">:-:</td><td style="text-align:center">:-: </td></tr><tr><td style="text-align:center">论文算法实现</td><td style="text-align:center">大数据服务</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">:-:</td><td style="text-align:center">:-:</td><td style="text-align:center">:-: </td></tr><tr><td style="text-align:center">跟拍摄影</td><td style="text-align:center">广告接入</td><td style="text-align:center"></td></tr></tbody></table></div><blockquote><p>全网唯一ID：Thinkgamer，左侧”关于我“关注微信公众号”数据与算法联盟“，可在公众号添加我的微信，本人涉猎范围包括：推荐系统，Python，机器学习，Web开发，大数据云计算，ELK。</p></blockquote><h1 id="▶-Web全栈"><a href="#▶-Web全栈" class="headerlink" title="▶ Web全栈"></a>▶ Web全栈</h1><blockquote><p>如果您在创业，苦于没有额外精力管理一个技术团队；如果您在工作，遇到了一些您解决不了的问题；如果您的网站苦于没有运维；如果您的数据需要备份；如果一切有关Web开发运维的问题。您都可以来找我，我虽不是最厉害的，但绝对会为您提供最优质的服务。</p></blockquote><h1 id="▶-数据服务"><a href="#▶-数据服务" class="headerlink" title="▶ 数据服务"></a>▶ 数据服务</h1><blockquote><p>包含但不局限于以下数据相关的服务：</p></blockquote><ul><li>数据采集（一次性和程序开发）</li><li>数据清洗</li><li>数据可视化（不限于Web）</li><li>数据存储方案设计与实现</li><li>… …</li></ul><p>本人曾多次向他人提供数据相关的技术服务，积累了一定的经验，相信能够为您提供全方位的数据服务。</p><h1 id="▶-模型构建"><a href="#▶-模型构建" class="headerlink" title="▶ 模型构建"></a>▶ 模型构建</h1><blockquote><p>根据对方提供的具体业务场景，进行相关模型选择与构建。当然，如果有荣幸参与您的场景选定和数据准备阶段，也是极好的。</p></blockquote><h1 id="▶-论文算法实现"><a href="#▶-论文算法实现" class="headerlink" title="▶ 论文算法实现"></a>▶ 论文算法实现</h1><blockquote><p>如果您是一个马上要毕业的本科或者研究生，如果您苦于论文的立项与项目实现，如果您没有更好的主意，欢迎您来找我，加我的个人微信，为您的毕业保驾护航。</p></blockquote><h1 id="▶-大数据与分布式计算"><a href="#▶-大数据与分布式计算" class="headerlink" title="▶ 大数据与分布式计算"></a>▶ 大数据与分布式计算</h1><blockquote><p>提供大数据相关的服务，包含但不局限于：</p></blockquote><ul><li>大数据分析平台方案设计</li><li>大数据分析平台搭建</li><li>基于平台的数据分析Demo实现</li><li>海量数据的分布式计算处理</li><li>… …</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;欢迎来找我，24小时在线。</p><h1 id="▶-广告接入"><a href="#▶-广告接入" class="headerlink" title="▶ 广告接入"></a>▶ 广告接入</h1><blockquote><p>眼前的黑不是黑，Ta们说的白是什么白，也许一直是我们忘了搭一座桥，到对方的心里瞧一瞧。你的品牌，你的知名度为什么那么低，因为你没有使用我的广告接入，那么问题来了，包含但不局限于以下几种情况的，可以加我微信私聊了：</p></blockquote><ul><li>品牌宣传</li><li>广告位接入</li><li>公众号互相推广</li><li>个人网站/社区主页链接</li><li>… …</li></ul><h1 id="▶-跟拍摄影"><a href="#▶-跟拍摄影" class="headerlink" title="▶ 跟拍摄影"></a>▶ 跟拍摄影</h1><blockquote><p>如果您在旅游途中缺少了一个摄影的小跟班；如果您苦于找不到好的角度拍照；如果您是一个人，苦于没有人照出你的美；如果您的照片需要美化与调整。那么请您来找我，保证为您提供最优质的技术与服务。</p><p>业务涉及：</p></blockquote><ul><li>跟拍摄影</li><li>照片美化与调整</li><li>PS技术服务</li></ul><center><font color="#0099ff" size="8" face="黑体">小本生意&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;诚信经验<br><br>大神勿扰&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;自求多福</font> </center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;center&gt;
&lt;h1&gt;WelCome To “Thinkgamer 小站”&lt;/h1&gt;
&lt;/center&gt;

&lt;hr&gt;
&lt;center&gt;
&lt;h2&gt;合作范围&lt;/h2&gt;
&lt;/center&gt;

&lt;div class=&quot;table-container&quot;&gt;
&lt;table&gt;
&lt;t
      
    
    </summary>
    
    
      <category term="商务合作" scheme="http://thinkgamer.cn/tags/%E5%95%86%E5%8A%A1%E5%90%88%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>线性模型篇之softmax数学公式推导</title>
    <link href="http://thinkgamer.cn/2019/04/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%AF%87%E4%B9%8Bsoftmax%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/"/>
    <id>http://thinkgamer.cn/2019/04/07/机器学习/线性模型篇之softmax数学公式推导/</id>
    <published>2019-04-06T23:24:46.000Z</published>
    <updated>2019-04-13T06:01:56.575Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><blockquote><p>Softmax回归也称多项（multinomial）或者多类（multi-class）的Logistic回归，是Logistic回归在多类分类问题上的推广。和逻辑回归一样属于线性模型。</p></blockquote><h1 id="SoftMax回归简介"><a href="#SoftMax回归简介" class="headerlink" title="SoftMax回归简介"></a>SoftMax回归简介</h1><p>对于多类问题，类别标签</p><script type="math/tex; mode=display">y \in {1,2,3,...,C}</script><p>可以用C个取值，给定一个样本x，softmax回归预测的是属于类别c的概率为(公式-1)：</p><script type="math/tex; mode=display">p(y=c|x)=softmax(w_c^Tx)=\frac{exp(w_c^Tx)}{\sum_{c=1}^{C}exp(w_c^Tx)}</script><p>其中w_c是第c类的权重向量。</p><p>softmax回归的决策函数可以表示为(公式-2)：</p><script type="math/tex; mode=display">\hat{y}=  \underset{c=1}{ \overset{C}{arg max} } \ p(y=c|x) =\underset{c=1}{ \overset{C}{arg max} } \ w_c^T x</script><hr><p>softMax与Logistic回归的关系：</p><p>当类别个C=2时，softMax回归的决策函数为(公式-3)：</p><script type="math/tex; mode=display">\hat{y} = \underset{y\in {0,1}}{ arg max } \ w_y^Tx=I(w_1^Tx - w_0^Tx >0 )=I((w_1 - w_0)^Tx >0 )</script><p>其中I(.)是指示函数，对比二分类决策函数(公式-4)</p><script type="math/tex; mode=display">g(f(x,w))=sgn(f(x,w))=\begin{cases} & +1 \text{ if } f(x,w)>0 \\  & -1 \text{ if } f(x,w)<0 \end{cases}</script><p>其中sgn表示符号函数(sign function)，可以发现两类分类中的权重向量w=w1-w0</p><hr><p>向量表示：</p><p>公式-1用向量形式可以写为(公式-5)</p><script type="math/tex; mode=display">\hat{y}=softmax(W^Tx)=\frac{erp(W^Tx)}{1^Texp(W^Tx)}</script><p>其中W=[w_1,w_2,…,w_C]是由C个类的权重向量组成的矩阵，1为全1的向量，</p><script type="math/tex; mode=display">\hat{y}\in  R^C</script><p>为所有类别的预测条件概率组成的向量，第c维的值是第c类的预测条件概率。</p><h1 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h1><p>给定N个训练样本{(x^n, y^n)},n&lt;=N，softmax回归使用交叉熵损失函数来学习最优的参数矩阵W。</p><p>这里用C维的one-hot向量</p><script type="math/tex; mode=display">y \in {0,1} ^C</script><p>来表示类别标签，其向量表示为(公式-6)：</p><script type="math/tex; mode=display">y = [I(1=c),I(2=c),...,I(C=c)]^T</script><p>其中I(.)为指示函数。</p><p>采用交叉熵损失函数，softmax的经验风险函数为(公式-7)：</p><script type="math/tex; mode=display">R(W)=-\frac{1}{N}\sum_{n=1}^{N}\sum_{c=1}^{C}y_c^nlog\hat{y}_c^nR(W)=-\frac{1}{N}\sum_{n=1}^{N} (y^n)^Tlog\hat{y}^n</script><p>其中</p><script type="math/tex; mode=display">\hat{y}^n = softmax(W^Tx^n)</script><p>为样本x^n在每个类别的后验概率。</p><p>==说明：公式-7第一个式变换到第二个式是因为y_c类别中只有一个为1，其余为0，所以将第二个求和去除。==</p><p>风险函数R(W)关于W的梯度为(公式-8)：</p><script type="math/tex; mode=display">\frac{\partial R(W)}{\partial W} = -\frac{1}{N}\sum_{n=1}^{N}x^n(y^n-\hat{y}^n)^T</script><p>==<strong>证明：</strong>==</p><p>计算公式-8中的梯度，关键在于计算每个样本的损失函数</p><script type="math/tex; mode=display">L^n(W)=-(y^n)^Tlog\hat{y}^n</script><p>关于参数W的梯度，其中需要用到两个导数公式为：</p><ul><li>若y=softmax(z)，则</li></ul><script type="math/tex; mode=display">\frac{\partial y}{\partial z}=diag(y)-yy^T</script><ul><li>若</li></ul><script type="math/tex; mode=display">z=W^Tx=[w_1^Tx,w_2^Tx,...,w_C^Tx]^T</script><p>则</p><script type="math/tex; mode=display">\frac{\partial y}{\partial w_c}</script><p>为第c列为x，其余为0的矩阵。</p><script type="math/tex; mode=display">\frac{\partial z}{\partial w_c} = [ \frac{\partial w_1^Tx}{\partial w_c},\frac{\partial w_2^Tx}{\partial w_c},...,\frac{\partial w_C^Tx}{\partial w_c} ]=[0,0,..,x,...,0]=M_c(x)</script><p>根据链式法则，</p><script type="math/tex; mode=display">L^n(W) = -(y^n)^T log\hat{y}^n</script><p>关于w_c的偏导数为(公式-12)：</p><script type="math/tex; mode=display">\frac{\partial L^n(W) }{\partial w_c}= -\frac{ \partial ((y^n)^T log \hat{y}^n) }{\partial w_c}</script><script type="math/tex; mode=display">= -\frac{\partial z^n}{ \partial w_c } \frac{\partial \hat{y}^n}{ \partial z^n }\frac{\partial log \hat{y}^n}{ \partial \hat{y}^n } y^n</script><script type="math/tex; mode=display">=-M_c(x^n)(diag(\hat{y}^n)-\hat{y}^n(\hat{y}^n)^T)(diag(\hat{y}^n))^{-1} y^n</script><script type="math/tex; mode=display">=-M_c(x^n)(I-\hat{y}^n1^T)y^n</script><script type="math/tex; mode=display">=-M_c(x^n)(y^n-\hat{y}^n1^Ty^n)</script><script type="math/tex; mode=display">=-M_c(x^n)(y^n-\hat{y}^n)</script><script type="math/tex; mode=display">=-x^n[y^n-\hat{y}^n]_c</script><p>公式-12也可以表示为非向量形式(公式-13)：</p><script type="math/tex; mode=display">\frac{\partial L^n(W) }{\partial w_c}= -x^n(I(y^n=c)-\hat{y}_c^n)</script><p>其中I(.)为指示函数，根据公式-12可以得到(公式-14)</p><script type="math/tex; mode=display">\frac{\partial L^n(W) }{\partial W} = -x^n(y^n-\hat{y}^n)^T</script><p>采用梯度下降法，softmax回归的训练过程为：初始化W_0 &lt;- 0，然后通过下式进行迭代更新。</p><script type="math/tex; mode=display">W_{t+1} = W_t + \alpha (\frac{1}{N} \sum_{n=1}^{N}x^n(y^n - \hat{y}_{W_t} ^ n)^T)</script><p>其中a是学习率，</p><script type="math/tex; mode=display">\hat{y}_{W_t}^n</script><p>是当参数为W_t时，softmax回归模型的输出。</p><hr><p><strong>注意：</strong></p><blockquote><p>softmax回归中使用的C个权重向量是冗余的，即对所有权重向量都减去一个同样的向量v，不改变其输出结果。因此，softmax往往需要正则化来约束参数。此外，可以利用这个特性来避免计算softmax函数时在数值计算上溢出问题。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="线性模型" scheme="http://thinkgamer.cn/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="SoftMax" scheme="http://thinkgamer.cn/tags/SoftMax/"/>
    
  </entry>
  
  <entry>
    <title>线性模型篇之Logistic Regression数学公式推导</title>
    <link href="http://thinkgamer.cn/2019/04/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%AF%87%E4%B9%8BLogistic%20Regression%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/"/>
    <id>http://thinkgamer.cn/2019/04/02/机器学习/线性模型篇之Logistic Regression数学公式推导/</id>
    <published>2019-04-02T14:31:51.000Z</published>
    <updated>2019-04-13T05:09:02.305Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="两分类与多分类"><a href="#两分类与多分类" class="headerlink" title="两分类与多分类"></a>两分类与多分类</h1><ul><li>两类分类（Binary Classification）<ul><li>类别标签y只有两种取值，通常设为{0，1}</li><li>线性判别函数，即形如 y = w^T*x + b</li><li>分割超平面（hyper plane）,由满足f(w,x)=0的点组成</li><li>决策边界（Decision boundary）、决策平面（Decision surface）：即分分割超平面，决策边界将特征空间一分为二，划分成两个区域，每个区域对应一个类别。</li><li>有向距离（signed distance）</li></ul></li><li>多样分类（Multi-class Classification）<ul><li>分类的类别个数大于2，多分类一般需要多个线性判别函数，但设计这些判别函数有很多方式。eg：<ul><li>一对其余：属于和不属于</li><li>一对一</li><li>argmax（改进的一对其余）：属于每个类别的概率，找概率最大值</li></ul></li><li>参考：<a href="https://blog.csdn.net/Gamer_gyt/article/details/86378882" target="_blank" rel="external">多分类实现方式介绍和在Spark上实现多分类逻辑回归</a></li></ul></li></ul><h1 id="Logistic回归"><a href="#Logistic回归" class="headerlink" title="Logistic回归"></a>Logistic回归</h1><h2 id="LR回归"><a href="#LR回归" class="headerlink" title="LR回归"></a>LR回归</h2><p>Logistic回归（Logistic Regression，LR）是一种常见的处理二分类的线性回归模型。</p><p>为了解决连续的线性回归函数不适合做分类的问题，引入函数g：R^d -&gt; (0,1)来预测类别标签的后验概率p(y=1 | x)</p><p>其中g(.)通常称为激活函数（activation function），其作用是把线性函数的值域从实数区间“挤压”到了（0，1）之间，可以用概率表示。在统计文献中，g(.)的逆函数g(.)^-1也称为联系函数（Link Function）</p><p>在逻辑回归中使用Logistic作为激活函数，标签y=1的后验概率为(公式-1)：</p><script type="math/tex; mode=display">p(y=1 | x) = \sigma (w^T x)</script><script type="math/tex; mode=display">p(y=1 | x)= \frac{1}{1+exp(-w^T x)}</script><p>标签 y=0的后验概率为(公式-2)：</p><script type="math/tex; mode=display">p(y=0 | x) =1-p(y=0 | x)</script><script type="math/tex; mode=display">p(y=0 | x)= \frac{exp(-w^T x)}{1+exp(-w^T x)}</script><p>将公式-1进行等价变换，可得(公式-3)：</p><script type="math/tex; mode=display">w^T x = log \frac{p(y=1 | x)}{1-p(y=1 | x)}</script><script type="math/tex; mode=display">w^T x = log \frac { p(y=1 | x)}{p(y=0|x)}</script><p>其中</p><script type="math/tex; mode=display">\frac { p(y=1 | x)}{p(y=0|x)}</script><p>为样本x正反例后验概率的比例，称为几率（odds），几率的对数称为对数几率（log odds或者logit），公式-3中第一个表达式，左边是线性函数，logistic回归可以看做是预测值为“标签的对数几率”的线性回归模型，因为Logistic回归也称为对数几率回归（Logit Regression）。</p><p>附公式-1到公式-3的推导：</p><script type="math/tex; mode=display">p(y=1 | x)= \frac{1}{1+exp(-w^T x)}</script><script type="math/tex; mode=display">=> exp(-w^Tx) = \frac{1-p(y=1 | x)}{p(y=1 | x)}</script><script type="math/tex; mode=display">=> - w^T x = log \frac{1- p(y=1 | x)}{p(y=1 | x)}</script><script type="math/tex; mode=display">=>  w^T x = log (\frac{1- p(y=1 | x)}{p(y=1 | x)})^{-1}</script><script type="math/tex; mode=display">=> w^T x = log \frac{p(y=1 | x)}{1-p(y=1 | x)}</script><script type="math/tex; mode=display">=> w^T x = log \frac{p(y=1 | x)}{p(y=0 | x)}</script><h2 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h2><p>LR回归采用交叉熵作为损失函数，并使用梯度下降法对参数进行优化。给定N个训练样本{x_i,y_i}，i&lt;=N，使用LR对每个样本进行预测，并用输出x_i的标签为1的后验概率，记为y’_i(x)  (公式-4)</p><script type="math/tex; mode=display">y'_i(x) = \sigma(w^Tx_i),i\in N</script><p>由于y_i属于{0，1}，样本{x_i,y_i}的真实概率可以表示为(公式-5)：</p><script type="math/tex; mode=display">p_r(y_i =1 | x_i) = y_i</script><script type="math/tex; mode=display">p_r(y_i =0 | x_i) = 1- y_i</script><p>使用交叉熵损失函数，其风险函数为(公式-6)：</p><script type="math/tex; mode=display">R(w)= - \frac{1}{N}\sum_{n=1}^{N} (p_r(y_i =1 | x_i) log(y_i') + p_r(y_i =0 | x_i) log(1-y_i') )</script><script type="math/tex; mode=display">= - \frac{1}{N}\sum_{n=1}^{N} ( y_i log(y_i') + (1-y_i') log(1-y_i') )</script><p>风险函数R(w)关于参数w的导数为(公式-7)：</p><script type="math/tex; mode=display">\frac{ \partial R(w)}{ \partial w} = - \frac{1}{N}\sum_{n=1}^{N}( y_i \frac{y_i'(1-y_i')}{y_i'}x_i -(1-y_i)\frac{y_i'(1-y_i')}{1-y_i'}x_i  )</script><script type="math/tex; mode=display">= - \frac{1}{N}\sum_{n=1}^{N}( y_i(1-y_i')x_i -(1-y_i)y_i'x_i)</script><script type="math/tex; mode=display">= - \frac{1}{N}\sum_{n=1}^{N}x_i(y_i-y_i')</script><p>采用梯度下降算法，Logistic的回归训练过程为：初始化w_0 为0，然后通过下式来更新迭代参数(公式-8)。</p><script type="math/tex; mode=display">w_{t+1} \leftarrow w_t + \alpha \frac{1}{N}\sum_{n=1}^{N} x_i(y_i-y_{w_t}')</script><p>其中a是学习率，y_{wt}’是当参数为w_t 时，Logistic回归的输出。</p><p>从公式-6可知，风险函数R(w)是关于参数w的连续可导的凸函数，因此除了梯度下降算法外，Logistic还可以使用高阶的优化算法，比如牛顿法来进行优化。</p><p>说明:</p><ul><li>两个未知数相乘求导：<script type="math/tex; mode=display">(ab)' = a'b + ab'</script></li><li>sigmoid函数求导后为：<script type="math/tex; mode=display">\sigma ' = \sigma (1-\sigma )x</script></li></ul><hr><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/44591359" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/44591359</a></li><li><a href="https://blog.csdn.net/wgdzz/article/details/48816307" target="_blank" rel="external">https://blog.csdn.net/wgdzz/article/details/48816307</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="LR" scheme="http://thinkgamer.cn/tags/LR/"/>
    
      <category term="线性模型" scheme="http://thinkgamer.cn/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="Logistic Regression" scheme="http://thinkgamer.cn/tags/Logistic-Regression/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法分类</title>
    <link href="http://thinkgamer.cn/2019/03/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB/"/>
    <id>http://thinkgamer.cn/2019/03/26/机器学习/机器学习算法分类/</id>
    <published>2019-03-26T09:43:37.000Z</published>
    <updated>2019-04-13T05:10:24.623Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><blockquote><p>机器学习算法可以按照不同的标准进行分类。比如按函数f(X)的不同，机器学习算法可以分为线性模型和非线性模型；按照学习准则的不同，机器学习算法也可以分为统计方法和非统计方法。</p><p>但一般而言，会按照训练样本提供的信息以及反馈方式不同，将机器学习算法分为以下几类，下面将一一细说。</p></blockquote><ul><li>监督学习</li><li>无监督学习</li><li>强化学习</li></ul><h1 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h1><p>如果机器学习的目标是通过建模样本的特征x和标签y之间的关系：</p><script type="math/tex; mode=display">y=f(x,\theta )</script><p>或</p><script type="math/tex; mode=display">p(y|x,\theta)</script><p>并且训练集中的每个样本都有标签，那么这类学习称之为监督学习（Supervised Learning）。根据标签类型的不同，监督学习又可以分为回归和分类两种。</p><h2 id="回归（Regression）"><a href="#回归（Regression）" class="headerlink" title="回归（Regression）"></a>回归（Regression）</h2><p>回归问题中的标签y是连续值（实数或者连续整数）</p><script type="math/tex; mode=display">y=f(x,\theta )</script><p>的输出也是连续值。</p><h2 id="分类（Classification）"><a href="#分类（Classification）" class="headerlink" title="分类（Classification）"></a>分类（Classification）</h2><p>分类问题中的标签y是离散的类别，在分类问题中，学习到的模型也成为分类器（Classifier）。分类问题根据其类别的数量又可以分为二分类（Binary Clssification）和多分类（Mutil-class Classification）。</p><h2 id="机构化学习（Structured-Learning）"><a href="#机构化学习（Structured-Learning）" class="headerlink" title="机构化学习（Structured Learning）"></a>机构化学习（Structured Learning）</h2><p>结构化学习的输出对象是结构化的对象，比如序列、树、图等，由于结构化学习的输出空间比较大，因此我们一般定义一个联合特征空间，将x,y映射为该空间中的联合特征向量（x,y）,预测模型可以写为：</p><script type="math/tex; mode=display">\hat{y}=\underset{y \in Gen(x)}{arg max f(\phi (x,y),\theta )}</script><p>其中gen(x)表示x所有可能的输出目标集合。计算arg max的过程也称为解码（decoding）过程，一般通过动态规划的方法来计算。</p><h1 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h1><p>无监督学习（Unsupervised Learning）是指从不包含目标标签的训练样本中自动学习到一些有价值的信息。典型的无监督学习问题有聚类、密度估计、特征学习、降维等。</p><h1 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h1><p>强化学习（Reinforcement Learning）是一类通过交互来学习的机器学习算法。在强化学习中，智能体根据环境的状态作出一个动作，并得到即时或延时的奖励。智能体在和环境的交互中不断学习并调整策略，以取得最大化的期望总回报。</p><p>下表给出了三种机器学习类型</p><p><img src="https://img-blog.csdnimg.cn/20190326174240856.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>监督学习需要每个样本都有标签，而无监督学习则不需要标签。一般而言，监督学习通常大量的有标签数据集，这些数据集是一般都需要由人工进行标注，成本很高。因此，也出现了很多弱监督学习（Weak Supervised Learning）和半监督学习（Semi-Supervised Learning）的方法，希望从大规模的无标注数据中充分挖掘有用的信息，降低对标注样本数量的要求。强化学习和监督学习的不同在于强化学习不需要显式地以“输入/输出对”的方式给出训练样本，是一种在线的学习机制。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="机器学习" scheme="http://thinkgamer.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法" scheme="http://thinkgamer.cn/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>从线性回归看偏差-方差分解（Bias-Variance Decomposition）</title>
    <link href="http://thinkgamer.cn/2019/03/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BB%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9C%8B%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE%E5%88%86%E8%A7%A3%EF%BC%88Bias-Variance%20Decomposition%EF%BC%89/"/>
    <id>http://thinkgamer.cn/2019/03/25/机器学习/从线性回归看偏差-方差分解（Bias-Variance Decomposition）/</id>
    <published>2019-03-25T15:18:55.000Z</published>
    <updated>2019-04-13T05:12:00.678Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>对于数字序列1，3，5，7，？，正常情况下大家脑海里蹦出的是9，但是217314也是其一个解<br>9对应的数学公式为</p><script type="math/tex; mode=display">f(x)=2x-1</script><p>217314对应的数学公式为</p><script type="math/tex; mode=display">f(x)=\frac{18111}{2} x^{4}-90555x^{3}+\frac{633885}{2}x^{2}-452773x+217331</script><p>Python 实现为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; def f(x):</span><br><span class="line">...     return 18111/2 * pow(x,4) -90555 * pow(x,3) + 633885/2 * pow(x,2) -452773 * x +217331</span><br><span class="line">... </span><br><span class="line">&gt;&gt;&gt; f(1)</span><br><span class="line">1.0</span><br><span class="line">&gt;&gt;&gt; f(2)</span><br><span class="line">3.0</span><br><span class="line">&gt;&gt;&gt; f(3)</span><br><span class="line">5.0</span><br><span class="line">&gt;&gt;&gt; f(4)</span><br><span class="line">7.0</span><br><span class="line">&gt;&gt;&gt; f(5)</span><br><span class="line">217341.0</span><br></pre></td></tr></table></figure></p><p>当机器学习模型进行预测的时候，通常都需要把握一个非常微妙的平衡，一方面我们希望模型能够匹配更多的训练数据，相应的增加其复杂度，否则会丢失相关特征的趋势（即模型过拟合）；但是另一方面，我们又不想让模型过分的匹配训练数据，相应的舍弃部分复杂的，因为这样存在过度解析所有异常值和伪规律的风险，导致模型的泛化能力差（即模型欠拟合）。因此在模型的拟合能力和复杂度之前取得一个比较好的权衡，对于一个模型来讲十分重要。而偏差-方差分解（Bias-Variance Decomposition）就是用来指导和分析这种情况的工具。</p><h1 id="偏差和方差定义"><a href="#偏差和方差定义" class="headerlink" title="偏差和方差定义"></a>偏差和方差定义</h1><ul><li>偏差（Bias）：即预测数据偏离真实数据的情况。</li><li>方差（Variance）：描述的是随机变量的离散程度，即随机变量在其期望值附近的波动程度。</li></ul><h1 id="偏差-方差推导过程"><a href="#偏差-方差推导过程" class="headerlink" title="偏差-方差推导过程"></a>偏差-方差推导过程</h1><p>以回归问题为例，假设样本的真实分布为p_r(x,y)，并采用平方损失函数，模型f(x)的期望错误为(公式2.1)：</p><script type="math/tex; mode=display">R(f) = E_{(x,y)\sim p_r{(x,y)}} \left [ (y-f(x))^2 \right ]</script><p>那么最优模型为(公式2.2)：</p><script type="math/tex; mode=display">f^*(x) = E_{y\sim p_r{(y|x)}} \left [ y \right ]</script><p>其中p_r(y|x)为真实的样本分布，f^*(x)为使用平方损失作为优化目标的最优模型，其损失为(公式2.3)：</p><script type="math/tex; mode=display">\varepsilon  = E_{(x,y)\sim p_r{(x,y)}} \left [ (y-f^*(x))^2 \right ]</script><p>损失</p><script type="math/tex; mode=display">\varepsilon</script><p>通常是由于样本分布及其噪声引起的，无法通过优化模型来减少。<br>期望错误可以分解为(公式2.4)：</p><script type="math/tex; mode=display">R(f)</script><script type="math/tex; mode=display">= E_{(x,y)\sim p_r{(x,y)}} \left [ (y- f^*(x) + f^*(x) -f(x))^2 \right ]</script><script type="math/tex; mode=display">= E_{x\sim p_r{(x)}}\left [ (f(x) - f^*(x))^2 \right ] + \varepsilon</script><p>公式2.4中的第一项是机器学习可以优化的真实目标，是当前模型和最优模型之间的差距。</p><p>在实际训练一个模型f(x)时，训练集D是从真实分布p_r(x,y)上独立同分布的采样出来的有限样本集合。不同的训练集会得到不同的模型。令f_D(x)表示在训练集D上学习到的模型，一个机器学习算法（包括模型和优化算法）的能力可以通过模型在不同训练集上的平均性能来体现。</p><p>对于单个样本x，不同训练集D得到的模型f_D(x)和最优模型f^*(x)的上的期望差距为（公式2.5）：</p><script type="math/tex; mode=display">E_D[( f_D(x)-f^*(x) )^2]</script><script type="math/tex; mode=display">=E_D\left [  ( f_D(x)  - E_D[f_D(x)] +E_D[f_D(x)]   -f^*(x) )^2   \right ]</script><script type="math/tex; mode=display">=( E_D[f_D(x)]   -f^*(x) )^2 )  + E_D[(f_D(x)  - E_D[f_D(x)] )^2]</script><p>公式2.5最后一行中的第一项为偏差(bias)，是指一个模型在不同训练集上的平均性能和与最优模型的差异，偏差可以用来衡量一个模型的拟合能力；第二项是方差（variance），是指一个模型在不同训练集上的差异，可以用来衡量一个模型是否容易过拟合。</p><p>集合公式2.4和公式2.5，期望错误可以分解为(公式2.6)：</p><script type="math/tex; mode=display">R(f)= (bias)^2 + variance +  \varepsilon</script><p>其中</p><script type="math/tex; mode=display">(bias)^2 =  E_X[E_D[f_D(x)]   -f^*(x) )^2 ]</script><script type="math/tex; mode=display">variance = E_X [ E_D[(f_D(x)  - E_D[f_D(x)] )^2] ]</script><p>最小化期望错误等价于最小化偏差和方差之和。</p><h1 id="偏差和方差分析"><a href="#偏差和方差分析" class="headerlink" title="偏差和方差分析"></a>偏差和方差分析</h1><p><img src="https://img-blog.csdnimg.cn/20190325231703314.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>上图为机器学习中偏差和方差的四种不同情况。每个图的中心点为最优模型f*(x)，蓝点为不同训练集D 上得到的模型f_D(x)。</p><ul><li>(a)给出了一种理想情况，方差和偏差都比较小</li><li>(b)为高偏差低方差的情况，表示模型的泛化能力很好，但拟合能力不足</li><li>(c)为低偏差高方差的情况，表示模型的拟合能力很好，但泛化能力比较差。当训练数据比较少时会导致过拟合</li><li>(d)为高偏差高方差的情况，是一种最差的情况</li></ul><p>方差一般会随着训练样本的增加而减少。当样本比较多时，方差比较少，我们可以选择能力强的模型来减少偏差。然而在很多机器学习任务上，训练集上往往都比较有限，最优的偏差和最优的方差就无法兼顾。</p><p>随着模型复杂度的增加，模型的拟合能力变强，偏差减少而方差增大，从而导致过拟合。以结构错误最小化为例，我们可以调整正则化系数λ来控制模型的复杂度。当λ变大时，模型复杂度会降低，可以有效地减少方差，避免过拟合，但偏差会上升。当λ过大时，总的期望错误反而会上升。因此，正则化系数λ需要在偏差和方差之间取得比较好的平衡。下图给出了机器学习模型的期望错误、偏差和方差随复杂度的变化情况。最优的模型并不一定是偏差曲线和方差曲线的交点。</p><p><img src="https://img-blog.csdnimg.cn/20190325231720819.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>偏差和方差分解给机器学习模型提供了一种分析途径，但在实际操作中难以直接衡量。一般来说，当一个模型在训练集上的错误率比较高时，说明模型的拟合能力不够，偏差比较高。这种情况可以增加数据特征、提高模型复杂度，减少正则化系数等操作来改进模型。当模型在训练集上的错误率比较低，但验证集上的错误率比较高时，说明模型过拟合，方差比较高。这种情况可以通过降低模型复杂度，加大正则化系数，引入先验等方法来缓解。此外，还有一种有效的降低方差的方法为集成模型，即通过多个高方差模型的平均来降低方差。 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="线性模型" scheme="http://thinkgamer.cn/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="线性回归" scheme="http://thinkgamer.cn/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="方差" scheme="http://thinkgamer.cn/tags/%E6%96%B9%E5%B7%AE/"/>
    
      <category term="偏差" scheme="http://thinkgamer.cn/tags/%E5%81%8F%E5%B7%AE/"/>
    
  </entry>
  
  <entry>
    <title>基于神经网络实现Mnist数据集的多分类</title>
    <link href="http://thinkgamer.cn/2019/03/09/TensorFlow/%E5%9F%BA%E4%BA%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0Mnist%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%A4%9A%E5%88%86%E7%B1%BB/"/>
    <id>http://thinkgamer.cn/2019/03/09/TensorFlow/基于神经网络实现Mnist数据集的多分类/</id>
    <published>2019-03-09T13:19:08.000Z</published>
    <updated>2019-04-13T05:56:37.338Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><blockquote><p>在之前的文章中介绍了基于Logistic Regression实现Mnist数据集的多分类，本篇文章主要介绍基于TensorFlow实现Mnist数据集的多分类。</p></blockquote><p>一个典型的神经网络训练图如下所示：<br><img src="https://img-blog.csdnimg.cn/20190308005540655.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><p>只不过在Mnist数据集是十分类的，起输出由y1和y2换成y1，….，y10。本文实现的神经网络如下所示：</p><p>这是使用的是两层的神经网络，第一层神经元个数是256，第二层为128，最终输出的是10个类别。对应的神经网络结果如下图所示：<br><img src="https://img-blog.csdnimg.cn/20190309205947261.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><p>接着我们创建一个MutilClass类，并初始化相关参数用来实现基于神经网络的多分类函数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"> </span><br><span class="line">class MutilClass:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        # 加载数据集</span><br><span class="line">        self.Mnsit = input_data.read_data_sets(&quot;./data/&quot;, one_hot=True)</span><br><span class="line"> </span><br><span class="line">        # 设置神经网络层参数</span><br><span class="line">        self.n_hidden_1 = 256</span><br><span class="line">        self.n_hidden_2 = 128</span><br><span class="line">        self.n_input = 784</span><br><span class="line">        self.n_classes = 10</span><br><span class="line"> </span><br><span class="line">        self.x = tf.placeholder(dtype=float, shape=[None, self.n_input], name=&quot;x&quot;)</span><br><span class="line">        self.y = tf.placeholder(dtype=float, shape=[None, self.n_classes], name=&quot;y&quot;)</span><br><span class="line">        # random_normal 高斯分布初始化权重</span><br><span class="line">        self.weights = &#123;</span><br><span class="line">            &quot;w1&quot;: tf.Variable(tf.random_normal([self.n_input, self.n_hidden_1],stddev = 0.1)),</span><br><span class="line">            &quot;w2&quot;: tf.Variable(tf.random_normal([self.n_hidden_1, self.n_hidden_2], stddev = 0.1)),</span><br><span class="line">            &quot;out&quot;: tf.Variable(tf.random_normal([self.n_hidden_2, self.n_classes], stddev = 0.1))</span><br><span class="line">        &#125;</span><br><span class="line">        self.bias = &#123;</span><br><span class="line">            &quot;b1&quot;: tf.Variable(tf.random_normal([ self.n_hidden_1 ])),</span><br><span class="line">            &quot;b2&quot;: tf.Variable(tf.random_normal([ self.n_hidden_2 ])),</span><br><span class="line">            &quot;out&quot;: tf.Variable(tf.random_normal([ self.n_classes ]))</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        print(&quot;参数初始化完成！&quot;)</span><br></pre></td></tr></table></figure><p>神经网络首次循环，是根据初始化的参数和偏置，向前传播，经过两层隐层，最终的到一个对应各个类别的概率，然后再根据反向传播，最小化损失函数求解参数，所以这里创建一个前向传播和反向传播的函数，如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 定义一个MLP，前向感知器</span><br><span class="line">def _multilayer_perceptron(self,_X, _weights, _bias):</span><br><span class="line">    layer_1 = tf.nn.sigmoid(tf.add ( tf.matmul(_X, _weights[&quot;w1&quot;]), _bias[&quot;b1&quot;] ) )</span><br><span class="line">    layer_2 = tf.nn.sigmoid(tf.add ( tf.matmul(layer_1, _weights[&quot;w2&quot;]), _bias[&quot;b2&quot;] ) )</span><br><span class="line">    return (tf.matmul( layer_2 ,_weights[&quot;out&quot;] ) + _bias[&quot;out&quot;])</span><br><span class="line"> </span><br><span class="line"># 定义反向传播</span><br><span class="line">def _back_propagation(self):</span><br><span class="line">    pred = self._multilayer_perceptron(self.x, self.weights, self.bias)</span><br><span class="line">    # logits 未归一化的概率</span><br><span class="line">    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=self.y) )</span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer( learning_rate= 0.001).minimize(cost)</span><br><span class="line">    corr = tf.equal(tf.argmax(pred, 1), tf.argmax(self.y, 1) )</span><br><span class="line">    accr =tf.reduce_mean(tf.cast(corr, dtype=float))</span><br><span class="line"> </span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    return init, optimizer,cost, accr</span><br></pre></td></tr></table></figure></p><p>接着就是训练网络了，指定的迭代次数为：100，batch_size：100，其对应的函数未：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># 训练模型</span><br><span class="line">def _train_model(self, _init, _optimizer, _cost, _accr):</span><br><span class="line">    epochs = 100</span><br><span class="line">    batch_size = 100</span><br><span class="line">    display_steps = 1</span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    sess.run(_init)</span><br><span class="line"> </span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        avg_cost = 0</span><br><span class="line">        total_batch = int (self.Mnsit.train.num_examples / batch_size)</span><br><span class="line">        for i in range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = self.Mnsit.train.next_batch(batch_size)</span><br><span class="line">            feeds = &#123;self.x: batch_xs, self.y: batch_ys&#125;</span><br><span class="line">            sess.run(_optimizer, feed_dict=feeds)</span><br><span class="line">            avg_cost += sess.run(_cost, feed_dict=feeds)</span><br><span class="line">        avg_cost = avg_cost / total_batch</span><br><span class="line"> </span><br><span class="line">        if (epoch +1) % display_steps ==0:</span><br><span class="line">            print(&quot;Epoch: &#123;&#125; / &#123;&#125;, cost: &#123;&#125;&quot;.format(epoch, epochs, avg_cost))</span><br><span class="line">            feeds = &#123;self.x: batch_xs, self.y: batch_ys&#125;</span><br><span class="line">            train_acc = sess.run(_accr, feed_dict=feeds)</span><br><span class="line">            print(&quot;Train Accuracy: &#123;&#125;&quot;.format(train_acc))</span><br><span class="line"> </span><br><span class="line">            feeds = &#123;self.x : self.Mnsit.test.images, self.y: self.Mnsit.test.labels&#125;</span><br><span class="line">            test_acc = sess.run(_accr, feed_dict= feeds)</span><br><span class="line">            print(&quot;Test Accuracy: &#123;&#125;&quot;.format(test_acc))</span><br><span class="line">            print(&quot;-&quot; * 50)</span><br></pre></td></tr></table></figure><p>创建主函数，进行迭代训练<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    network = MutilClass()</span><br><span class="line">    init, optimizer, cost, accr = network._back_propagation()</span><br><span class="line">    network._train_model(init, optimizer,cost, accr)</span><br></pre></td></tr></table></figure></p><p>最后的迭代结果为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 0 / 100, cost: 2.4407591546665537</span><br><span class="line">Train Accuracy: 0.12999999523162842</span><br><span class="line">Test Accuracy: 0.12960000336170197</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Epoch: 1 / 100, cost: 2.290777679356662</span><br><span class="line">Train Accuracy: 0.12999999523162842</span><br><span class="line">Test Accuracy: 0.1469999998807907</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Epoch: 2 / 100, cost: 2.2774649468335237</span><br><span class="line">Train Accuracy: 0.17000000178813934</span><br><span class="line">Test Accuracy: 0.21799999475479126</span><br><span class="line">--------------------------------------------------</span><br><span class="line"> </span><br><span class="line">.......</span><br><span class="line"> </span><br><span class="line">--------------------------------------------------</span><br><span class="line">Epoch: 98 / 100, cost: 0.7186844098567963</span><br><span class="line">Train Accuracy: 0.8299999833106995</span><br><span class="line">Test Accuracy: 0.8371999859809875</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Epoch: 99 / 100, cost: 0.7124480505423112</span><br><span class="line">Train Accuracy: 0.8100000023841858</span><br><span class="line">Test Accuracy: 0.8377000093460083</span><br><span class="line">--------------------------------------------------</span><br></pre></td></tr></table></figure></p><p>从结果中可以看出，cost是一直在减少，训练集和测试集评估模型的准确率也在一直提高。当然我们也可以通过调节epoch，batch_size来重新训练模型。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="深度学习" scheme="http://thinkgamer.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="TensorFlow" scheme="http://thinkgamer.cn/tags/TensorFlow/"/>
    
      <category term="LR" scheme="http://thinkgamer.cn/tags/LR/"/>
    
  </entry>
  
  <entry>
    <title>机器学习在微博信息流推荐中的应用实践</title>
    <link href="http://thinkgamer.cn/2019/03/05/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9C%A8%E5%BE%AE%E5%8D%9A%E4%BF%A1%E6%81%AF%E6%B5%81%E6%8E%A8%E8%8D%90%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5/"/>
    <id>http://thinkgamer.cn/2019/03/05/推荐系统/机器学习在微博信息流推荐中的应用实践/</id>
    <published>2019-03-05T00:03:35.000Z</published>
    <updated>2019-04-13T05:53:28.074Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><p>本文分为四部分介绍机器学习在微博信息流中的应用实践，分别为：微博信息流推荐场景介绍，内容理解与用户画像，大规模推荐系统实践和总结展望。</p><h1 id="微博信息流推荐场景介绍"><a href="#微博信息流推荐场景介绍" class="headerlink" title="微博信息流推荐场景介绍"></a>微博信息流推荐场景介绍</h1><blockquote><p>微博的feed流内容形态各异，有视频，图片，文字，长文，问答等，其用户量也很大，2018年Q2统计DAU（日活）为1.9亿，MAU（月活）为4.3亿，这么庞大的用户量，如何做好首页feed流的个性化推荐就显得格外重要。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/20190305162718873.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305162740161.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><h1 id="内容理解与用户画像"><a href="#内容理解与用户画像" class="headerlink" title="内容理解与用户画像"></a>内容理解与用户画像</h1><blockquote><p>由于个性化推荐是给用户推荐其感兴趣的内容，所以对于微博的内容理解和用户画像部分就显得格外重要。内容理解即通过文本内容理解和视觉理解技术，对微博内容进行细粒度表征，即形成每篇微博内容的表征向量。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/20190305162850198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305162905934.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><blockquote><p>用户画像即基于用户的发博内容，行为数据，自填信息等进行深度挖掘，精准分析刻画用户，从而在进行微博内容推送时能够实现其个性化。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/20190305162934416.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><h1 id="大规模推荐系统实践"><a href="#大规模推荐系统实践" class="headerlink" title="大规模推荐系统实践"></a>大规模推荐系统实践</h1><blockquote><p>目前推荐架构的实现思路都是先从海量原始数据中，依据用户画像，召回用户偏好的数据，在利用排序算法对其进行排序，最终选择top K返回给用户。微博推荐亦是如此。其整体的流程图如下所示：</p><p>物料召回：即从候选物料集合中粗筛物料，作为进行模型的待排序物料。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/20190305163036955.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><p><img src="https://img-blog.csdnimg.cn/20190305163059488.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><p><img src="https://img-blog.csdnimg.cn/20190305163120489.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><blockquote><p>算法排序则是结合相关特征对物料召回的内容进行预估排序，其特征主要分为：用户特征，内容特征，环境特征，组合特征和上下文特征等。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/20190305163427457.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163455977.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163515432.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163530356.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163645420.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163705966.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163719819.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163749462.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163805882.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163818878.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163833250.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><h1 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a>总结与展望</h1><ul><li><p>总结</p><ul><li>业务和数据决定了模型算法的应用场景</li><li>模型算法殊途同归</li><li>工程能力和算法架构是基本保障</li></ul></li><li><p>展望</p><ul><li>采用多模型融合，能更好的对非结构化内容进行表征</li><li>更多的融合网络结构适用于CTR预估场景</li></ul></li></ul><hr><p>完整内容请阅读原文：<a href="https://blog.csdn.net/Gamer_gyt/article/details/88164127" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/88164127</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="深度学习" scheme="http://thinkgamer.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="推荐系统" scheme="http://thinkgamer.cn/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="机器学习" scheme="http://thinkgamer.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow实现Mnist数据集的多分类逻辑回归模型</title>
    <link href="http://thinkgamer.cn/2019/02/27/TensorFlow/TensorFlow%E5%AE%9E%E7%8E%B0Mnist%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%A4%9A%E5%88%86%E7%B1%BB%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
    <id>http://thinkgamer.cn/2019/02/27/TensorFlow/TensorFlow实现Mnist数据集的多分类逻辑回归模型/</id>
    <published>2019-02-27T05:15:55.000Z</published>
    <updated>2019-04-13T05:46:04.119Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><blockquote><p>多分类逻辑回归基于逻辑回归（Logistic Regression，LR）和softMax实现，其在多分类分类任务中应用广泛，本篇文章基于tf实现多分类逻辑回归，使用的数据集为Mnist。</p></blockquote><p>多分类逻辑回归的基础概要和在Spark上的实现可参考：</p><ul><li>多分类逻辑回归（Multinomial Logistic Regression）</li><li>多分类实现方式介绍和在Spark上实现多分类逻辑回归（Multinomial Logistic Regression）</li></ul><p>本篇文章涉及到的tf相关接口函数及释义如下：</p><h2 id="tf-nn-softmax"><a href="#tf-nn-softmax" class="headerlink" title="tf.nn.softmax"></a>tf.nn.softmax</h2><p>Softmax 在机器学习和深度学习中有着非常广泛的应用。尤其在处理多分类（C &gt; 2）问题，分类器最后的输出单元需要Softmax 函数进行数值处理。关于Softmax 函数的定义如下所示：</p><p>…</p><hr><p>完整内容请阅读原文：<a href="https://blog.csdn.net/Gamer_gyt/article/details/87970776" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/87970776</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="深度学习" scheme="http://thinkgamer.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="TensorFlow" scheme="http://thinkgamer.cn/tags/TensorFlow/"/>
    
      <category term="LR" scheme="http://thinkgamer.cn/tags/LR/"/>
    
  </entry>
  
  <entry>
    <title>深度学习中的epochs、batch_size、iterations详解</title>
    <link href="http://thinkgamer.cn/2019/02/26/TensorFlow/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84epochs%E3%80%81batch_size%E3%80%81iterations%E8%AF%A6%E8%A7%A3/"/>
    <id>http://thinkgamer.cn/2019/02/26/TensorFlow/深度学习中的epochs、batch_size、iterations详解/</id>
    <published>2019-02-25T16:32:29.000Z</published>
    <updated>2019-04-13T05:38:21.475Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><blockquote><p>深度学习中涉及到很多参数，如果对于一些参数不了解，那么去看任何一个框架都会有难度，在TensorFlow中有一些模型训练的基本参数，这些参数是训练模型的前提，也在一定程度上影响着模型的最终效果。下面主要介绍几个参数。</p></blockquote><ul><li>batch_size</li><li>iterations</li><li>epochs</li></ul><h1 id="batch-size"><a href="#batch-size" class="headerlink" title="batch_size"></a>batch_size</h1><p>深度学习的优化算法，其实就是梯度下降，在之前的文章中我们也介绍过梯度下降，这里就不详细说明。梯度下降分为三种：</p><ul><li>批量梯度下降算法（BGD，Batch gradient descent algorithm）</li><li>随机梯度下降算法（SGD，Stochastic gradient descent algorithm）</li><li>小批量梯度下降算法（MBGD，Mini-batch gradient descent algorithm）</li></ul><p>批量梯度下降算法，每一次计算都需要遍历全部数据集，更新梯度，计算开销大，花费时间长，不支持在线学习。</p><p>随机梯度下降算法，每次随机选取一条数据，求梯度更新参数，这种方法计算速度快，但是收敛性能不太好，可能在最优点附近晃来晃去，hit不到最优点。两次参数的更新也有可能互相抵消掉，造成目标函数震荡的比较剧烈。</p><p>为了克服两种方法的缺点，现在一般采用的是一种折中手段，mini-batch gradient decent，小批的梯度下降，这种方法把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也不是很大。</p><p>tf框架中的batch_size指的就是更新梯度中使用的样本数。当然这里如果把batch_size设置为数据集的长度，就成了批量梯度下降算法，batch_size设置为1就是随机梯度下降算法。</p><h1 id="iterations"><a href="#iterations" class="headerlink" title="iterations"></a>iterations</h1><p>迭代次数，每次迭代更新一次网络结构的参数。</p><p>迭代是重复反馈的动作，神经网络中我们希望通过迭代进行多次的训练以到达所需的目标或结果。</p><p>每一次迭代得到的结果都会被作为下一次迭代的初始值。</p><p>一个迭代 = 一个（batch_size）数据正向通过（forward）+ 一个（batch_size）数据反向（backward）</p><p><img src="https://img-blog.csdnimg.cn/2019022523170956.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="神经网络"></p><p>前向传播：构建由（x1,x2,x3）得到Y（hwb(x)）的表达式</p><p>反向传播：基于给定的损失函数，求解参数的过程</p><h1 id="epochs"><a href="#epochs" class="headerlink" title="epochs"></a>epochs</h1><p>epochs被定义为前向和反向传播中所有批次的单次训练迭代。这意味着1个周期是整个输入数据的单次前向和反向传递。</p><p>简单说，epochs指的就是训练过程中数据将被“轮”多少次</p><p>例如在某次模型训练过程中，总的样本数是10000，batch_size=100，epochs=10，其对应的伪代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data = </span><br><span class="line">batch_size = 100</span><br><span class="line">for i in range(epochs):</span><br><span class="line">    for j in range(int(data_length / batch_size - 1)):</span><br><span class="line">        x_data = data[begin:end, ]</span><br><span class="line">        y_data = data[begin:end, ]</span><br><span class="line">        mode.train(x_data, y_data)</span><br><span class="line">        begin += batch_size</span><br><span class="line">        end += batch_size</span><br></pre></td></tr></table></figure></p><p>其中iterations = data_length / batch_size</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="深度学习" scheme="http://thinkgamer.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="tensorFlow" scheme="http://thinkgamer.cn/tags/tensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Spark排序算法系列之（MLLib、ML）GBTs使用方式介绍</title>
    <link href="http://thinkgamer.cn/2019/01/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Spark%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%E4%B9%8B%EF%BC%88MLLib%E3%80%81ML%EF%BC%89GBTs%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F%E4%BB%8B%E7%BB%8D/"/>
    <id>http://thinkgamer.cn/2019/01/29/机器学习/Spark排序算法系列之（MLLib、ML）GBTs使用方式介绍/</id>
    <published>2019-01-29T13:16:35.000Z</published>
    <updated>2019-04-13T05:43:53.236Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>【Spark排序算法系列】主要介绍的是目前推荐系统或者广告点击方面用的比较广的几种算法，和他们在Spark中的应用实现，本篇文章主要介绍GBDT算法，本系列还包括（持续更新）：</p><ul><li>Spark排序算法系列之LR（逻辑回归）</li><li>Spark排序算法系列之模型融合（GBDT+LR）</li><li>Spark排序算法系列之XGBoost</li><li>Spark排序算法系列之FTRL（Follow-the-regularized-Leader）</li><li>Spark排序算法系列之FM与FFM</li></ul><p>在本篇文章中你可以学到：</p><ul><li>Spark MLLib包中的GBDT使用方式</li><li>模型的通过保存、加载、预测</li><li>PipeLine</li><li>ML包中的GBDT</li></ul><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>LR因为其容易并行最早应用到推荐排序中的，但学习能力有限，需要大量的特征工程来增加模型的学习能力。但大量的特征工程耗时耗力，且不一定带来效果的提升，因此在如何能有效的发现特征组合，来缩短LR特征实验周期的背景下，GBDT被应用了起来。GBDT模型全称是Gradient Boosting Decision Tree，梯度提升决策树。是属于Boosing算法中的一种，关于Boosting的介绍可以参考文章集成学习（Ensemble Learning)</p><p>关于GBDT算法理解可参考：</p><ul><li>Spark排序算法系列之GBTs基础——梯度上升和梯度下降</li><li>梯度提升决策树GBDT（Gradient Boosting Decision Tree）</li></ul><p>其实相信很多人对Spark 机器学习包（ml和mllib）中的GBDT傻傻分不清楚，这里我们先来捋一捋。Spark中的GBDT较GBTs——梯度提升树，因为其是基于决策树（Decision Tree，DT）实现的，所以叫GBDT。Spark 中的GBDT算法存在于ml包和mllib包中，mllib是基于RDD的，ml包则是针对DataFrame的，ml包中的GBDT分为分类和回归，在实际使用过程中，需要根据具体情况进行衡量和选择。由于在实际生产环境中使用基于RDD的较多，所以下面将着重介绍下MLLib包中的GBTs，ML包中的将进行简单说明。</p><hr><p>完整内容请阅读原文：<a href="https://blog.csdn.net/Gamer_gyt/article/details/86695837" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/86695837</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="Spark" scheme="http://thinkgamer.cn/tags/Spark/"/>
    
      <category term="机器学习" scheme="http://thinkgamer.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>资源分享：从数理统计到DL、RL，还不快来！</title>
    <link href="http://thinkgamer.cn/2019/01/28/NLP/%E8%B5%84%E6%BA%90%E5%88%86%E4%BA%AB%EF%BC%9A%E4%BB%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E5%88%B0DL%E3%80%81RL%EF%BC%8C%E8%BF%98%E4%B8%8D%E5%BF%AB%E6%9D%A5/"/>
    <id>http://thinkgamer.cn/2019/01/28/NLP/资源分享：从数理统计到DL、RL，还不快来/</id>
    <published>2019-01-27T19:16:19.000Z</published>
    <updated>2019-04-13T05:41:18.266Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><p>之前在自己的年度总结里写到：19年的目标就是技术沉淀与突破。技术突破不仅包含现有技术的总结和反思，更是对未知技术的探索和求知，希望19年能够更上一层楼。</p><p>这个repo是我一直维护和整理的一个技术资料分享的repo，是我包括群友一块整理的一个免费技术资料分享的库，不仅包含了机器学习，数据挖掘，深度学习，还包含了大数据，数理统计，强化学习等，希望在技术这条路上你能跑的更快。</p><p>repo：<a href="https://github.com/Thinkgamer/books" target="_blank" rel="external">https://github.com/Thinkgamer/books</a></p><p>先来张图片镇楼！！！<br><img src="https://img-blog.csdnimg.cn/20190128031446639.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==,size_16,color_FFFFFF,t_70"></p><h1 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h1><p>建立该Repo的目的有两个：</p><ul><li>本人在各个平台共享书籍，进行一个统一管理</li><li>分享给各个搞技术的朋友，知识无私藏之说</li></ul><hr><h1 id="What"><a href="#What" class="headerlink" title="What"></a>What</h1><p>该Repo会涉及包含以下类别书籍：</p><ul><li>机器学习</li><li>数据挖掘</li><li>深度学习</li><li>NLP</li><li>云计算</li><li>统计学概率论</li><li>收藏的论文</li><li>杂乱无章</li></ul><hr><h1 id="List"><a href="#List" class="headerlink" title="List"></a>List</h1><p>注明：排名无先后顺序</p><h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><ul><li>scikit-learn 中英文</li><li>机器学习-周志华</li><li>机器学习实战</li><li>机器学习导论</li><li>集体智慧编程中文版</li><li>[英文版]叶斯思维：统计建模的Python学习法</li></ul><h2 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h2><ul><li>python数据分析与挖掘实战</li><li>利用python进行数据分析</li><li>面向程序员的数据挖掘指南</li><li>数据挖掘：概念与技术（中文第三版）</li><li>数据挖掘应用20个案例分析</li><li>数据挖掘与数据化运营实战_思路_方法_技巧与应用_完整版</li></ul><h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><ul><li>神经网络与机器学习（加）Simon Haykin</li><li>TensorFlow实战-黄文坚</li><li>深度学习 中文版</li><li>神经网络与深度学习</li></ul><h2 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h2><ul><li>模式识别与机器学习 中文版</li><li>NLP汉语自然语言处理原理与实践</li><li>PYTHON自然语言处理中文版</li></ul><h2 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h2><ul><li>推荐系统实践</li><li>learning-to-rank.pdf </li><li>Recommender Systems Handbook.pdf</li><li>Context-Aware-Recommender-Systems-chapter-7.pdf</li></ul><h2 id="云计算"><a href="#云计算" class="headerlink" title="云计算"></a>云计算</h2><ul><li>《快学Scala》</li><li>Learning PySpark.pdf</li><li>SparkMLlib机器学习</li><li>Spark快速大数据分析</li><li>数据算法  Hadoop Spark大数据处理技巧</li></ul><h2 id="统计学与概率论"><a href="#统计学与概率论" class="headerlink" title="统计学与概率论"></a>统计学与概率论</h2><ul><li>《概率论与数理统计》浙大版（第四版）教材</li><li>《概率论与数理统计习题全解指南》.浙大版（第四版）</li><li>数理统计与数据分析原书第3版</li><li>应用商务统计分析 王汉生(2008).pdf</li></ul><h2 id="收藏的论文"><a href="#收藏的论文" class="headerlink" title="收藏的论文"></a>收藏的论文</h2><ul><li>平滑系数自适应的二次指数平滑模型及其应用</li><li>The Structure of Collaborative Tagging Systems</li><li>FM</li><li>FFM</li><li>DeepFFM</li><li>Focal Loss for Dense Object Detection</li><li>Attentive Group Recommendation.pdf</li><li>Real-time Personalization using Embeddings for Search.pdf</li></ul><h2 id="杂乱无章"><a href="#杂乱无章" class="headerlink" title="杂乱无章"></a>杂乱无章</h2><ul><li>阿里技术之瞳-p260</li><li>数据敏感性测试</li><li>正则表达式经典实例.（美）高瓦特斯，（美）利维森</li><li>阿里广告中的机器学习平台.pdf</li><li>广告数据上的大规模机器学习.pdf</li><li>绿盟大数据安全分析平台 产品白皮书.pdf</li><li>Xdef2013-基于机器学习和NetFPGA的智能高速入侵防御系统.ppt</li><li>04-程佳-推荐广告机器学习实践</li><li>A Gentle Introduction to Gradient Boosting.pdf</li><li>GBDT算法原理与系统设计简介.pdf</li><li>[微博] 机器学习在微博信息流推荐应用实践.pdf</li><li>[知乎] 首页信息流系统的框架及机器学习技术在推荐策略中的应用.pdf</li></ul><h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><ul><li>强化学习在阿里的技术演进与业务创新.pdf</li></ul><h2 id="技术集锦"><a href="#技术集锦" class="headerlink" title="技术集锦"></a>技术集锦</h2><ul><li>AAAI2018.pdf</li><li>数字经济下的算法力量.pdf</li><li>2018美团点评算法系列.pdf</li><li>Learning To Rank在个性化电商搜索中的应用</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="资源" scheme="http://thinkgamer.cn/tags/%E8%B5%84%E6%BA%90/"/>
    
      <category term="DL" scheme="http://thinkgamer.cn/tags/DL/"/>
    
      <category term="RL" scheme="http://thinkgamer.cn/tags/RL/"/>
    
      <category term="ML" scheme="http://thinkgamer.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>多分类实现方式介绍和在Spark上实现多分类逻辑回归</title>
    <link href="http://thinkgamer.cn/2019/01/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%88%86%E7%B1%BB%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%9C%A8Spark%E4%B8%8A%E5%AE%9E%E7%8E%B0%E5%A4%9A%E5%88%86%E7%B1%BB%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://thinkgamer.cn/2019/01/12/机器学习/多分类实现方式介绍和在Spark上实现多分类逻辑回归/</id>
    <published>2019-01-12T14:06:02.000Z</published>
    <updated>2019-04-13T05:38:13.219Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>在之前的文章中介绍了多分类逻辑回归算法的数据原理，参考文章链接</p><p>CSDN文章链接：<a href="https://blog.csdn.net/Gamer_gyt/article/details/85209496" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/85209496</a><br>公众号：多分类逻辑回归（Multinomial Logistic Regression）<br>该篇文章介绍一下Spark中多分类算法，主要包括的技术点如下</p><ul><li><p>多分类实现方式</p><ul><li>一对一 （One V One）</li><li>一对其余（One V Remaining）</li><li>多对多 （More V More）</li></ul></li><li><p>Spark中的多分类实现</p><h1 id="多分类实现方式"><a href="#多分类实现方式" class="headerlink" title="多分类实现方式"></a>多分类实现方式</h1><h2 id="一对一"><a href="#一对一" class="headerlink" title="一对一"></a>一对一</h2><p>假设某个分类中有N个类别，将这N个类别两两配对（继而转化为二分类问题），这样可以得到 N（N-1）/ 2个二分类器，这样训练模型时需要训练 N（N-1）/ 2个模型，预测时将样本输送到这些模型中，最终统计出现次数较多的类别结果作为最终类别。</p></li></ul><p>假设现在有三个类别：类别A，类别B，类别C，类别D。一对一实现多分类如下图所示：<br><img src="https://img-blog.csdnimg.cn/20190112220044121.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==,size_16,color_FFFFFF,t_70"></p><hr><p>完整内容请阅读原文：<a href="https://blog.csdn.net/Gamer_gyt/article/details/86378882" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/86378882</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="Spark" scheme="http://thinkgamer.cn/tags/Spark/"/>
    
      <category term="LR" scheme="http://thinkgamer.cn/tags/LR/"/>
    
      <category term="机器学习" scheme="http://thinkgamer.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Hive Join 分析和优化</title>
    <link href="http://thinkgamer.cn/2019/01/03/Spark/Hive%20Join%20%E5%88%86%E6%9E%90%E5%92%8C%E4%BC%98%E5%8C%96/"/>
    <id>http://thinkgamer.cn/2019/01/03/Spark/Hive Join 分析和优化/</id>
    <published>2019-01-03T05:34:48.000Z</published>
    <updated>2019-04-13T05:33:15.758Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>Sku对应品牌进行关联，大表对应非大表（这里的非大表并不能用小表来定义）</p><h1 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h1><p>进行表左关联时，最后一个reduce任务卡到99%，运行时间很长，发生了严重的数据倾斜。</p><p>什么是数据倾斜？数据倾斜主要表现在，map /reduce程序执行时，reduce节点大部分执行完毕，但是有一个或者几个reduce节点运行很慢，导致整个程序的处理时间很长，这是因为某一个key的条数比其他key多很多（有时是百倍或者千倍之多），这条key所在的reduce节点所处理的数据量比其他节点就大很多，从而导致某几个节点迟迟运行不完。</p><p>完整内容请阅读原文：<a href="https://blog.csdn.net/Gamer_gyt/article/details/85690885" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/85690885</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="Spark" scheme="http://thinkgamer.cn/tags/Spark/"/>
    
      <category term="Hive" scheme="http://thinkgamer.cn/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 特征工程:feature_column</title>
    <link href="http://thinkgamer.cn/2019/01/03/TensorFlow/TensorFlow%20%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B:%20feature_column/"/>
    <id>http://thinkgamer.cn/2019/01/03/TensorFlow/TensorFlow 特征工程: feature_column/</id>
    <published>2019-01-03T05:18:16.000Z</published>
    <updated>2019-04-13T05:28:32.930Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="特征工程-feature-column"><a href="#特征工程-feature-column" class="headerlink" title="特征工程: feature_column"></a>特征工程: feature_column</h1><p>在使用很多模型的时候，都需要对输入的数据进行必要的特征工程处理。最典型的就是:one-hot处理，还有hash分桶等处理。为了方便处理这些特征，tensorflow提供了一些列的特征工程方法来方便使用.</p><h2 id="公共的import"><a href="#公共的import" class="headerlink" title="公共的import"></a>公共的import</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.python.estimator.inputs import numpy_io</span><br><span class="line">import numpy as np</span><br><span class="line">import collections</span><br><span class="line">from tensorflow.python.framework import errors</span><br><span class="line">from tensorflow.python.platform import test</span><br><span class="line">from tensorflow.python.training import coordinator</span><br><span class="line">from tensorflow import feature_column</span><br><span class="line"></span><br><span class="line">from tensorflow.python.feature_column.feature_column import _LazyBuilder</span><br></pre></td></tr></table></figure><h2 id="numeric-column"><a href="#numeric-column" class="headerlink" title="numeric_column"></a>numeric_column</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">numeric_column(</span><br><span class="line">    key,</span><br><span class="line">    shape=(1,),</span><br><span class="line">    default_value=None,</span><br><span class="line">    dtype=tf.float32,</span><br><span class="line">    normalizer_fn=None</span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>key：特征的名字。也就是对应的列名称</li><li>shape：该key所对应的特征的shape. 默认是1，但是比如one-hot类型的，shape就不是1，而是实际的维度。总之，这里是key所对应的维度，不一定是1</li><li>default_value：如果不存在使用的默认值</li><li>normalizer_fn：对该特征下的所有数据进行转换。如果需要进行normalize，那么就是使用normalize的函数.这里不仅仅局限于normalize，也可以是任何的转换方法，比如取对数，取指数，这仅仅是一种变换方法</li></ul><p>完整内容请阅读：<a href="https://blog.csdn.net/Gamer_gyt/article/details/85689840" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/85689840</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="深度学习" scheme="http://thinkgamer.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="tensorFlow" scheme="http://thinkgamer.cn/tags/tensorFlow/"/>
    
      <category term="特征过程" scheme="http://thinkgamer.cn/tags/%E7%89%B9%E5%BE%81%E8%BF%87%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>集成学习（Ensemble Learning)</title>
    <link href="http://thinkgamer.cn/2019/01/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%EF%BC%88Ensemble%20Learning)/"/>
    <id>http://thinkgamer.cn/2019/01/03/机器学习/集成学习（Ensemble Learning)/</id>
    <published>2019-01-03T05:14:38.000Z</published>
    <updated>2019-04-13T05:24:56.867Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><blockquote><p>在机器学习的有监督学习算法中，我们的目标是学习出一个稳定的且在各个方面表现都较好的模型，但实际情况往往不这么理想，有时我们只能得到多个有偏好的模型（弱监督模型，在某些方面表现的比较好）。集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。</p></blockquote><p>集成学习在各个规模的数据集上都有很好的策略。</p><ul><li>数据集大：划分成多个小数据集，学习多个模型进行组合</li><li>数据集小：利用Bootstrap方法进行抽样，得到多个数据集，分别训练多个模型再进行组合</li></ul><p>这篇博客介绍一下集成学习的几类：Bagging，Boosting以及Stacking。</p><h1 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h1><p>Bagging是bootstrap aggregating的简写。先说一下bootstrap，bootstrap也称为自助法，它是一种有放回的抽样方法，目的为了得到统计量的分布以及置信区间。具体步骤如下</p><ul><li>采用重抽样方法（有放回抽样）从原始样本中抽取一定数量的样本</li><li>根据抽出的样本计算想要得到的统计量T</li><li>重复上述N次（一般大于1000），得到N个统计量T</li><li>根据这N个统计量，即可计算出统计量的置信区间</li></ul><p>在Bagging方法中，利用bootstrap方法从整体数据集中采取有放回抽样得到N个数据集，在每个数据集上学习出一个模型，最后的预测结果利用N个模型的输出得到，具体地：分类问题采用N个模型预测投票的方式，回归问题采用N个模型预测平均的方式。</p><p>例如随机森林（Random Forest）就属于Bagging。随机森林简单地来说就是用随机的方式建立一个森林，森林由很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。</p><p>在我们学习每一棵决策树的时候就需要用到Bootstrap方法。在随机森林中，有两个随机采样的过程：对输入数据的行（数据的数量）与列（数据的特征）都进行采样。对于行采样，采用有放回的方式，若有N个数据，则采样出N个数据（可能有重复），这样在训练的时候每一棵树都不是全部的样本，相对而言不容易出现overfitting；接着进行列采样从M个feature中选择出m个（m&lt;&lt;M）。最近进行决策树的学习。</p><p>预测的时候，随机森林中的每一棵树的都对输入进行预测，最后进行投票，哪个类别多，输入样本就属于哪个类别。这就相当于前面说的，每一个分类器（每一棵树）都比较弱，但组合到一起（投票）就比较强了。</p><h1 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h1><p>提升方法（Boosting）是一种可以用来减小监督学习中偏差的机器学习算法。主要也是学习一系列弱分类器，并将其组合为一个强分类器。Boosting中有代表性的是AdaBoost（Adaptive boosting）算法：刚开始训练时对每一个训练例赋相等的权重，然后用该算法对训练集训练t轮，每次训练后，对训练失败的训练例赋以较大的权重，也就是让学习算法在每次学习以后更注意学错的样本，从而得到多个预测函数。具体可以参考《统计学习方法》。</p><h1 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h1><p>Stacking方法是指训练一个模型用于组合其他各个模型。首先我们先训练多个不同的模型，然后把之前训练的各个模型的输出为输入来训练一个模型，以得到一个最终的输出。理论上，Stacking可以表示上面提到的两种Ensemble方法，只要我们采用合适的模型组合策略即可。但在实际中，我们通常使用logistic回归作为组合策略。</p><p>如下图，先在整个训练数据集上通过bootstrap抽样得到各个训练集合，得到一系列分类模型，称之为Tier 1分类器（可以采用交叉验证的方式学习），然后将输出用于训练Tier 2 分类器。</p><p>完整内容请阅读： <a href="https://blog.csdn.net/Gamer_gyt/article/details/85689424" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/85689424</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="机器学习" scheme="http://thinkgamer.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="集成学习" scheme="http://thinkgamer.cn/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>基于TF-IDF算法的短标题关键词提取</title>
    <link href="http://thinkgamer.cn/2019/01/03/NLP/%E5%9F%BA%E4%BA%8ETF-IDF%E7%AE%97%E6%B3%95%E7%9A%84%E7%9F%AD%E6%A0%87%E9%A2%98%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96/"/>
    <id>http://thinkgamer.cn/2019/01/03/NLP/基于TF-IDF算法的短标题关键词提取/</id>
    <published>2019-01-02T19:27:48.000Z</published>
    <updated>2019-04-13T05:31:29.638Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="TF-IDF算法介绍"><a href="#TF-IDF算法介绍" class="headerlink" title="TF-IDF算法介绍"></a>TF-IDF算法介绍</h1><p>TF-IDF（Term Frequency–InverseDocument Frequency）是一种用于资讯检索与文本挖掘的常用加权技术。TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。</p><p>完整内容请阅读：<a href="https://blog.csdn.net/Gamer_gyt/article/details/85690389" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/85690389</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="NLP" scheme="http://thinkgamer.cn/tags/NLP/"/>
    
      <category term="TF_IDF" scheme="http://thinkgamer.cn/tags/TF-IDF/"/>
    
      <category term="短标题" scheme="http://thinkgamer.cn/tags/%E7%9F%AD%E6%A0%87%E9%A2%98/"/>
    
      <category term="关键词提取" scheme="http://thinkgamer.cn/tags/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96/"/>
    
  </entry>
  
  <entry>
    <title>【内附PDF资料】Python实现下载图片并生产PDF文件</title>
    <link href="http://thinkgamer.cn/2019/01/02/Python/%E3%80%90%E5%86%85%E9%99%84PDF%E8%B5%84%E6%96%99%E3%80%91Python%E5%AE%9E%E7%8E%B0%E4%B8%8B%E8%BD%BD%E5%9B%BE%E7%89%87%E5%B9%B6%E7%94%9F%E4%BA%A7PDF%E6%96%87%E4%BB%B6/"/>
    <id>http://thinkgamer.cn/2019/01/02/Python/【内附PDF资料】Python实现下载图片并生产PDF文件/</id>
    <published>2019-01-02T12:28:22.000Z</published>
    <updated>2019-04-13T05:22:04.809Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><blockquote><p>2018AICon大会中的一些PPT，官方没有提供完整的PDF文件，而是一张张图片，不方便下载和后续阅读，这里使用Python爬取相关演讲的图片，并生产PDF文件</p></blockquote><h1 id="下载函数"><a href="#下载函数" class="headerlink" title="下载函数"></a>下载函数</h1><p>创建下载图片函数，使用的是Python的urllib库，代码如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 下载图片</span><br><span class="line">def save_img(img_url,file_path=&quot;./img/&quot;):</span><br><span class="line">    if not os.path.exists(file_path):</span><br><span class="line">        print(&quot;保存图片的文件不存在，创建该目录&quot;)</span><br><span class="line">        os.mkdir(file_path)</span><br><span class="line">    # 图片后缀</span><br><span class="line">    file_suffix = os.path.splitext(img_url)[1]</span><br><span class="line">    file_name = str ( int(img_url.split(&quot;-&quot;)[1].split(&quot;.&quot;)[0]) )</span><br><span class="line">    # 拼接图片名（包含路径）</span><br><span class="line">    filename = &apos;&#123;&#125;&#123;&#125;&#123;&#125;&apos;.format(file_path,file_name,file_suffix)</span><br><span class="line">    # urllib</span><br><span class="line">    urllib.request.urlretrieve(img_url, filename=filename)</span><br></pre></td></tr></table></figure><h1 id="生成PDF函数"><a href="#生成PDF函数" class="headerlink" title="生成PDF函数"></a>生成PDF函数</h1><p>创建图片生成PDF文件的函数，使用的是Python的reportlab库，代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 生成pdf</span><br><span class="line">def convert_img_to_pdf(img_path,pdf_path):</span><br><span class="line">    pages = 0</span><br><span class="line">    (w, h) = landscape(portrait(A4))</span><br><span class="line">    can = canvas.Canvas(pdf_path, pagesize=landscape(portrait(A4)))</span><br><span class="line">    # 获取img_path下文件，并进行排序</span><br><span class="line">    files = os.listdir(img_path)</span><br><span class="line">    files.sort(key=lambda x: int(x[:-4]))</span><br><span class="line">    # 遍历每个文件</span><br><span class="line">    for f_name in files:</span><br><span class="line">        # 拼装成完整的file路径</span><br><span class="line">        file_path = img_path + os.sep + str(f_name)</span><br><span class="line">        can.drawImage(file_path, 0, 0, w, h)</span><br><span class="line">        can.showPage()</span><br><span class="line">        pages = pages + 1</span><br><span class="line">    can.save()</span><br></pre></td></tr></table></figure></p><p>需要注意的是，pagesize不能直接指定值portrait(A4)，因为一张图片会完整的嵌套在一页A4纸张上，极其不美观，这里需要将A4的大小进行转置，pagesize=landscape(portrait(A4))</p><h1 id="定义要生成的PDF相关信息"><a href="#定义要生成的PDF相关信息" class="headerlink" title="定义要生成的PDF相关信息"></a>定义要生成的PDF相关信息</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">_list = (</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;pdf_name&quot;: &quot;FFM及DeepFFM模型在推荐系统的探索及实践.pdf&quot;,</span><br><span class="line">        &quot;img_path&quot;: &quot;./img_1/&quot;,</span><br><span class="line">        &quot;page&quot;: 52,</span><br><span class="line">        &quot;id&quot;: &quot;3670025915&quot;</span><br><span class="line">    &#125;,&#123;</span><br><span class="line">        &quot;pdf_name&quot;: &quot;知乎推荐系统的实践及重构之路.pdf&quot;,</span><br><span class="line">        &quot;img_path&quot;: &quot;./img_2/&quot;,</span><br><span class="line">        &quot;page&quot;: 38,</span><br><span class="line">        &quot;id&quot;: &quot;4291192513&quot;</span><br><span class="line">    &#125;,&#123;</span><br><span class="line">        &quot;pdf_name&quot;: &quot;深度树匹配——下一代推荐技术的探索和实践.pdf&quot;,</span><br><span class="line">        &quot;img_path&quot;: &quot;./img_3/&quot;,</span><br><span class="line">        &quot;page&quot;: 33,</span><br><span class="line">        &quot;id&quot;: &quot;3621355867&quot;</span><br><span class="line">    &#125;,&#123;</span><br><span class="line">        &quot;pdf_name&quot;: &quot;基于知识的搜索推荐技术及应用.pdf&quot;,</span><br><span class="line">        &quot;img_path&quot;: &quot;./img_4/&quot;,</span><br><span class="line">        &quot;page&quot;: 31,</span><br><span class="line">        &quot;id&quot;: &quot;436657700&quot;</span><br><span class="line">    &#125;,&#123;</span><br><span class="line">        &quot;pdf_name&quot;: &quot;瓜子二手车个性化推荐的挑战与应对.pdf&quot;,</span><br><span class="line">        &quot;img_path&quot;: &quot;./img_5/&quot;,</span><br><span class="line">        &quot;page&quot;: 40,</span><br><span class="line">        &quot;id&quot;: &quot;1433077746&quot;</span><br><span class="line">    &#125;,&#123;</span><br><span class="line">        &quot;pdf_name&quot;: &quot;机器学习在苏宁搜索平台中的实践.pdf&quot;,</span><br><span class="line">        &quot;img_path&quot;: &quot;./img_6/&quot;,</span><br><span class="line">        &quot;page&quot;: 55,</span><br><span class="line">        &quot;id&quot;: &quot;1155556309&quot;</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h1 id="生成PDF"><a href="#生成PDF" class="headerlink" title="生成PDF"></a>生成PDF</h1><p>遍历要生成PDF的每个信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">for one in _list:</span><br><span class="line">    print(one)</span><br><span class="line">    for i in range(1,one[&quot;page&quot;]+1):</span><br><span class="line">        img_path = one[&quot;img_path&quot;]</span><br><span class="line">        pdf_path = one[&quot;pdf_name&quot;]</span><br><span class="line">        if i &lt; 10:</span><br><span class="line">            img_url = &quot;https://static001.geekbang.org/con/37/pdf/&quot; + one[&quot;id&quot;] + &quot;/image/page-00&quot; + str(i) + &quot;.jpg&quot;</span><br><span class="line">        else:</span><br><span class="line">            img_url = &quot;https://static001.geekbang.org/con/37/pdf/&quot; + one[&quot;id&quot;] + &quot;/image/page-0&quot; + str(i) + &quot;.jpg&quot;</span><br><span class="line">        print(img_url)</span><br><span class="line">        # 下载图片</span><br><span class="line">        save_img(img_url,img_path)</span><br><span class="line">        # 合成pdf</span><br><span class="line">        convert_img_to_pdf(img_path, pdf_path)</span><br><span class="line">    print(&quot;Image转PDF完成！&quot;)</span><br></pre></td></tr></table></figure></p><p>PDF文件资料获取方式：百度网盘</p><p>链接：<a href="https://pan.baidu.com/s/1MC_bH2x5jjsP1LvBB5cZCA" target="_blank" rel="external">https://pan.baidu.com/s/1MC_bH2x5jjsP1LvBB5cZCA</a></p><p>提取码：58zq</p><hr><p>原文链接：<a href="https://blog.csdn.net/Gamer_gyt/article/details/85637436" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/85637436</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="Python" scheme="http://thinkgamer.cn/tags/Python/"/>
    
      <category term="资料" scheme="http://thinkgamer.cn/tags/%E8%B5%84%E6%96%99/"/>
    
  </entry>
  
  <entry>
    <title>多分类逻辑回归（Multinomial Logistic Regression）</title>
    <link href="http://thinkgamer.cn/2018/12/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%88%86%E7%B1%BB%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Multinomial%20Logistic%20Regression%EF%BC%89/"/>
    <id>http://thinkgamer.cn/2018/12/21/机器学习/多分类逻辑回归（Multinomial Logistic Regression）/</id>
    <published>2018-12-21T07:53:10.000Z</published>
    <updated>2019-04-13T05:18:32.057Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>分类从结果的数量上可以简单的划分为：</p><ul><li>二分类（Binary Classification）</li><li>多分类（Multinomial  Classification）。</li></ul><p>其中二分类是最常见且使用最多的分类场景，解决二分类的算法有很多，比如：</p><ul><li>基本的KNN、贝叶斯、SVM</li><li>Online Ranking中用来做二分类的包括FM、FFM、GBDT、LR、XGBoost等</li></ul><p>多分类中比如：</p><ul><li>改进版的KNN、改进版的贝叶斯、改进版的SVM等</li><li>多类别的逻辑回归<br>啰嗦了这么多，其实就是为了说这个多分类的逻辑回归。</li></ul><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>在统计学里，多类别逻辑回归是一个将逻辑回归一般化成多类别问题得到的分类方法。用更加专业的话来说，它就是一个用来预测一个具有类别分布的因变量不同可能结果的概率的模型。 </p><p>另外，多类别逻辑回归也有很多其它的名字，包括polytomous LR，multiclass LR，softmax regression，multinomial logit，maximum entropy classifier，conditional maximum entropy model。 </p><p>在多类别逻辑回归中，因变量是根据一系列自变量（就是我们所说的特征、观测变量）来预测得到的。具体来说，就是通过将自变量和相应参数进行线性组合之后，使用某种概率模型来计算预测因变量中得到某个结果的概率，而自变量对应的参数是通过训练数据计算得到的，有时我们将这些参数成为回归系数。</p><h2 id="模型分析"><a href="#模型分析" class="headerlink" title="模型分析"></a>模型分析</h2><p>原文：<a href="https://blog.csdn.net/Gamer_gyt/article/details/85209496" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/85209496</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="LR" scheme="http://thinkgamer.cn/tags/LR/"/>
    
      <category term="多分类" scheme="http://thinkgamer.cn/tags/%E5%A4%9A%E5%88%86%E7%B1%BB/"/>
    
      <category term="逻辑回归" scheme="http://thinkgamer.cn/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
      <category term="Logistic Regression" scheme="http://thinkgamer.cn/tags/Logistic-Regression/"/>
    
  </entry>
  
  <entry>
    <title>点击率预估中的FM算法和FFM算法</title>
    <link href="http://thinkgamer.cn/2018/07/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%82%B9%E5%87%BB%E7%8E%87%E9%A2%84%E4%BC%B0%E4%B8%AD%E7%9A%84FM&amp;FFM%E7%AE%97%E6%B3%95/"/>
    <id>http://thinkgamer.cn/2018/07/13/机器学习/点击率预估中的FM&amp;FFM算法/</id>
    <published>2018-07-13T08:45:14.000Z</published>
    <updated>2019-04-13T04:36:17.794Z</updated>
    
    <content type="html"><![CDATA[<p>特征决定了所有算法效果的上限，而不同的算法只是离这个上限的距离不同而已</p><h1 id="CTR方法概览"><a href="#CTR方法概览" class="headerlink" title="CTR方法概览"></a>CTR方法概览</h1><ol><li>广义线性模型+人工特征组合（LR+FeatureEngineering）</li><li>非线性模型（GBDT，FM，FFM，DNN，MLR）</li><li>广义线性模型+非线性模型组合特征（模型融合，常见的是LR+GBDT）</li></ol><p>其中 2（非线性模型）又可以分为：<br>      ○ 矩阵分解类 （FM，FFM）<br>      ○ 回归类        （GBDT，MLR）<br>      ○ 神经网络类 （DNN）</p><hr><p>转载请注明出处：<a href="https://blog.csdn.net/gamer_gyt/article/details/81038913" target="_blank" rel="external">https://blog.csdn.net/gamer_gyt/article/details/81038913</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="FM算法"><a href="#FM算法" class="headerlink" title="FM算法"></a>FM算法</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>FM算法（Factorization Machine）一般翻译为“因子分解机”，2010年，它由当时还在日本大阪大学的Steffen Rendle提出。此算法的主要作用是可以把所有特征进行高阶组合，减少人工参与特征组合的工作，工程师可以将精力集中在模型参数调优。FM只需要线性时间复杂度，可以应用于大规模机器学习。经过部分数据集试验，此算法在稀疏数据集合上的效果要明显好于SVM。</p><h2 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h2><p>假设一个广告分类问题，根据用户和广告位相关的特征，预测用户是否点击了广告。</p><div class="table-container"><table><thead><tr><th>Clicked?</th><th>Country</th><th>Day</th><th>Ad_type</th></tr></thead><tbody><tr><td>1</td><td>USA</td><td>26/11/15</td><td>Movie</td></tr><tr><td>0</td><td>China</td><td>1/7/14</td><td>Game</td></tr><tr><td>1</td><td>China</td><td>19/2/15</td><td>Game</td></tr></tbody></table></div><p>“Clicked?”是label，Country、Day、Ad_type是特征。由于三种特征都是categorical类型的，需要经过独热编码（One-Hot Encoding）转换成数值型特征。</p><div class="table-container"><table><thead><tr><th>Clicked?</th><th>Country=USA</th><th>Country=China</th><th>Day=26/11/15</th><th>Day=1/7/14</th><th>Day=19/2/15</th><th>Ad_type=Movie</th><th>Ad_type=Game</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td></tr><tr><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td></tr><tr><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td></tr></tbody></table></div><p>经过one-hot编码之后，特征变得稀疏，上边实例中每个样本有7维的特征，但平均仅有3维为非0值，在电商场景中预测sku是否被点击的过程中，特征往往要比上例中多的多，可见数据稀疏性是工业环境中不可避免的问题。另外一个问题就是特征在经过one-hot编码之后，维度会变得非常大，比如说三级品类有3k个，那么sku的所属三级品类特征经过编码之后就会变成3k维，特征空间就会剧增。</p><p>依旧分析上边的例子，特征在经过关联之后，与label之间的相关性就会增加。例如“USA”与“Thanksgiving”、“China”与“Chinese New Year”这样的关联特征，对用户的点击有着正向的影响。换句话说，来自“China”的用户很可能会在“Chinese New Year”有大量的浏览、购买行为，而在“Thanksgiving”却不会有特别的消费行为。这种关联特征与label的正向相关性在实际问题中是普遍存在的，如“化妆品”类商品与“女”性，“球类运动配件”的商品与“男”性，“电影票”的商品与“电影”品类偏好等。因此，引入两个特征的组合是非常有意义的。</p><p>综上，FM所解决的问题是<br>1)：特征稀疏<br>2)：特征组合</p><h2 id="模型形式"><a href="#模型形式" class="headerlink" title="模型形式"></a>模型形式</h2><p>对于特征集合x=（x1,x2,x3,x4,….,xn）和标签y，希望得到x和y的对应关系，最简单的是建立线性回归模型，<br><img src="https://bdn.135editor.com/files/users/360/3608534/201807/fBKAMR8S_7Dba.png"></p><p>但是，一般线性模型无法学习到高阶组合特征，所以会将特征进行高阶组合，这里以二阶为例(理论上，FM可以组合任意高阶，但是由于计算复杂度，实际中常用二阶，后面也主要介绍二阶组合的内容)。模型形式为，</p><p><img src="https://bdn.135editor.com/files/users/360/3608534/201807/cOBUyUJj_TmqU.png"></p><p>其中n代表样本的特征数量，x_i是第i个特征，w_0，w_i，w_ij是模型参数<br>从公式(2)中可以看出，组合特征参数一共有n(n-1)/2个，任意两个参数之间都是独立的，在数据特别稀疏的情况下，二次项参数的训练是十分困难的，原因为每个wij的训练都需要大量的非零x_i，x_j样本，由于样本稀疏，满足条件的样本很少，就会导致参数w_ij不准确，最终将影响模型的性能。<br>如何解决二次项参数的训练？可以利用矩阵分解的思路。<br>在model-based的协同过滤中，一个rating矩阵可以分解为user矩阵和item矩阵，每个user和item都可以采用一个隐向量表示。比如在下图中的例子中，我们把每个user表示成一个二维向量，同时把每个item表示成一个二维向量，两个向量的点积就是矩阵中user对item的打分。</p><p><img src="https://bdn.135editor.com/files/users/360/3608534/201807/UhsawK7r_t2gO.png"></p><p>所有的二次项参数 w_ij 可以组成一个对称矩阵W，那么这个矩阵可以分解为W=V^T * V，V的第j列便是第j维特征的隐向量，换句话说就是每个参数w_ij=<v_i,v_j>，这里的v_i和v_j是分别是第i，j个特征的隐向量，这就是FM的核心思想，因此FM的模型方程为（二阶形式）</v_i,v_j></p><p><img src="https://bdn.135editor.com/files/users/360/3608534/201807/ta3RTSJ7_P6vQ.png"></p><p>其中，V_i是第i维特征的隐向量，&lt;<em>,</em>&gt;表示两个向量内积，隐向量的长度k（k&lt;&lt;n），包含k个描述特征的因子，这样二次项的参数便从n^2 减少到了nk，远少于多项式模型的参数数量。<br>另外，参数因子化使得 x_h，x_i的参数和 x_i，x_j 的参数不再是相互独立的，因此我们可以在样本稀疏的情况下相对合理地估计FM的二次项参数。具体来说，x_h，x_i 和 x_i，x_j 的系数分别为 ⟨v_h,v_i⟩ 和 ⟨v_i,v_j⟩，它们之间有共同项 vi。也就是说，所有包含“xi的非零组合特征”（存在某个 j≠i，使得 x_i x_j≠0）的样本都可以用来学习隐向量 v_i，这很大程度上避免了数据稀疏性造成的影响。而在多项式模型中，w_hi 和 w_ij 是相互独立的。<br>显而易见，公式(3)是一个通用的拟合方程，可以采用不同的损失函数用于解决回归、二元分类等问题，比如可以采用MSE（Mean Square Error）损失函数来求解回归问题，也可以采用Hinge/Cross-Entropy损失（铰链损失，互熵损失）来求解分类问题。当然，在进行二元分类时，FM的输出需要经过sigmoid变换，这与Logistic回归是一样的。直观上看，FM的复杂度是 O(kn^2)。但是，通过公式(3)的等式，FM的二次项可以化简，其复杂度可以优化到 O(kn)。由此可见，FM可以在线性时间对新样本作出预测。</p><p><img src="https://bdn.135editor.com/files/users/360/3608534/201807/ZvytpfVA_wCpX.png"></p><p>从(3)—&gt;(4)推导如下：</p><p><img src="https://bdn.135editor.com/files/users/360/3608534/201807/MS5AAkAm_kPrI.png"></p><p>解读第（1）步到第（2）步，这里用A表示系数矩阵V的上三角元素，B表示对角线上的交叉项系数。由于系数矩阵V是一个对称阵，所以下三角与上三角相等，有下式成立：</p><p><img src="https://bdn.135editor.com/files/users/360/3608534/201807/RpSGrOmQ_AfmL.png"><br>如果用随机梯度下降（Stochastic Gradient Descent）法学习模型参数。那么，模型各个参数的梯度如下：</p><p><img src="https://bdn.135editor.com/files/users/360/3608534/201807/T8MAnmvn_apWg.png"></p><p>其中，v_j,f 是隐向量 v_j 的第 f 个元素。由于 ∑nj=1vj,fxj 只与 f 有关，而与 i 无关，在每次迭代过程中，只需计算一次所有 f 的 ∑nj=1vj,fxj，就能够方便地得到所有 v_i,f 的梯度。显然，计算所有 f 的 ∑nj=1vj,fxj 的复杂度是 O(kn)；已知 ∑nj=1vj,fxj 时，计算每个参数梯度的复杂度是 O(1)；得到梯度后，更新每个参数的复杂度是 O(1)；模型参数一共有 nk+n+1 个。因此，FM参数训练的复杂度也是 O(kn)。综上可知，FM可以在线性时间训练和预测，是一种非常高效的模型。</p><h2 id="FM总结"><a href="#FM总结" class="headerlink" title="FM总结"></a>FM总结</h2><h3 id="FM降低了交叉项参数学习不充分的影响"><a href="#FM降低了交叉项参数学习不充分的影响" class="headerlink" title="FM降低了交叉项参数学习不充分的影响"></a>FM降低了交叉项参数学习不充分的影响</h3><p>one-hot编码后的样本数据非常稀疏，组合特征更是如此。为了解决交叉项参数学习不充分、导致模型有偏或不稳定的问题。借鉴矩阵分解的思路：每一维特征用k维的隐向量表示，交叉项的参数w_ij用对应特征隐向量的内积表示，即⟨v_i,v_j⟩（也可以理解为平滑技术）。这样参数学习由之前学习交叉项参数w_ij的过程，转变为学习n个单特征对应k维隐向量的过程。<br>很明显，单特征参数（k维隐向量v_i）的学习要比交叉项参数w_ij学习得更充分。示例说明：<br>假如有10w条训练样本，其中出现女性特征的样本数为3w，出现男性特征的样本数为7w，出现汽车特征的样本数为2000，出现化妆品的样本数为1000。特征共现的样本数如下：</p><div class="table-container"><table><thead><tr><th>共现交叉特征</th><th>样本数</th><th>注</th></tr></thead><tbody><tr><td>&lt;女性，汽车&gt;</td><td>500</td><td>同时出现&lt;女性，汽车&gt;的样本数</td></tr><tr><td>&lt;女性，化妆品&gt;</td><td>1000</td><td>同时出现&lt;女性，化妆品&gt;的样本数</td></tr><tr><td>&lt;男性，汽车&gt;</td><td>1500</td><td>同时出现&lt;男性，汽车&gt;的样本数</td></tr><tr><td>&lt;男性，化妆品&gt;</td><td>0</td><td>样本中无此特征组合项</td></tr></tbody></table></div><p>&lt;女性，汽车&gt;的含义是女性看汽车广告。可以看到，单特征对应的样本数远大于组合特征对应的样本数。训练时，单特征参数相比交叉项特征参数会学习地更充分。<br>因此，可以说FM降低了因数据稀疏，导致交叉项参数学习不充分的影响</p><h3 id="FM提升了模型预估能力"><a href="#FM提升了模型预估能力" class="headerlink" title="FM提升了模型预估能力"></a>FM提升了模型预估能力</h3><p>依然看上面的示例，样本中没有&lt;男性，化妆品&gt;交叉特征，即没有男性看化妆品广告的数据。如果用多项式模型来建模，对应的交叉项参数W_男性,化妆品是学不出来的，因为数据中没有对应的共现交叉特征。那么多项式模型就不能对出现的男性看化妆品广告场景给出准确地预估。<br>FM模型是否能得到交叉项参数W_男性,化妆品呢？答案是肯定的。由于FM模型是把交叉项参数用对应的特征隐向量内积表示，这里表示为W_男性,化妆品=⟨v_男性,v_化妆品⟩。<br>用男性特征隐向量v_男性和v_化妆品特征隐向量v化妆品的内积表示交叉项参数 W_男性,化妆品。<br>由于FM学习的参数就是单特征的隐向量，那么男性看化妆品广告的预估结果可以用⟨v_男性,v_化妆品⟩得到。这样，即便训练集中没有出现男性看化妆品广告的样本，FM模型仍然可以用来预估，提升了预估能力。</p><h3 id="FM提升了参数学习效率"><a href="#FM提升了参数学习效率" class="headerlink" title="FM提升了参数学习效率"></a>FM提升了参数学习效率</h3><p>这个显而易见，参数个数由(n^2+n+1)变为(nk+n+1)个，模型训练复杂度也由O(mn^2)变为O(mnk)。m为训练样本数。对于训练样本和特征数而言，都是线性复杂度。<br>此外，就FM模型本身而言，它是在多项式模型基础上对参数的计算做了调整，因此也有人把FM模型称为多项式的广义线性模型，也是恰如其分的。<br>从交互项的角度看，FM仅仅是一个可以表示特征之间交互关系的函数表法式，可以推广到更高阶形式，即将多个互异特征分量之间的关联信息考虑进来。例如在广告业务场景中，如果考虑User-Ad-Context三个维度特征之间的关系，在FM模型中对应的degree为3。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>FM最大特点和优势：FM模型对稀疏数据有更好的学习能力，通过交互项可以学习特征之间的关联关系，并且保证了学习效率和预估能力</p><h1 id="FFM算法"><a href="#FFM算法" class="headerlink" title="FFM算法"></a>FFM算法</h1><h2 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h2><p>FFM（Field-aware Factorization Machine）最初的概念来自Yu-Chin Juan（阮毓钦，毕业于中国台湾大学，现在美国Criteo工作）与其比赛队员，是他们借鉴了来自Michael Jahrer的论文中的field概念提出了FM的升级版模型。</p><h2 id="模型形式-1"><a href="#模型形式-1" class="headerlink" title="模型形式"></a>模型形式</h2><p>通过引入field的概念，FFM把相同性质的特征，归结于同一个field。还是以FM中的广告分类为例，“Day=26/11/15”、“Day=1/7/14”、“Day=19/2/15”这三个特征都是代表日期的，可以放到同一个field中。同理，三级品类有3k个，那么sku的所属三级品类特征经过编码之后就会变成3k维，那么这3k维可以放到一个field中，简单来说，同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个field。<br>在FFM中，每一维特征x_i，针对其他特征的每一种field f_j，都会学习到一个隐向量V_i,f_ j。因此，隐向量不仅与特征有关，还与filed有关。也就是说，“Day=26/11/15”这个特征与“Country”特征和“Ad_type”特征进行关联的时候使用不同的隐向量，这与“Country”和“Ad_type”的内在差异相符，也是FFM中“field-aware”的由来。<br>假设样本的 n 个特征属于 f 个field，那么FFM的二次项有 nf个隐向量。而在FM模型中，每一维特征的隐向量只有一个。FM可以看作FFM的特例，是把所有特征都归属到一个field时的FFM模型。根据FFM的field敏感特性，可以导出其模型方程。</p><p><img src="https://bdn.135editor.com/files/users/360/3608534/201807/jUKaHRqz_pIvY.png"></p><p>其中，f_ j 是第 j 个特征所属的field。如果隐向量的长度为 k，那么FFM的二次参数有 f * kn 个，远多于FM模型的 kn 个。此外，由于隐向量与field相关，FFM二次项并不能化简，其预测复杂度是 O(kn^2)。<br>下面以一个例子简单说明FFM的特征组合方式。输入记录如下<br>User Movie Genre Price<br>YuChin 3Idiots Comedy, Drama $9.99<br>这条记录可以编码成5个特征，其中“Genre=Comedy”和“Genre=Drama”属于同一个field，“Price”是数值型，不用One-Hot编码转换。为了方便说明FFM的样本格式，我们将所有的特征和对应的field映射成整数编号。</p><div class="table-container"><table><thead><tr><th>Field name</th><th>Field index</th><th>Feature name</th><th>Feature index</th></tr></thead><tbody><tr><td>User</td><td>1</td><td>User=YuChin</td><td>1</td></tr><tr><td>Movie</td><td>2</td><td>Movie=3Idiots</td><td>2</td></tr><tr><td>Genre</td><td>3</td><td>Genre=Comedy</td><td>3</td></tr><tr><td>Price</td><td>4</td><td>Genre=Drama</td><td>4</td></tr><tr><td></td><td></td><td>Price</td><td>5</td></tr></tbody></table></div><p>那么，FFM的组合特征有10项，如下图所示。</p><p><img src="https://bdn.135editor.com/files/users/360/3608534/201807/BW3Vpww7_n3ZY.png"></p><p>其中，红色是field编号，蓝色是特征编号，绿色是此样本的特征取值。二次项的系数是通过与特征field相关的隐向量点积得到的，二次项共有 n(n−1)/2 个。</p><h2 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h2><p>在DSP的场景中，FFM主要用来预估站内的CTR和CVR，即一个用户对一个商品的潜在点击率和点击后的转化率。<br>CTR和CVR预估模型都是在线下训练，然后用于线上预测。两个模型采用的特征大同小异，主要有三类：<br>  ● -用户相关的特征<br>用户相关的特征包括年龄、性别、职业、兴趣、品类偏好、浏览/购买品类等基本信息，以及用户近期点击量、购买量、消费额等统计信息。<br>  ● 商品相关的特征<br>商品相关的特征包括所属品类、销量、价格、评分、历史CTR/CVR等信息。<br>  ● 用户-商品匹配特征<br>用户-商品匹配特征主要有浏览/购买品类匹配、浏览/购买商家匹配、兴趣偏好匹配等几个维度。<br>为了使用FFM方法，所有的特征必须转换成“field_id:feat_id:value”格式，field_id代表特征所属field的编号，feat_id是特征编号，value是特征的值。数值型的特征比较容易处理，只需分配单独的field编号，如用户评论得分、商品的历史CTR/CVR等。categorical特征需要经过One-Hot编码成数值型，编码产生的所有特征同属于一个field，而特征的值只能是0或1，如用户的性别、年龄段，商品的品类id等。<br>除此之外，还有第三类特征，如用户浏览/购买品类，有多个品类id且用一个数值衡量用户浏览或购买每个品类商品的数量。这类特征按照categorical特征处理，不同的只是特征的值不是0或1，而是代表用户浏览或购买数量的数值。按前述方法得到field_id之后，再对转换后特征顺序编号，得到feat_id，特征的值也可以按照之前的方法获得。<br>CTR、CVR预估样本的类别是按不同方式获取的。CTR预估的正样本是站内点击的用户-商品记录，负样本是展现但未点击的记录；CVR预估的正样本是站内支付（发生转化）的用户-商品记录，负样本是点击但未支付的记录。构建出样本数据后，采用FFM训练预估模型，并测试模型的性能。</p><div class="table-container"><table><thead><tr><th></th><th>#(field)</th><th>#(feature)</th><th>AUC</th><th>Logloss</th></tr></thead><tbody><tr><td>站内CTR</td><td>39</td><td>2456</td><td>0.77</td><td>0.38</td></tr><tr><td>站内CVR</td><td>67</td><td>2441</td><td>0.92</td><td>0.13</td></tr></tbody></table></div><p>由于模型是按天训练的，每天的性能指标可能会有些波动，但变化幅度不是很大。这个表的结果说明，站内CTR/CVR预估模型是非常有效的。<br>在训练FFM的过程中，有许多小细节值得特别关注。<br>第一，样本归一化。FFM默认是进行样本数据的归一化，即 pa.norm 为真；若此参数设置为假，很容易造成数据inf溢出，进而引起梯度计算的nan错误。因此，样本层面的数据是推荐进行归一化的。<br>第二，特征归一化。CTR/CVR模型采用了多种类型的源特征，包括数值型和categorical类型等。但是，categorical类编码后的特征取值只有0或1，较大的数值型特征会造成样本归一化后categorical类生成特征的值非常小，没有区分性。例如，一条用户-商品记录，用户为“男”性，商品的销量是5000个（假设其它特征的值为零），那么归一化后特征“sex=male”（性别为男）的值略小于0.0002，而“volume”（销量）的值近似为1。特征“sex=male”在这个样本中的作用几乎可以忽略不计，这是相当不合理的。因此，将源数值型特征的值归一化到 [0,1] 是非常必要的。<br>第三，省略零值特征。从FFM模型的表达式(8)可以看出，零值特征对模型完全没有贡献。包含零值特征的一次项和组合项均为零，对于训练模型参数或者目标值预估是没有作用的。因此，可以省去零值特征，提高FFM模型训练和预测的速度，这也是稀疏样本采用FFM的显著优势。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ol><li><a href="http://bourneli.github.io/ml/fm/2017/07/02/fm-remove-combine-features-by-yourself.html" target="_blank" rel="external">http://bourneli.github.io/ml/fm/2017/07/02/fm-remove-combine-features-by-yourself.html</a></li><li><a href="https://tech.meituan.com/deep_understanding_of_ffm_principles_and_practices.html" target="_blank" rel="external">https://tech.meituan.com/deep_understanding_of_ffm_principles_and_practices.html</a></li><li><a href="http://www.52caml.com/head_first_ml/ml-chapter9-factorization-family/" target="_blank" rel="external">http://www.52caml.com/head_first_ml/ml-chapter9-factorization-family/</a></li><li><a href="https://cloud.tencent.com/developer/article/1104673?fromSource=waitui" target="_blank" rel="external">https://cloud.tencent.com/developer/article/1104673?fromSource=waitui</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;特征决定了所有算法效果的上限，而不同的算法只是离这个上限的距离不同而已&lt;/p&gt;
&lt;h1 id=&quot;CTR方法概览&quot;&gt;&lt;a href=&quot;#CTR方法概览&quot; class=&quot;headerlink&quot; title=&quot;CTR方法概览&quot;&gt;&lt;/a&gt;CTR方法概览&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;广
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="推荐系统" scheme="http://thinkgamer.cn/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="FM" scheme="http://thinkgamer.cn/tags/FM/"/>
    
      <category term="FFM" scheme="http://thinkgamer.cn/tags/FFM/"/>
    
  </entry>
  
</feed>
