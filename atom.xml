<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>文艺与Code | Thinkgamer的博客</title>
  <icon>https://www.gravatar.com/avatar/1b9c8afc3fc1dc6be26316835c6f4fc4</icon>
  <subtitle>CTR/DL/ML/RL</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://thinkgamer.cn/"/>
  <updated>2019-04-21T07:34:39.260Z</updated>
  <id>http://thinkgamer.cn/</id>
  
  <author>
    <name>Thinkgamer</name>
    <email>thinkgamer@163.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Thinkgamer&#39;s 简历</title>
    <link href="http://thinkgamer.cn/8888/08/08/%E5%85%B3%E4%BA%8E%E6%88%91/"/>
    <id>http://thinkgamer.cn/8888/08/08/关于我/</id>
    <published>8888-08-08T00:08:08.000Z</published>
    <updated>2019-04-21T07:34:39.260Z</updated>
    
    <content type="html"><![CDATA[<hr><h1 id="个人信息"><a href="#个人信息" class="headerlink" title="个人信息"></a>个人信息</h1><p>唯一：&nbsp;&nbsp;&nbsp;&nbsp;<strong>Thinkgamer</strong><br>姓名：&nbsp;&nbsp;&nbsp;&nbsp;<strong>高阳团</strong><br>家乡：&nbsp;&nbsp;&nbsp;&nbsp;<strong>河南-郑州</strong><br>邮箱：&nbsp;&nbsp;&nbsp;&nbsp;<strong>thinkgamer@163.com</strong><br>毕业：&nbsp;&nbsp;&nbsp;&nbsp;<strong>沈阳航空航天大学-计算机学院-软件工程</strong><br>就职：&nbsp;&nbsp;&nbsp;&nbsp;<strong>北京 | 京东商城 | 算法工程师</strong></p><hr><h1 id="技术园地："><a href="#技术园地：" class="headerlink" title="技术园地："></a>技术园地：</h1><ul><li>CSDN：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a></li><li>Github：<a href="https://github.com/Thinkgamer" target="_blank" rel="external">https://github.com/Thinkgamer</a></li><li>知乎: <a href="https://www.zhihu.com/people/thinkgamer/activities" target="_blank" rel="external">https://www.zhihu.com/people/thinkgamer/activities</a></li><li>公众号：数据与算法联盟<br><img src="/assets/img/gongzhonghao.jpg" weight="250px" height="250px"></li></ul><hr><h1 id="工作经历"><a href="#工作经历" class="headerlink" title="工作经历"></a>工作经历</h1><h2 id="2017-12-25～至今-京东商城"><a href="#2017-12-25～至今-京东商城" class="headerlink" title="2017-12-25～至今 | 京东商城"></a>2017-12-25～至今 | 京东商城</h2><ul><li>个性化消息Push</li></ul><blockquote><p>简称“种草”。针对京东用户进行消息的个性化Push，增强用户黏性和交互。<br>负责种草整个联动方案，组织会议进行讨论和需求下发，资源安排等。<br>基于机器学习的个性化消息push模型开发，训练，调优，上线等。<br>深度学习模型调研，基于公司内部平台，上线基于tensorflow-serving的深度学习模型，效果较ML模型提升显著。</p></blockquote><ul><li>Plus会员个性化推荐</li></ul><blockquote><p>理解内部推荐架构原理，负责开发方案，资源安排。<br>基于机器学习的个性化消息push模型开发，训练，调优，上线等。<br>相关CTR预估模型研究与在plus业务数据集上的离线测试。</p></blockquote><ul><li>商品价格段模型</li></ul><blockquote><p>基于KMeans构建商品价格段模型，实现了基于MR和Spark两个版本的代码。</p></blockquote><ul><li>商品质量分模型</li></ul><blockquote><p>基于线性模型构建商品质量分模型，用户推荐架构中的召回粗排。 </p></blockquote><ul><li>特征监控模型</li></ul><blockquote><p>数据和特征决定了机器学习的上限,而模型和算法只是逼近这个上限。特征对于模型来说极其重要，因为对于特征的监控十分有必要，该模型支持使用者自定义监控指标和监控字段，能够有效的减少出现问题时的排查时间提高效果，并进行预警。</p></blockquote><ul><li>基础数据开发<ul><li>业务内特征开发</li><li>全站特征开发</li><li>召回数据开发</li></ul></li></ul><h2 id="2016-10～2017-12-北京万维星辰科技有限公司"><a href="#2016-10～2017-12-北京万维星辰科技有限公司" class="headerlink" title="2016-10～2017-12 | 北京万维星辰科技有限公司"></a>2016-10～2017-12 | 北京万维星辰科技有限公司</h2><ul><li>搭建基于 Hadoop 和 ELK 技术栈的日志分析系统</li></ul><blockquote><p>参与设计了基于 ELK 的日志分析系统，提出并搭建了 Hadoop 数据备份系统，研究了 ELK 周边的<br>开源产品 ，学习并使用 rails 实现 es 数据的快照备份。</p></blockquote><ul><li>异常检测算法研究与实现</li></ul><blockquote><p>1：根据合作方提供的 wlan 上网数据，对用户进行肖像刻画，从而对后入数据进行异常值估计。</p><p>2：研究基于指数平滑和线性回归的异常值检测，并使用 python 的 elasticsearch 进行实现。</p></blockquote><ul><li>中彩/德州银行日志审计项目</li></ul><blockquote><p>利用公司的日志分析系统对中国福利彩票和德州银行的日志进行分析，并形成安全事件，提出相应的整改和解决意见，形成月度报告。</p></blockquote><h2 id="2016-07～2016-09-北京广联达软件有限公司"><a href="#2016-07～2016-09-北京广联达软件有限公司" class="headerlink" title="2016-07～2016-09 | 北京广联达软件有限公司"></a>2016-07～2016-09 | 北京广联达软件有限公司</h2><blockquote><p>实习以课题形式（课题为：基于质量数据的数据分析平台搭建）展开，利用 Hadoop 等开源组件搭建了 5 台分布式系统，包含 Hadoop，Hive，Spark，Zookeeper，Sqoop 和 Hbase，在该平台上完成了豆瓣影评数据分析 Demo</p></blockquote><hr><h1 id="技能掌握"><a href="#技能掌握" class="headerlink" title="技能掌握"></a>技能掌握</h1><ul><li>熟练掌握基于机器学习和深度学习的CTR预估算法，包括GBDT/LR/FM/FFM/FTRL/XGBoost/Wide&amp;Deep/DeepFM/DNN/FNN等。</li><li>熟悉推荐系统的数据流和过滤，召回，排序，展示等架构。</li><li>熟练掌握相关机器学习算法并用来构建基本模型。</li><li>熟练用户画像/物品画像/特征工程。</li><li>熟练Spark/MR/Hive/Python开发，了解相关大数据产品。</li><li>了解爬虫/Web后端开发，曾开发多个基于Django的网站后端。</li><li>了解强化学习/迁移学习/NLP。</li><li>熟悉ELK技术栈/Linux/Docker。</li></ul><hr><h1 id="大学经历"><a href="#大学经历" class="headerlink" title="大学经历"></a>大学经历</h1><h2 id="项目经历"><a href="#项目经历" class="headerlink" title="项目经历"></a>项目经历</h2><ul><li>基于 Hadoop 和机器学习的博客统计分析平台</li></ul><blockquote><p>采用 Django 作为 Web 开发基础，Python 爬取了 CSDN 博客的部分数据，存储到 hdfs 上，利用 MapReduce 对数据进行了离线计算，将解析好的字段存储到 Hive 中，利用 python 开发实现了协同过滤算法和 PangRank 算法。最终此项目在辽宁省计算机作品大赛中获得二等奖，中国大学生计算机作品大赛中获得三等奖。</p></blockquote><ul><li>图书推荐系统</li></ul><blockquote><p>python 爬取了豆瓣图书数据，对数据进行清洗之后，使用基于 Item 和 User 的协同过滤算法对登录用户产生图书推荐，此项目为大三期间为一个网友做的毕业设计。</p></blockquote><h2 id="荣誉奖励"><a href="#荣誉奖励" class="headerlink" title="荣誉奖励"></a>荣誉奖励</h2><ul><li>单项一等奖学金  * 2</li><li>综合二等奖学金  * 2</li><li>单项支援服务标兵</li><li>优秀团干 * 2</li><li>辽宁省ACM优秀志愿者</li><li>校ACM三等奖</li><li>沈阳航空航天大学计算机作品大赛二等奖【网站】</li><li>辽宁省计算机作品大赛二等奖【博客统计分析系统】</li><li>中国大学生作品大赛三等奖【博客统计分析系统】</li></ul><h2 id="工作经历-1"><a href="#工作经历-1" class="headerlink" title="工作经历"></a>工作经历</h2><ul><li>助理辅导员 | 2014.09-2015.07</li></ul><blockquote><p>计算机学院 2014 级新生助理辅导员，协助辅导员进行大一班级的日常管理</p></blockquote><ul><li>活动部部长 | 2014.09-2015.07</li></ul><blockquote><p>爱心联合会活动部部长，负责相关活动的宣传推广与执行</p></blockquote><ul><li>班级团支书 | 2013.09-2017.06</li></ul><blockquote><p>协助辅导员进行班级日常的管理和相关共青团工作的开展</p></blockquote><hr><h1 id="自我评价"><a href="#自我评价" class="headerlink" title="自我评价"></a>自我评价</h1><ul><li>不服输，爱钻研，具有自学能力和解决问题能力。</li><li>喜欢看书，看文章，整理笔记。</li><li>文艺Coder。</li></ul><hr><h1 id="联系我："><a href="#联系我：" class="headerlink" title="联系我："></a>联系我：</h1><ul><li><img src="/assets/img/myweixin.png" height="300" width="220"></li></ul><p>PS：加我微信，拉你进数据与算法交流群，进行头脑风暴</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;h1 id=&quot;个人信息&quot;&gt;&lt;a href=&quot;#个人信息&quot; class=&quot;headerlink&quot; title=&quot;个人信息&quot;&gt;&lt;/a&gt;个人信息&lt;/h1&gt;&lt;p&gt;唯一：&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;strong&gt;Thinkgamer&lt;/strong&gt;&lt;br
      
    
    </summary>
    
    
      <category term="Thinkgamer" scheme="http://thinkgamer.cn/tags/Thinkgamer/"/>
    
  </entry>
  
  <entry>
    <title>商务合作介绍</title>
    <link href="http://thinkgamer.cn/6666/06/06/%E5%95%86%E5%8A%A1%E5%90%88%E4%BD%9C%E4%BB%8B%E7%BB%8D/"/>
    <id>http://thinkgamer.cn/6666/06/06/商务合作介绍/</id>
    <published>6666-06-05T16:00:00.000Z</published>
    <updated>2019-04-13T04:40:15.346Z</updated>
    
    <content type="html"><![CDATA[<hr><center><h1>WelCome To “Thinkgamer 小站”</h1></center><hr><center><h2>合作范围</h2></center><div class="table-container"><table><thead><tr><th style="text-align:center">Web全栈</th><th style="text-align:center">数据服务</th><th style="text-align:center">模型构建</th></tr></thead><tbody><tr><td style="text-align:center">:-:</td><td style="text-align:center">:-:</td><td style="text-align:center">:-: </td></tr><tr><td style="text-align:center">论文算法实现</td><td style="text-align:center">大数据服务</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">:-:</td><td style="text-align:center">:-:</td><td style="text-align:center">:-: </td></tr><tr><td style="text-align:center">跟拍摄影</td><td style="text-align:center">广告接入</td><td style="text-align:center"></td></tr></tbody></table></div><blockquote><p>全网唯一ID：Thinkgamer，左侧”关于我“关注微信公众号”数据与算法联盟“，可在公众号添加我的微信，本人涉猎范围包括：推荐系统，Python，机器学习，Web开发，大数据云计算，ELK。</p></blockquote><h1 id="▶-Web全栈"><a href="#▶-Web全栈" class="headerlink" title="▶ Web全栈"></a>▶ Web全栈</h1><blockquote><p>如果您在创业，苦于没有额外精力管理一个技术团队；如果您在工作，遇到了一些您解决不了的问题；如果您的网站苦于没有运维；如果您的数据需要备份；如果一切有关Web开发运维的问题。您都可以来找我，我虽不是最厉害的，但绝对会为您提供最优质的服务。</p></blockquote><h1 id="▶-数据服务"><a href="#▶-数据服务" class="headerlink" title="▶ 数据服务"></a>▶ 数据服务</h1><blockquote><p>包含但不局限于以下数据相关的服务：</p></blockquote><ul><li>数据采集（一次性和程序开发）</li><li>数据清洗</li><li>数据可视化（不限于Web）</li><li>数据存储方案设计与实现</li><li>… …</li></ul><p>本人曾多次向他人提供数据相关的技术服务，积累了一定的经验，相信能够为您提供全方位的数据服务。</p><h1 id="▶-模型构建"><a href="#▶-模型构建" class="headerlink" title="▶ 模型构建"></a>▶ 模型构建</h1><blockquote><p>根据对方提供的具体业务场景，进行相关模型选择与构建。当然，如果有荣幸参与您的场景选定和数据准备阶段，也是极好的。</p></blockquote><h1 id="▶-论文算法实现"><a href="#▶-论文算法实现" class="headerlink" title="▶ 论文算法实现"></a>▶ 论文算法实现</h1><blockquote><p>如果您是一个马上要毕业的本科或者研究生，如果您苦于论文的立项与项目实现，如果您没有更好的主意，欢迎您来找我，加我的个人微信，为您的毕业保驾护航。</p></blockquote><h1 id="▶-大数据与分布式计算"><a href="#▶-大数据与分布式计算" class="headerlink" title="▶ 大数据与分布式计算"></a>▶ 大数据与分布式计算</h1><blockquote><p>提供大数据相关的服务，包含但不局限于：</p></blockquote><ul><li>大数据分析平台方案设计</li><li>大数据分析平台搭建</li><li>基于平台的数据分析Demo实现</li><li>海量数据的分布式计算处理</li><li>… …</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;欢迎来找我，24小时在线。</p><h1 id="▶-广告接入"><a href="#▶-广告接入" class="headerlink" title="▶ 广告接入"></a>▶ 广告接入</h1><blockquote><p>眼前的黑不是黑，Ta们说的白是什么白，也许一直是我们忘了搭一座桥，到对方的心里瞧一瞧。你的品牌，你的知名度为什么那么低，因为你没有使用我的广告接入，那么问题来了，包含但不局限于以下几种情况的，可以加我微信私聊了：</p></blockquote><ul><li>品牌宣传</li><li>广告位接入</li><li>公众号互相推广</li><li>个人网站/社区主页链接</li><li>… …</li></ul><h1 id="▶-跟拍摄影"><a href="#▶-跟拍摄影" class="headerlink" title="▶ 跟拍摄影"></a>▶ 跟拍摄影</h1><blockquote><p>如果您在旅游途中缺少了一个摄影的小跟班；如果您苦于找不到好的角度拍照；如果您是一个人，苦于没有人照出你的美；如果您的照片需要美化与调整。那么请您来找我，保证为您提供最优质的技术与服务。</p><p>业务涉及：</p></blockquote><ul><li>跟拍摄影</li><li>照片美化与调整</li><li>PS技术服务</li></ul><center><font color="#0099ff" size="8" face="黑体">小本生意&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;诚信经验<br><br>大神勿扰&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;自求多福</font> </center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;hr&gt;
&lt;center&gt;
&lt;h1&gt;WelCome To “Thinkgamer 小站”&lt;/h1&gt;
&lt;/center&gt;

&lt;hr&gt;
&lt;center&gt;
&lt;h2&gt;合作范围&lt;/h2&gt;
&lt;/center&gt;

&lt;div class=&quot;table-container&quot;&gt;
&lt;table&gt;
&lt;t
      
    
    </summary>
    
    
      <category term="商务合作" scheme="http://thinkgamer.cn/tags/%E5%95%86%E5%8A%A1%E5%90%88%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>前馈神经网络介绍和参数学习</title>
    <link href="http://thinkgamer.cn/2019/04/23/TensorFlow/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0/"/>
    <id>http://thinkgamer.cn/2019/04/23/TensorFlow/前馈神经网络介绍和参数学习/</id>
    <published>2019-04-23T08:36:43.000Z</published>
    <updated>2019-04-23T22:33:06.712Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><blockquote><p>给定一组神经元，我们可以以神经元为节点来构建一个网络。不同的神经网络模型有着不同网络连接的拓扑结构。一种比较直接的拓扑结构是前馈网络。前馈神经网络（Feedforward Neural Network，FNN）是最早发明的简单人工神经网络。</p></blockquote><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>在前馈神经网络中，不同的神经元属于不同的层，每一层的神经元可以接受到前一层的神经元信号，并产生信号输出到下一层。第0层叫做输入层，最后一层叫做输出层，中间的叫做隐藏层，整个网络中无反馈，信号从输入层到输出层单向传播，可用一个有用无环图表示。</p><p>前馈神经网络也成为多层感知器（Mutlti-Layer Perceptron，MLP）。但是多层感知器的叫法并不准确，因为前馈神经网络其实是由多层Logistic回归模型（连续的非线性模型）组成，而不是有多层感知器模型（非连续的非线性模型）组成。</p><p>下图为简单的前馈神经网络图：</p><p><img src="https://img-blog.csdnimg.cn/20190422193716850.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="多层前馈神经网络"></p><p>神经网络中涉及的多个概念：</p><ul><li>L：表示神经网络的层数</li><li>m^l：表示第 l 层神经元个数</li><li>f_l(.)：表示第 l 层神经元的激活函数</li><li>W^l：表示第 l-1 层到第 l 层的权重矩阵</li><li>b^l：表示第 l-1 层到第 l 层的偏置</li><li>z^l：表示第 l 层神经元的净输入（净活性值）</li><li>a^l：表示第l层的神经元输出（活性值）</li></ul><p>神经网络的信息传播公式如下（公式1-1）</p><script type="math/tex; mode=display">z^l  = W^l \cdot a^{l-1} + b^l\\a^l = f_l(z^l)</script><p>公式1-1也可以合并写为（公式1-2）：</p><script type="math/tex; mode=display">z^l = W^l \cdot f_{l-1}(z^{l-1}) + b^l</script><p>或者（公式1-3）</p><script type="math/tex; mode=display">a^l = f_l(W^l \cdot a^{l-1} + b^l)</script><p>这样神经网络可以通过逐层的信息传递，得到网络最后的输出a^L。整个网络可以看做一个符合函数</p><script type="math/tex; mode=display">\phi (x; W,b)</script><p>将向量x作为第一层的输入a^0，将第 l 层的输入a^0，将第L层的输出a^L 作为整个函数的输出。</p><script type="math/tex; mode=display">x = a^0 \rightarrow z^1 \rightarrow a^1 \rightarrow z^2 .... \rightarrow a^{L-1} \rightarrow  z^L \rightarrow  a^L = \phi (x;W,b)</script><p>其中W, b表示网络中所有层的连接权重和偏置。</p><h1 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h1><p>如果采用交叉熵损失函数，对于样本(x，y)，其损失函数为（公式1-4）：</p><script type="math/tex; mode=display">L(y,\hat{y}) = -y^T log (\hat{y})</script><p>其中 y 属于{0,1}^T为标签y对应的one-hot向量。</p><p>给定训练集D={(x^n,y^n)}, N &gt;= n &gt;=0，将每个样本x^n输入给前馈神经网络，得到网络输出为y^n，其在数据集D上的结构化风险函数为（公式1-5）：</p><script type="math/tex; mode=display">R(W,b)=\frac{1}{N}\sum_{n=1}^{N} L(y^n,\hat{y}^n) + \frac{1}{2}\lambda \left \| W \right \|_F^2</script><p>其中W和b分别表示网络中所有的权重矩阵和偏置向量， (||W||_F)^2是正则化项，用来防止过拟合，lambda是为正数的超参数，lambda越大，W越接近于0。这里的(||W||_F)^2一般使用Frobenius范数：</p><script type="math/tex; mode=display">\left \| W \right \|_F^2= \sum_{l=1}^{L} \sum_{i=1}^{m^l} \sum_{j=1}^{m^{l-1}} (W_{ij}^l)^2</script><p>有了学习准则和训练样本，网络参数可以通过梯度下降法来进行学习。在梯度下降方法的每次迭代过程中，第l层的参数 W^l 和 b^l 参数更新方式为（公式1-6）：</p><script type="math/tex; mode=display">W^l \leftarrow W^l - \alpha \frac{\partial R(W,b)}{\partial W^l}=W^l - \alpha ( \frac{1}{N} \sum_{n=1}^{N}(\frac{\partial L(y^n,\hat{y}^n)}{\partial W^l}) + \lambda W^l )\\b^l \leftarrow b^l - \alpha \frac{\partial R(W,b)}{\partial b^l}=b^l - \alpha ( \frac{1}{N} \sum_{n=1}^{N}(\frac{\partial L(y^n,\hat{y}^n)}{\partial b^l}) )</script><p>其中alpha为学习参数。</p><p>梯度下降法需要计算损失函数对参数的偏导数，如果通过链式法则逐一对每个参数进行求偏导效率比较低。在神经网络的训练中经常使用反向传播算法来高效的计算梯度。</p><h1 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h1><p>基于误差的反向传播算法（backpropagation，BP）的前馈神经网络训练过程可以分为以下三步：</p><ul><li>前馈计算每一层的净输入z^l  和激活值 a^l，直到最后一层</li><li>反向传播计算每一层的误差项</li><li>计算每一层参数的偏导数，并更新参数</li></ul><p>其具体训练过程如下：</p><p><img src="https://img-blog.csdnimg.cn/20190423152427560.png" alt="image"></p><h1 id="自动梯度计算"><a href="#自动梯度计算" class="headerlink" title="自动梯度计算"></a>自动梯度计算</h1><p>神经网络中的参数主要是通过梯度下降来进行优化的。当确定了风险函数及网络结构后，我们就可以手动用链式法则来计算风险函数对每个参数的梯度，并用代码进行实现。</p><p>目前几乎所有的深度学习框架都包含了自动梯度计算的功能，在使用框架进行神经网络开发时，我们只需要考虑网络的结构并用代码实现，其梯度可以自动进行计算，无需人工干预，这样开发效率就大大提高了。</p><p>自动梯度计算方法分为以下三种：</p><h2 id="数值微分"><a href="#数值微分" class="headerlink" title="数值微分"></a>数值微分</h2><p>数值微分（Numerical Differentiation）是用数值方法计算函数f(x)的导数。函数f(x)的点x的导数定义为：</p><script type="math/tex; mode=display">f'(x) = \underset{\Delta x \rightarrow 0}{ lim } \frac{ f(x + \Delta x) -f(x) }{ \Delta x }</script><p>要计算f(x)在点x的导数，可以对x加上一个很少的非零扰动，然后通过上述定义来直接计算函数f(x)的梯度。数值微分方法非常容易实现，但找到一个合适扰动非常难，如果扰动过小会引起数值计算问题，比如<strong>舍入误差</strong>；如果扰动过大，会增加<strong>截断误差</strong>，使得导数计算不准确，因此数值微分的实用性比较差，在实际应用中，常用以下公式来计算梯度可以减少截断误差。</p><script type="math/tex; mode=display">f'(x) = \underset{\Delta x \rightarrow 0}{ lim } \frac{ f(x + \Delta x) -f(x -\Delta x) }{2 \Delta x }</script><ul><li>舍入误差：是指数值计算中由于数字舍入造成的近似值和精确值之间的差异，比如用浮点数来表示实数。</li><li>截断误差：数学模型的理论解与数值计算问题的精确解之间的误差</li></ul><h2 id="符号微分"><a href="#符号微分" class="headerlink" title="符号微分"></a>符号微分</h2><p>符号微分（Symbolic Differentiation）是一种基于符号计算的自动求导方法。符号计算，也叫代数计算，是指用计算机来处理带有变量的数学表达式。</p><p>符号计算的输入和输出都是数学表达式的化简、因式分解、微分、积分、解代数方程、求解常微分方程等运算。</p><p>比如数学表达式的化简</p><ul><li>输入：3x-x+2x+1</li><li>输出：4x+1</li></ul><p>符号计算一般来讲是对输入的表达式，通过迭代或递归使用一些事先定义的规则进行转换。当转换结果不能再继续使用变换规则时，便停止计算。</p><h2 id="自动微分"><a href="#自动微分" class="headerlink" title="自动微分"></a>自动微分</h2><p>自动微分（Automatic Differentiation，AD）是一种可以对一个（程序）函数进行计算导数的方法。符号微分的处理对象是数学表达式，而自动微分的处理对象是一个函数或一段程序。而自动微分可以直接在原始程序代码进行微分。自动微分的基本原理是所有的数值计算可以分解为一些基本操作，包含+,−,×, / 和一些初等函数exp, log, sin, cos 等。</p><p>自动微分也是利用链式法则来自动计算一个复合函数的梯度。我们以一个神经网络中常见的复合函数的例子来说明自动微分的过程。为了简单起见，令复合函数f(x;w, b) 为</p><script type="math/tex; mode=display">f(x;w,b)=\frac{1}{ exp(-(wx+b))+1 }</script><p>其中x 为输入标量，w和b 分别为权重和偏置参数。</p><p>复合函数f(x;w,b) 可以拆解为：</p><p><img src="https://img-blog.csdnimg.cn/20190423161240321.png" alt="image"></p><p>继而就可以通过链式求导法则进行复合函数求导。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="深度学习" scheme="http://thinkgamer.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="TensorFlow" scheme="http://thinkgamer.cn/tags/TensorFlow/"/>
    
      <category term="神经网络" scheme="http://thinkgamer.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>神经网络中的激活函数介绍</title>
    <link href="http://thinkgamer.cn/2019/04/21/TensorFlow/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E4%BB%8B%E7%BB%8D/"/>
    <id>http://thinkgamer.cn/2019/04/21/TensorFlow/神经网络中的激活函数介绍/</id>
    <published>2019-04-21T14:25:15.000Z</published>
    <updated>2019-04-23T22:31:14.644Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><blockquote><p>人工神经元（Artifical Neuron）简称神经元（Neuron），是构成神经网络的基本单元，其主要是模拟生物神经元的结构和特性，接受一组输入信息并产出输出。</p></blockquote><h1 id="激活函数（Activation-Function）"><a href="#激活函数（Activation-Function）" class="headerlink" title="激活函数（Activation Function）"></a>激活函数（Activation Function）</h1><p>是神经元中非常重要的一部分，为了增强网络的表示能力和学习能力，激活函数需要具备以下几点性质：</p><ul><li>连续并可导的非线性函数，可导的激活函数可以直接利用数值优化的方法来学习网络参数。</li><li>激活函数及其导函数要尽可能的简单，有利于提高网络计算效率。</li><li>激活函数的导函数的值域要在一个合适的区间内，不能太大也不能太小，否则会影响训练的效率和温度性。</li></ul><h2 id="Sigmoid型激活函数"><a href="#Sigmoid型激活函数" class="headerlink" title="Sigmoid型激活函数"></a>Sigmoid型激活函数</h2><p>S型曲线函数，常见的Sigmoid函数有Logistic函数和tanh函数。</p><blockquote><p>知识点：对于函数f(x)，若x趋向于负无穷大，其导数f’(x)趋向于0，则称其为左饱和。若x趋向于正无穷大，其导数f’(x)趋向于0，则称其为右饱和。同时满足左右饱和时，称为两端饱和。</p></blockquote><ul><li>Logistic 函数<script type="math/tex; mode=display">\sigma (x) = \frac{1} { 1+ exp(-x)}</script></li><li>tanh函数</li></ul><script type="math/tex; mode=display">tanh(x) = \frac{ exp(x)-exp(-x) }{ exp(x) + exp(-x) }</script><p>tanh函数可以看作是放大并平移的Logistic函数，其值域是(-1，1)。</p><script type="math/tex; mode=display">tanh(x) = 2 \sigma(2x) - 1</script><p>tanh函数的输出是零中心化的（Zero-Centered），而Logistic函数的输出值恒大于0。非零中心化的输出会使得最后一层的神经元的输入发生位置偏移（Bias Shift），并进一步使得梯度下降的收敛速度变慢。</p><p><img src="https://img-blog.csdnimg.cn/2019042122140374.jpg" alt="image"></p><h2 id="修正线性单元"><a href="#修正线性单元" class="headerlink" title="修正线性单元"></a>修正线性单元</h2><p>Rectified Linear Unit（ReLU）也叫rectifier函数，是目前深层神经网络中经常使用的激活函数。ReLU实际上是一个斜坡函数，定义为：</p><script type="math/tex; mode=display">ReLU(x) = \begin{cases}x & \text{ if } x \geq 0 \\ 0 & \text{ if } x < 0\end{cases}= max(0,x)</script><hr><p>ReLU的优缺点：</p><ul><li>优点<blockquote><p>采用ReLU的神经元只需要进行加，乘，和，比较的操作，计算上更加高效。Sigmoid型激活函数会导致一个非稀疏的神经网络，而ReLU却具有很好的稀疏性，大约50%的神经元会处于激活状态。</p></blockquote></li></ul><blockquote><p>在优化方面，由于Sigmoid型函数的两端饱和，ReLU函数为左饱和函数，且在x&gt;0时导数为1，在一定程度上缓解了神经网络的梯度消失问题，加速梯度下降的收敛速度。</p></blockquote><ul><li>缺点<blockquote><p>ReLU的输出是非零中心化的，给后一层的神经网络引入偏置偏移，会影响梯度下降的效率。此外ReLU神经元在训练时比较容易死亡。在训练时，如果参数在一次不恰当的更新后，第一个隐藏层中的某个ReLU神经元在所有的训练数据上都不能被激活，那么这个神经元自身参数的梯度永远都会是0，在以后的训练过程中永远不能被激活。这种现象称为死亡ReLU问题（Dying ReLU Problem），并且也有kennel会发生在其他隐藏层。</p></blockquote></li></ul><hr><p>在实际使用中，为了避免上述情况，有集中ReLU的变种也会被广泛使用。</p><h3 id="带泄漏的ReLU"><a href="#带泄漏的ReLU" class="headerlink" title="带泄漏的ReLU"></a>带泄漏的ReLU</h3><p>带泄漏的ReLU在输入x&lt;0时，保持一个很小的梯度 lambda。这样当神经元非激活时也能又一个非零的梯度可以更新参数，避免永远不能被激活。带泄漏的ReLU的定义如下：</p><script type="math/tex; mode=display">LeakyReLU(x) = \begin{cases}x & \text{ if } x > 0 \\ \gamma x & \text{ if } x  \leq 0\end{cases}= max(0,x) + \gamma min(0,x)</script><p>其中 gamma是一个很小的常数，比如0.01。当gamma &lt; 1时，带泄漏的ReLU也可以写为：</p><script type="math/tex; mode=display">LeakyReLU(x) = max(x, \gamma x)</script><p>相当于是一个比较简单的maxout单元。</p><h3 id="带参数的ReLU"><a href="#带参数的ReLU" class="headerlink" title="带参数的ReLU"></a>带参数的ReLU</h3><p>带参数的ReLU引入一个可学习的参数，不同神经元可以有不同的参数，对于第i个神经元，其PReLU的定义为：</p><script type="math/tex; mode=display">PReLU(x) = \begin{cases}x & \text{ if } x > 0 \\ \gamma _ix & \text{ if } x  \leq 0\end{cases}= max(0,x) + \gamma_imin(0,x)</script><p>其中γi为x≤0时函数的斜率。因此，PReLU是非饱和函数。如果γi =0，那 么 PReLU 就退化为 ReLU。如果 γi 为一个很小的常数，则 PReLU 可以看作带 泄露的 ReLU。PReLU 可以允许不同神经元具有不同的参数，也可以一组神经 元共享一个参数。</p><hr><h3 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h3><p>指数线性单元（Exponential Linear Unit）是一个近似的零中心化的非线性函数，其定义为：</p><script type="math/tex; mode=display">ELU(x) = \begin{cases}x & \text{ if } x > 0 \\ \gamma (exp(x)-1) & \text{ if } x  \leq 0\end{cases}= max(0,x) + min(0,\gamma(exp(x)-1))</script><p>其中 γ ≥ 0是一个超参数，决定x ≤ 0时的饱和曲线，并调整输出均值在0附<br>近。</p><hr><h4 id="Softplus函数"><a href="#Softplus函数" class="headerlink" title="Softplus函数"></a>Softplus函数</h4><p>Softplus函数可以看作是rectifier函数的平滑版本，其定义为：</p><script type="math/tex; mode=display">Softplus(x) = log(1 + exp(x))</script><p>Softplus函数及其导数刚好是Logistic函数。Softplus函数虽然也具有单侧抑制，宽兴奋边界的特征，却没有稀疏激活性。</p><p>下图为几种激活函数的示例：</p><p><img src="https://img-blog.csdnimg.cn/20190421213409642.jpg" alt="激活函数对比"></p><h2 id="Swish函数"><a href="#Swish函数" class="headerlink" title="Swish函数"></a>Swish函数</h2><p>Swish函数是一种自门控（self-Gated）激活函数，其定义为：</p><script type="math/tex; mode=display">swish(x) = x \sigma (\beta x)</script><p>其中 sigma(.)为logistic函数，beta为可学习的参数或一个固定超参数。 sigma(.) 属于 (0,1)可以看做是一种软性的门控机构。当sigma(beta x)接近于1时，门处于“开”状态，激活函数的输出近似于x本身；当sigma(beta x)接近于0时，门的状态为“关”，激活函数的输出近似于0。</p><p>下图为Swish函数的示例：<br><img src="https://img-blog.csdnimg.cn/20190421214743755.jpg" alt="image"></p><p>当 beta=0时，Swish函数变成线性函数 x/2。 当 beta=1时，Swish 函数在 x&gt;0时近似线性，在x &lt; 0时近似饱和，同时具有一定的非单调性。当beta趋向于正无穷大时， sigma(beta x)趋向于离散的0-1函数，Switch函数近似为ReLU函数。</p><p>因此Swish函数可以看作时线性函数和ReLU函数之间的非线性插值函数，其程度由参数beta控制。</p><hr><h3 id="Maxout单元"><a href="#Maxout单元" class="headerlink" title="Maxout单元"></a>Maxout单元</h3><p>Maxout单元也是一种分段线性函数。Sigmoid型函数， ReLU等激活函数的输入是神经元的净输入z，是一个标量。而maxout单元的输入是上一层神经元的全部原始输入，是一个向量x=[x1;x2;…;x_d]。</p><p>每个maxout单元有K个权重向量w_k 属于 R^d 和偏置 b_k（1 &lt;= k &lt;= K）。对于输入x，可以得到K个净输入z_k，1 &lt;= k &lt;=K。</p><script type="math/tex; mode=display">z_k = w^T_kx + b_k</script><p>其中</p><script type="math/tex; mode=display">w_k = [w_{k,1},w_{k,2},...,w_{k,d}]^T</script><p>为第k个权重向量。<br>Maxout单元的非线性函数定义为：</p><script type="math/tex; mode=display">maxout(x) = \underset{k\in [1,K]}{max} (z_k)</script><p>Maxout单元不单是净输入到输出之间的非线性映射，而是整体学习输入到输出之间的非线性映射关系。Maxout激活函数可以看作任意凸函数的分段线性近似，并且在有限的点上是不可微的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="深度学习" scheme="http://thinkgamer.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="TensorFlow" scheme="http://thinkgamer.cn/tags/TensorFlow/"/>
    
      <category term="神经网络" scheme="http://thinkgamer.cn/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>线性模型篇之SVM数学公式推导</title>
    <link href="http://thinkgamer.cn/2019/04/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%AF%87%E4%B9%8BSVM%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/"/>
    <id>http://thinkgamer.cn/2019/04/21/机器学习/线性模型篇之SVM数学公式推导/</id>
    <published>2019-04-20T16:17:00.000Z</published>
    <updated>2019-04-21T07:32:08.985Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><blockquote><p>支持向量机（Support Vector Machine，SVM）是一个经典两类分类算法，其找到的分割超平面具有更好的鲁棒性，因此广泛使用在很多任务上，并表现出了很强优势。</p></blockquote><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>给定一个两分类数据集D={(x^n, y^n)}，n属于N，其中y_n 属于{+1,-1}，如果两类样本是线性可分的，即存在一个超平面（公式-1）</p><script type="math/tex; mode=display">w^Tx + b =0</script><p>将两类样本分开，那么对于每个样本都有</p><script type="math/tex; mode=display">y^n(w^Tx^n + b) > 0</script><p>数据集D中的每个样本x^n 到分隔超平面的距离为：</p><script type="math/tex; mode=display">\gamma ^n = \frac{\left \| w^Tx^n +b \right \|}{ \left \| w \right \|} = \frac{y^n(w^Tx^n + b)}{ \left \| w \right \| }</script><p>我们定义整个数据集D中所有样本到分隔超平面的最短距离为间隔（Margin）（公式-2）</p><script type="math/tex; mode=display">\gamma = \underset{n}{min} \gamma ^ n</script><p>如果间隔 gamma越大，其分隔超平面对两个数据集的划分越稳定，不容易受噪声等因素影响，支持向量机的目的是找到一个超平面(w^<em> , b^ </em>)使得gamma最大，即（公式-3）</p><script type="math/tex; mode=display">\underset{w,b}{max} \qquad \gamma \\s.t.  \qquad  \frac{y^n (w^Tx^n + b)}{\left \| w \right \|} \geq \gamma,\forall_n</script><p>令 </p><script type="math/tex; mode=display">\left \| w \right \| . \gamma =1</script><p>则（公式-3）等价于（公式-4）</p><script type="math/tex; mode=display">\underset{w,b}{max} \qquad \frac{1}{ \left \| w \right \| ^2} \\s.t.  \qquad  y^n(w^Tx^n + b) \geq 1, \forall_n</script><p>数据集中所有满足 y^n (w^T x^n +b) =1 的样本点，都称为支持向量（support vertor）</p><p>对于一个线性可分数据集，其分隔的超平面有多个，但是间隔最大的超平面是唯一的。下图给定了支持最大间隔分隔超平面的示例，其红色样本点为支持向量。</p><center>![支持向量机示例](https://img-blog.csdnimg.cn/20190417110614374.png)</center><h1 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h1><p><strong>凸函数 &amp; 凹函数</strong><br>关于凹凸函数的定义和性质可以参考下图：</p><center>![image](https://img-blog.csdnimg.cn/20190420165249483.jpg)</center><p>为了找到最大间隔分割超平面，将公式-4改写为凸优化问题（公式-5）：</p><script type="math/tex; mode=display">\underset{w,b}{min} \qquad \frac{1}{ 2} {\left \| w \right \| ^2} \\s.t.  \qquad  1-y^n(w^Tx^n + b) \leq 0, \forall n</script><p>使用拉格朗日乘数法，公式-5的拉格朗日函数为（公式-6）：</p><script type="math/tex; mode=display">\Lambda (w,b,\lambda )=\frac{1}{2} \left \| w^2 \right \| + \sum_{n-1}^{N} \lambda _n( 1-y^n(w^Tx^n + b) )</script><p>其中</p><script type="math/tex; mode=display">\lambda _1 \geq 0,...,\lambda _N \geq 0</script><p>为拉格朗日乘数。计算公式-6关于w和b的导数，并令其等于0得到（公式-7）</p><script type="math/tex; mode=display">w = \sum_{n=1}^{ N }\lambda _n y^nx^n</script><p>和（公式-8）</p><script type="math/tex; mode=display">0 = \sum_{n=1}^{N} \lambda _n y^n</script><p>将公式-7代入公式-6，并利用公式-8，得到拉格朗日对偶函数（公式-9）：</p><script type="math/tex; mode=display">\Gamma(\lambda) = -\frac{1}{2} \sum_{n=1}^{N}\sum_{m=1}^{N} \lambda_n \lambda_m y^m y^n (x^m)^Tx^n + \sum_{n=1}^{N}\lambda_n</script><p>支持向量机的主优化问题为凸优化问题，满足强对偶性，即主优化问题可以通过最大化对偶函数</p><script type="math/tex; mode=display">max_{\lambda \geq 0} \Gamma(\lambda)</script><p>对偶函数 Gamma(lambda)是一个凹函数，因此最大化对偶数是一个凸优化问题，可以通过多种凸优化方法进行求解，得到拉格朗日乘数的最优值 lambda^* 。但由于其约束条件的数量为训练样本数量，一般的优化方法代价比较高，因此在实践中通常采样比较高效的优化方法，比如SMO(Sequential Minimal Optimization)算法等。</p><p>根据KKT条件中的互补松弛条件，最优解满足(公式-10)</p><script type="math/tex; mode=display"> \lambda_n ^*(1-y^n(w^{*T}x^n+b^*))=0</script><p>如果样本x^n 不在约束边界上，(lambda_n)^<em>，其约束失效；如果样本x^n在约束边界上，(lambda_n)^</em> &gt;=0。这些在约束边界上的样本点称为支持向量（support vector），即离决策平面距离最近的点。</p><p>再计算出 lambda^<em>后，根据公式-7计算出最优权重w^</em>，最优偏置b^*可以通过任选一个支持向量(x,y)计算得到（公式-11）</p><script type="math/tex; mode=display">b^* = \tilde{y} - w^{*T}\tilde{x}</script><p>最优参数的支持向量机的决策函数为（公式-12）</p><script type="math/tex; mode=display">f(x)=sgn(w^{*T}x+b^*)=sgn(\sum_{n=1}^{N} \lambda_n^* y^n(x^n)^Tx + b^* )</script><p>支持向量机的决策函数只依赖 lambda_n^*&gt;0的样本点，即支持向量。</p><p>支持向量机的目标函数可以通过SMO等优化方法得到全局最优解，因此比其他分类器的学习效率更高。此外，支持向量机的决策函数只依赖与支持向量。与训练样本总数无关，分类速度比较快。</p><h1 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h1><p>支持向量机还有一个重要的优点是可以使用核函数(kernal)隐式的将样本从原始特征空间映射到更高维的空间，并解决原始特征空间中的线性不可分问题。比如在一个变换后的特征空间中，支持向量机的决策函数为（公式-13）</p><script type="math/tex; mode=display">f(x)=sgn(w^{*T} \phi(x)+b^*)=sgn(\sum_{n=1}^{N} \lambda_n^* y^n K(x^n,x) + b^* )</script><p>其中</p><script type="math/tex; mode=display">K(x,z)=\phi(x)^T \phi(z)</script><p>为核函数，通常不需要显式的给出φ(x)的具体形式，可以通过核技巧(kernel trick)来构造。比如以x,z属于R^2为例，我们可以构造一个核函数（公式-14）</p><script type="math/tex; mode=display">K(x,z)=(1+x^Tz)^2=\phi(x)^T\phi(z)</script><p>来隐式的计算x,z在特征空间φ中的内积，其中:</p><script type="math/tex; mode=display">\phi(x)=[1,\sqrt{2}x_1,\sqrt{2}x_2,\sqrt{2}x_1x_2,x_1^2,x_2^2]^T</script><h1 id="软间隔"><a href="#软间隔" class="headerlink" title="软间隔"></a>软间隔</h1><p>在支持向量机的优化问题中，约束条件比较严格。如果训练集中的样本在特征空间中不是线性可分的，就无法找到最优解。为了能够容忍部分不满足约束的样本，我们可以引入松弛变量，将优化问题变为（公式-15）：</p><script type="math/tex; mode=display">\underset{w,b}{min} \qquad \frac{1}{ 2} {\left \| w \right \| ^2} + C \sum_{n=1}^{N}\xi _n\\s.t.  \qquad  1-y^n(w^Tx^n + b) -\xi _n \leq 0, \forall n\\\xi _n \geq 0, \forall n</script><p>其中参数C&gt;0用来控制间隔和松弛变量惩罚的平衡，引入松弛变量的间隔称为软间隔（soft margin）。公式-15也可以表示为经验风险+正则化项的形式（公式-16）。</p><script type="math/tex; mode=display">\underset{w,b}{min} \qquad \sum_{n=1}^{N}max(0,1-y^n(w^Tx^n + b)) + \frac{1}{C}.\frac{1}{2}\left \| w \right \|^2</script><p>其中</p><script type="math/tex; mode=display">max(0,1-y^n(w^Tx^n + b))</script><p>称为hinge损失函数，1/C可以看作是正则化系数。软间隔支持向量机的参数学习和原始支持向量机类似，其最终决策函数也只和支持向量有关，即满足</p><script type="math/tex; mode=display">1-y^n(w^Tx^n + b) - \xi_n = 0</script><p>的样本。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="线性模型" scheme="http://thinkgamer.cn/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="SVM" scheme="http://thinkgamer.cn/tags/SVM/"/>
    
      <category term="向量机" scheme="http://thinkgamer.cn/tags/%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>线性模型之PLA数学公式推导</title>
    <link href="http://thinkgamer.cn/2019/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E4%B9%8BPLA%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/"/>
    <id>http://thinkgamer.cn/2019/04/16/机器学习/线性模型之PLA数学公式推导/</id>
    <published>2019-04-16T11:13:32.000Z</published>
    <updated>2019-04-16T23:09:04.145Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>感知机（Perceptron）是一种广泛使用的线性分类器，相当于最简单的人工神经网络，只有一个神经元。其全称是PLA（Perceptron Linear Algorithm），线性感知机算法。</p><p>感知机是对生物神经元的简单数学模型，有与生物神经元相对应的部件，比如权重（突触）、偏置（阈值）及激活函数（细胞体），输出值为 +1 或者 -1。</p><script type="math/tex; mode=display">\hat{y} = sgn(w^Tx)</script><p>对于二分类问题，可以使用感知机算法来解决。PLA的原理是逐点解决，首先在超平面上随意取一条分类面，统计分类错误的点，然后随机对某个错误点修正，即变换直线的位置，使该错误点被修正，接着再随机选取另外一个错误点进行修正，分类面不断变化，直到所有点都分类正确了，就得到了最佳分类面。</p><p>利用二维平面进行解释，第一种情况是错误的将正样本（y=1）分类为负样本（y=-1）。此时wx&lt;0，即w与x的夹角大于90度，分类线L的两侧。修正的方法是让夹角变小，修正w值，使二者位于直线同侧。</p><script type="math/tex; mode=display">w:=w+x=w+yx</script><p>修正过程如下：<br><img src="https://img-blog.csdnimg.cn/20190415193607475.jpg" alt="修正过程"></p><p>第二种情况就是错误的将负样本（y=-1）分类为正样本（y=1）。此时，wx&gt;0，即w与x的夹角小于90度，分类线L的同一侧。修正的方法是让夹角变大，修正w值，使二者位于分类线同侧。</p><script type="math/tex; mode=display">w:=w-x=w+yx</script><p>修正过程如下：<br><img src="https://img-blog.csdnimg.cn/20190415193855372.jpg" alt="修正过程"></p><p>经过上边两种情况分析，PLA每次更新参数w的表达式是一致的，掌握了每次w的优化表达式，那么PLA就能不断地将所有错误的分类样本纠正并分类正确。</p><h1 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h1><p>给定N个训练集样本{(x^n, y^n)},n&lt;=N，其中y^n 属于{+1,-1}，感知机试图学习到参数w*，使得对于每个样本(x^n,y^n)<br>有：</p><script type="math/tex; mode=display">y^n w^{*T}x^n>0,\forall n\in [1,N]</script><p>感知机算法是一种错误驱动的在线学习算法，先初始化一个权重向量w&lt;-0（通常是全零向量），然后每次分错一个样本（x,y）时，就用这个样本来更新权重。</p><script type="math/tex; mode=display">w \leftarrow w + yx</script><p>具体的感知机算法伪代码如下（==算法-1==）：<br><img src="https://img-blog.csdnimg.cn/20190415202606770.png" alt="感知机算法伪代码"></p><p>根据感知器的学习策略，可以反推出感知器的损失函数为：</p><script type="math/tex; mode=display">L (w;x,y)=max(0, -yw^Tx)</script><p>采用随机梯度的下降，其每次更新的梯度为：</p><script type="math/tex; mode=display">\frac{\partial L (w;x,y) }{ \partial w}=\begin{cases}0 & \text{ if } y^Tx>0 \\ -yx & \text{ if } y^Tx<0 \end{cases}</script><p>下图给出了感知机参数学习的过程，其中红色实心为正例，蓝色空心点为负例。黑色箭头表示权重向量，红色虚线箭头表示权重的更新方向。</p><p><img src="https://img-blog.csdnimg.cn/20190415203526997.png" alt="感知机参数学习的过程"></p><h1 id="感知机的收敛"><a href="#感知机的收敛" class="headerlink" title="感知机的收敛"></a>感知机的收敛</h1><p>Novikoff证明对于两类问题，如果训练集是线性可分的，那么感知器<br>算法可以在有限次迭代后收敛。然而，如果训练集不是线性分隔的，那么这个算法则不能确保会收敛。</p><p>当数据集是两类线性可分时，对于数据集D={(x^n,y^n)}，n属于N，其中x^n为样本的增广特征向量，y^n属于{-1，+1}，那么存在一个正的常数r(r&gt;0)和权重向量w，并且||w<em>||=1，对所有n都满足(w^</em>)(y^n x^n)&gt;r。</p><p>可以证明如下定理（定理-1）。</p><hr><p>给定一个训练集</p><script type="math/tex; mode=display">D={(x^n,y^n)},n\in {1,N}</script><p>假设R是训练集中最大的特征向量的模</p><script type="math/tex; mode=display">R=\underset{n}{max} \left \| x^n \right \|</script><p>如果训练集D线性可分，感知机学习算法-1的权重更新次数不超过 R^2/ r^2</p><hr><p>证明：<br>感知机算法的权重更新方式为（==公式-1==）：</p><script type="math/tex; mode=display">w_k = w_{k-1}+y^kx^k</script><p>其中x^k, y^k表示第k个错误分类的条件。<br>因为初始权重为0，在第K次更新时感知器的权重向量为（==公式-2==）：</p><script type="math/tex; mode=display">w_k = =\sum_{k=1}^{K}y^kx^k</script><p>分别计算||w||^2的上下界：</p><p><strong>计算其上界</strong>（公式-2）：</p><script type="math/tex; mode=display">\left \| w_k^2 \right \|</script><script type="math/tex; mode=display">= \left\| w_{K_1} + y^K x^K \right \|^2</script><script type="math/tex; mode=display">= \left \|  w_{K-1} \right \| ^2 + \left \| y^Kx^K \right \| ^2+2y^Kw_{K-1}x^K</script><script type="math/tex; mode=display">\leqslant \left \| w_{K-1} \right \|^2 + R^2</script><script type="math/tex; mode=display">\leqslant\left \| w_{K-2} \right \|^2+2R^2</script><script type="math/tex; mode=display">\leqslant KR^2</script><p><strong>计算其下界</strong>（公式-3）：</p><script type="math/tex; mode=display">\left \| w_k^2 \right \|</script><script type="math/tex; mode=display">=\left \| w^* \right \|^2 .\left \| w_K \right \|^2</script><script type="math/tex; mode=display">\geqslant \left \| w^{*T}w_K \right \| ^2</script><script type="math/tex; mode=display">=\left \| w^{*T}\sum_{k=1}^{K}(y^Kx^K) \right \|^2</script><script type="math/tex; mode=display">=\left \| \sum_{k=1}^{K}w^{*T}(y^Kx^K) \right \|^2</script><script type="math/tex; mode=display">\geq K^2r^2</script><p>==附==：两个向量内积的平方一定小于等于这两个向量的模的乘积。</p><p>由公式-2和公式-3得到（公式-4）</p><script type="math/tex; mode=display">K^2r^2 \leq \left \| w_K \right \|^2\leq KR^2</script><p>取最左和最右的两项，进一步得到K^2r^2 &lt;= K^2R^2，然后两边同时除以K，最终得到（公式-5）：</p><script type="math/tex; mode=display">K\leq \frac{R^2}{r^2}</script><p>因此在线性可分的情况下，算法-1会在R^2 / r^2步内收敛。</p><p>虽然感知机线性模型在线性可分的数据上可以保证收敛，但其存在以下不足：</p><ul><li>在数据集线性可分时，感知器虽然可以找到一个超平面把两类数据分开， 但并不能保证能其泛化能力。</li><li>感知器对样本顺序比较敏感。每次迭代的顺序不一致时，找到的分割超平 面也往往不一致。</li><li>如果训练集不是线性可分的，就永远不会收敛。</li></ul><h1 id="参数平均感知机"><a href="#参数平均感知机" class="headerlink" title="参数平均感知机"></a>参数平均感知机</h1><p>根据定理3.1，如果训练数据是线性可分的，那么感知器可以找到一个判别 函数来分割不同类的数据。如果间隔 γ 越大，收敛越快。但是感知器并不能保 证找到的判别函数是最优的(比如泛化能力高)，这样可能导致过拟合。</p><p>感知机的学习到的权重向量和训练样本的顺序相关。在迭代次序上排在后 面的错误样本，比前面的错误样本对最终的权重向量影响更大。比如有 1, 000 个 训练样本，在迭代 100 个样本后，感知器已经学习到一个很好的权重向量。在 接下来的 899 个样本上都预测正确，也没有更新权重向量。但是在最后第 1, 000 个样本时预测错误，并更新了权重。这次更新可能反而使得权重向量变差。</p><p>为了改善这种情况，可以使用“参数平均”的策略提高感知ji的鲁棒性，也叫投票感知机。</p><p>投票感知机记录第k次更新参数之后的权重w_k在之后的训练过程中正确分类样本的次数c_k。这样最后的分类器形式为（公式-6）：</p><script type="math/tex; mode=display">\hat{y} = sgn(\sum_{k=1}^{K}c_ksgn(w_k^Tx))</script><p>其中sgn(.)为符号函数。</p><p>投票感知机虽然提高了模型的泛化能力，但是需要保存K个权重向量。在实际的操作中会带来额外的开销。因此经常会使用一个简化的版本，即平均感知机。其表达式如下（公式-7）：</p><script type="math/tex; mode=display">\hat{y} = sgn(\sum_{k=1}^{K}c_k (w_k^Tx))=sgn( (\sum_{k=1}^{K}c_k w_k )^Tx )=sgn( \bar{w}^Tx)</script><p>其中 </p><script type="math/tex; mode=display">\bar{w}</script><p>为平均的权重向量。</p><p>假设w_{t,n}是在第t轮更新到第n个样本时的权重向量值，平均的权重向量也可以表示为（公式-8）：</p><script type="math/tex; mode=display">\bar{w} = \frac{\sum_{t=1}^{T} \sum_{n=1}^{N}w_{t,n}}{nT}</script><p>这个方法实现简单，只需要在算法-1中增加一个平均向量，并且在处理每一个样本后，进行更新（公式-9）：</p><script type="math/tex; mode=display">\bar{w} = \bar{w}+ w_{t,n}</script><p>但这个方法需要在处理每一个样本时都要更新平均权重，因为</p><script type="math/tex; mode=display"> \bar{w} ,w_{t,n}</script><p>都是稠密向量，因此更新操作比较费时。为了提高迭代速度，有很多改进的办法，让这个更新只需要在错误预测时才进行。下图给了一个改进的平均感知机算法的训练过程（算法-2）。<br><img src="https://img-blog.csdnimg.cn/20190416184043264.png" alt="改进的平均感知机算法的训练过程"></p><h1 id="扩展到多分类"><a href="#扩展到多分类" class="headerlink" title="扩展到多分类"></a>扩展到多分类</h1><p>原始的感知机是二分类模型，但也很容易的扩展到多分类，甚至是更一般的结构化学习问题。</p><p>之前介绍的线性分类模型中，分类函数都是在输入x的特征空间上。为了使得感知机可以处理更加复杂的输出，我们引入一个构建输入输出联合空间上的特征函数，将样本(x,y)对映射到一个特征向量空间。</p><p>在联合特征空间中，我们可以建立一个广义的感知机模型（公式-10）：</p><script type="math/tex; mode=display">\hat{y} = \underset{y\in Gen(x)}{ arg max} w^T \phi(x,y)</script><p>其中w为权重向量，Gen(x)表示输入x所有的输出目标集合。当处理C类分类问题时，Gen(x)={1,….,C}</p><p>在C分类中，一种常用的特征函数（公式-11）</p><script type="math/tex; mode=display">\phi(x,y)</script><p>是y和x的内积，其中y为类别的one-hot向量表示（公式-12）。</p><script type="math/tex; mode=display"> \phi(x,y) = vec(yx^T)\in R^{(d\times C)}</script><p>其中vec是向量化算子。</p><p>给定样本(x,y)，若</p><script type="math/tex; mode=display">x \in R^d</script><p>y为第c维为1的one-hot向量，则：<br><img src="https://img-blog.csdnimg.cn/20190416190108143.png" alt="y为第c维为1的one-hot向量"></p><p>广义感知器算法的训练过程如算法-3所示：</p><p><img src="https://img-blog.csdnimg.cn/20190416190225604.png" alt="广义感知器算法的训练过程"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="线性模型" scheme="http://thinkgamer.cn/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="Logistic Regression" scheme="http://thinkgamer.cn/tags/Logistic-Regression/"/>
    
      <category term="PLA" scheme="http://thinkgamer.cn/tags/PLA/"/>
    
      <category term="感知机" scheme="http://thinkgamer.cn/tags/%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    
  </entry>
  
  <entry>
    <title>线性模型篇之softmax数学公式推导</title>
    <link href="http://thinkgamer.cn/2019/04/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%AF%87%E4%B9%8Bsoftmax%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/"/>
    <id>http://thinkgamer.cn/2019/04/07/机器学习/线性模型篇之softmax数学公式推导/</id>
    <published>2019-04-06T23:24:46.000Z</published>
    <updated>2019-04-13T06:01:56.575Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><blockquote><p>Softmax回归也称多项（multinomial）或者多类（multi-class）的Logistic回归，是Logistic回归在多类分类问题上的推广。和逻辑回归一样属于线性模型。</p></blockquote><h1 id="SoftMax回归简介"><a href="#SoftMax回归简介" class="headerlink" title="SoftMax回归简介"></a>SoftMax回归简介</h1><p>对于多类问题，类别标签</p><script type="math/tex; mode=display">y \in {1,2,3,...,C}</script><p>可以用C个取值，给定一个样本x，softmax回归预测的是属于类别c的概率为(公式-1)：</p><script type="math/tex; mode=display">p(y=c|x)=softmax(w_c^Tx)=\frac{exp(w_c^Tx)}{\sum_{c=1}^{C}exp(w_c^Tx)}</script><p>其中w_c是第c类的权重向量。</p><p>softmax回归的决策函数可以表示为(公式-2)：</p><script type="math/tex; mode=display">\hat{y}=  \underset{c=1}{ \overset{C}{arg max} } \ p(y=c|x) =\underset{c=1}{ \overset{C}{arg max} } \ w_c^T x</script><hr><p>softMax与Logistic回归的关系：</p><p>当类别个C=2时，softMax回归的决策函数为(公式-3)：</p><script type="math/tex; mode=display">\hat{y} = \underset{y\in {0,1}}{ arg max } \ w_y^Tx=I(w_1^Tx - w_0^Tx >0 )=I((w_1 - w_0)^Tx >0 )</script><p>其中I(.)是指示函数，对比二分类决策函数(公式-4)</p><script type="math/tex; mode=display">g(f(x,w))=sgn(f(x,w))=\begin{cases} & +1 \text{ if } f(x,w)>0 \\  & -1 \text{ if } f(x,w)<0 \end{cases}</script><p>其中sgn表示符号函数(sign function)，可以发现两类分类中的权重向量w=w1-w0</p><hr><p>向量表示：</p><p>公式-1用向量形式可以写为(公式-5)</p><script type="math/tex; mode=display">\hat{y}=softmax(W^Tx)=\frac{erp(W^Tx)}{1^Texp(W^Tx)}</script><p>其中W=[w_1,w_2,…,w_C]是由C个类的权重向量组成的矩阵，1为全1的向量，</p><script type="math/tex; mode=display">\hat{y}\in  R^C</script><p>为所有类别的预测条件概率组成的向量，第c维的值是第c类的预测条件概率。</p><h1 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h1><p>给定N个训练样本{(x^n, y^n)},n&lt;=N，softmax回归使用交叉熵损失函数来学习最优的参数矩阵W。</p><p>这里用C维的one-hot向量</p><script type="math/tex; mode=display">y \in {0,1} ^C</script><p>来表示类别标签，其向量表示为(公式-6)：</p><script type="math/tex; mode=display">y = [I(1=c),I(2=c),...,I(C=c)]^T</script><p>其中I(.)为指示函数。</p><p>采用交叉熵损失函数，softmax的经验风险函数为(公式-7)：</p><script type="math/tex; mode=display">R(W)=-\frac{1}{N}\sum_{n=1}^{N}\sum_{c=1}^{C}y_c^nlog\hat{y}_c^nR(W)=-\frac{1}{N}\sum_{n=1}^{N} (y^n)^Tlog\hat{y}^n</script><p>其中</p><script type="math/tex; mode=display">\hat{y}^n = softmax(W^Tx^n)</script><p>为样本x^n在每个类别的后验概率。</p><p>==说明：公式-7第一个式变换到第二个式是因为y_c类别中只有一个为1，其余为0，所以将第二个求和去除。==</p><p>风险函数R(W)关于W的梯度为(公式-8)：</p><script type="math/tex; mode=display">\frac{\partial R(W)}{\partial W} = -\frac{1}{N}\sum_{n=1}^{N}x^n(y^n-\hat{y}^n)^T</script><p>==<strong>证明：</strong>==</p><p>计算公式-8中的梯度，关键在于计算每个样本的损失函数</p><script type="math/tex; mode=display">L^n(W)=-(y^n)^Tlog\hat{y}^n</script><p>关于参数W的梯度，其中需要用到两个导数公式为：</p><ul><li>若y=softmax(z)，则</li></ul><script type="math/tex; mode=display">\frac{\partial y}{\partial z}=diag(y)-yy^T</script><ul><li>若</li></ul><script type="math/tex; mode=display">z=W^Tx=[w_1^Tx,w_2^Tx,...,w_C^Tx]^T</script><p>则</p><script type="math/tex; mode=display">\frac{\partial y}{\partial w_c}</script><p>为第c列为x，其余为0的矩阵。</p><script type="math/tex; mode=display">\frac{\partial z}{\partial w_c} = [ \frac{\partial w_1^Tx}{\partial w_c},\frac{\partial w_2^Tx}{\partial w_c},...,\frac{\partial w_C^Tx}{\partial w_c} ]=[0,0,..,x,...,0]=M_c(x)</script><p>根据链式法则，</p><script type="math/tex; mode=display">L^n(W) = -(y^n)^T log\hat{y}^n</script><p>关于w_c的偏导数为(公式-12)：</p><script type="math/tex; mode=display">\frac{\partial L^n(W) }{\partial w_c}= -\frac{ \partial ((y^n)^T log \hat{y}^n) }{\partial w_c}</script><script type="math/tex; mode=display">= -\frac{\partial z^n}{ \partial w_c } \frac{\partial \hat{y}^n}{ \partial z^n }\frac{\partial log \hat{y}^n}{ \partial \hat{y}^n } y^n</script><script type="math/tex; mode=display">=-M_c(x^n)(diag(\hat{y}^n)-\hat{y}^n(\hat{y}^n)^T)(diag(\hat{y}^n))^{-1} y^n</script><script type="math/tex; mode=display">=-M_c(x^n)(I-\hat{y}^n1^T)y^n</script><script type="math/tex; mode=display">=-M_c(x^n)(y^n-\hat{y}^n1^Ty^n)</script><script type="math/tex; mode=display">=-M_c(x^n)(y^n-\hat{y}^n)</script><script type="math/tex; mode=display">=-x^n[y^n-\hat{y}^n]_c</script><p>公式-12也可以表示为非向量形式(公式-13)：</p><script type="math/tex; mode=display">\frac{\partial L^n(W) }{\partial w_c}= -x^n(I(y^n=c)-\hat{y}_c^n)</script><p>其中I(.)为指示函数，根据公式-12可以得到(公式-14)</p><script type="math/tex; mode=display">\frac{\partial L^n(W) }{\partial W} = -x^n(y^n-\hat{y}^n)^T</script><p>采用梯度下降法，softmax回归的训练过程为：初始化W_0 &lt;- 0，然后通过下式进行迭代更新。</p><script type="math/tex; mode=display">W_{t+1} = W_t + \alpha (\frac{1}{N} \sum_{n=1}^{N}x^n(y^n - \hat{y}_{W_t} ^ n)^T)</script><p>其中a是学习率，</p><script type="math/tex; mode=display">\hat{y}_{W_t}^n</script><p>是当参数为W_t时，softmax回归模型的输出。</p><hr><p><strong>注意：</strong></p><blockquote><p>softmax回归中使用的C个权重向量是冗余的，即对所有权重向量都减去一个同样的向量v，不改变其输出结果。因此，softmax往往需要正则化来约束参数。此外，可以利用这个特性来避免计算softmax函数时在数值计算上溢出问题。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="线性模型" scheme="http://thinkgamer.cn/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="SoftMax" scheme="http://thinkgamer.cn/tags/SoftMax/"/>
    
  </entry>
  
  <entry>
    <title>线性模型篇之Logistic Regression数学公式推导</title>
    <link href="http://thinkgamer.cn/2019/04/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%AF%87%E4%B9%8BLogistic%20Regression%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/"/>
    <id>http://thinkgamer.cn/2019/04/02/机器学习/线性模型篇之Logistic Regression数学公式推导/</id>
    <published>2019-04-02T14:31:51.000Z</published>
    <updated>2019-04-13T05:09:02.305Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="两分类与多分类"><a href="#两分类与多分类" class="headerlink" title="两分类与多分类"></a>两分类与多分类</h1><ul><li>两类分类（Binary Classification）<ul><li>类别标签y只有两种取值，通常设为{0，1}</li><li>线性判别函数，即形如 y = w^T*x + b</li><li>分割超平面（hyper plane）,由满足f(w,x)=0的点组成</li><li>决策边界（Decision boundary）、决策平面（Decision surface）：即分分割超平面，决策边界将特征空间一分为二，划分成两个区域，每个区域对应一个类别。</li><li>有向距离（signed distance）</li></ul></li><li>多样分类（Multi-class Classification）<ul><li>分类的类别个数大于2，多分类一般需要多个线性判别函数，但设计这些判别函数有很多方式。eg：<ul><li>一对其余：属于和不属于</li><li>一对一</li><li>argmax（改进的一对其余）：属于每个类别的概率，找概率最大值</li></ul></li><li>参考：<a href="https://blog.csdn.net/Gamer_gyt/article/details/86378882" target="_blank" rel="external">多分类实现方式介绍和在Spark上实现多分类逻辑回归</a></li></ul></li></ul><h1 id="Logistic回归"><a href="#Logistic回归" class="headerlink" title="Logistic回归"></a>Logistic回归</h1><h2 id="LR回归"><a href="#LR回归" class="headerlink" title="LR回归"></a>LR回归</h2><p>Logistic回归（Logistic Regression，LR）是一种常见的处理二分类的线性回归模型。</p><p>为了解决连续的线性回归函数不适合做分类的问题，引入函数g：R^d -&gt; (0,1)来预测类别标签的后验概率p(y=1 | x)</p><p>其中g(.)通常称为激活函数（activation function），其作用是把线性函数的值域从实数区间“挤压”到了（0，1）之间，可以用概率表示。在统计文献中，g(.)的逆函数g(.)^-1也称为联系函数（Link Function）</p><p>在逻辑回归中使用Logistic作为激活函数，标签y=1的后验概率为(公式-1)：</p><script type="math/tex; mode=display">p(y=1 | x) = \sigma (w^T x)</script><script type="math/tex; mode=display">p(y=1 | x)= \frac{1}{1+exp(-w^T x)}</script><p>标签 y=0的后验概率为(公式-2)：</p><script type="math/tex; mode=display">p(y=0 | x) =1-p(y=0 | x)</script><script type="math/tex; mode=display">p(y=0 | x)= \frac{exp(-w^T x)}{1+exp(-w^T x)}</script><p>将公式-1进行等价变换，可得(公式-3)：</p><script type="math/tex; mode=display">w^T x = log \frac{p(y=1 | x)}{1-p(y=1 | x)}</script><script type="math/tex; mode=display">w^T x = log \frac { p(y=1 | x)}{p(y=0|x)}</script><p>其中</p><script type="math/tex; mode=display">\frac { p(y=1 | x)}{p(y=0|x)}</script><p>为样本x正反例后验概率的比例，称为几率（odds），几率的对数称为对数几率（log odds或者logit），公式-3中第一个表达式，左边是线性函数，logistic回归可以看做是预测值为“标签的对数几率”的线性回归模型，因为Logistic回归也称为对数几率回归（Logit Regression）。</p><p>附公式-1到公式-3的推导：</p><script type="math/tex; mode=display">p(y=1 | x)= \frac{1}{1+exp(-w^T x)}</script><script type="math/tex; mode=display">=> exp(-w^Tx) = \frac{1-p(y=1 | x)}{p(y=1 | x)}</script><script type="math/tex; mode=display">=> - w^T x = log \frac{1- p(y=1 | x)}{p(y=1 | x)}</script><script type="math/tex; mode=display">=>  w^T x = log (\frac{1- p(y=1 | x)}{p(y=1 | x)})^{-1}</script><script type="math/tex; mode=display">=> w^T x = log \frac{p(y=1 | x)}{1-p(y=1 | x)}</script><script type="math/tex; mode=display">=> w^T x = log \frac{p(y=1 | x)}{p(y=0 | x)}</script><h2 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h2><p>LR回归采用交叉熵作为损失函数，并使用梯度下降法对参数进行优化。给定N个训练样本{x_i,y_i}，i&lt;=N，使用LR对每个样本进行预测，并用输出x_i的标签为1的后验概率，记为y’_i(x)  (公式-4)</p><script type="math/tex; mode=display">y'_i(x) = \sigma(w^Tx_i),i\in N</script><p>由于y_i属于{0，1}，样本{x_i,y_i}的真实概率可以表示为(公式-5)：</p><script type="math/tex; mode=display">p_r(y_i =1 | x_i) = y_i</script><script type="math/tex; mode=display">p_r(y_i =0 | x_i) = 1- y_i</script><p>使用交叉熵损失函数，其风险函数为(公式-6)：</p><script type="math/tex; mode=display">R(w)= - \frac{1}{N}\sum_{n=1}^{N} (p_r(y_i =1 | x_i) log(y_i') + p_r(y_i =0 | x_i) log(1-y_i') )</script><script type="math/tex; mode=display">= - \frac{1}{N}\sum_{n=1}^{N} ( y_i log(y_i') + (1-y_i') log(1-y_i') )</script><p>风险函数R(w)关于参数w的导数为(公式-7)：</p><script type="math/tex; mode=display">\frac{ \partial R(w)}{ \partial w} = - \frac{1}{N}\sum_{n=1}^{N}( y_i \frac{y_i'(1-y_i')}{y_i'}x_i -(1-y_i)\frac{y_i'(1-y_i')}{1-y_i'}x_i  )</script><script type="math/tex; mode=display">= - \frac{1}{N}\sum_{n=1}^{N}( y_i(1-y_i')x_i -(1-y_i)y_i'x_i)</script><script type="math/tex; mode=display">= - \frac{1}{N}\sum_{n=1}^{N}x_i(y_i-y_i')</script><p>采用梯度下降算法，Logistic的回归训练过程为：初始化w_0 为0，然后通过下式来更新迭代参数(公式-8)。</p><script type="math/tex; mode=display">w_{t+1} \leftarrow w_t + \alpha \frac{1}{N}\sum_{n=1}^{N} x_i(y_i-y_{w_t}')</script><p>其中a是学习率，y_{wt}’是当参数为w_t 时，Logistic回归的输出。</p><p>从公式-6可知，风险函数R(w)是关于参数w的连续可导的凸函数，因此除了梯度下降算法外，Logistic还可以使用高阶的优化算法，比如牛顿法来进行优化。</p><p>说明:</p><ul><li>两个未知数相乘求导：<script type="math/tex; mode=display">(ab)' = a'b + ab'</script></li><li>sigmoid函数求导后为：<script type="math/tex; mode=display">\sigma ' = \sigma (1-\sigma )x</script></li></ul><hr><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/44591359" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/44591359</a></li><li><a href="https://blog.csdn.net/wgdzz/article/details/48816307" target="_blank" rel="external">https://blog.csdn.net/wgdzz/article/details/48816307</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="LR" scheme="http://thinkgamer.cn/tags/LR/"/>
    
      <category term="线性模型" scheme="http://thinkgamer.cn/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="Logistic Regression" scheme="http://thinkgamer.cn/tags/Logistic-Regression/"/>
    
  </entry>
  
  <entry>
    <title>机器学习算法分类</title>
    <link href="http://thinkgamer.cn/2019/03/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB/"/>
    <id>http://thinkgamer.cn/2019/03/26/机器学习/机器学习算法分类/</id>
    <published>2019-03-26T09:43:37.000Z</published>
    <updated>2019-04-13T05:10:24.623Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><blockquote><p>机器学习算法可以按照不同的标准进行分类。比如按函数f(X)的不同，机器学习算法可以分为线性模型和非线性模型；按照学习准则的不同，机器学习算法也可以分为统计方法和非统计方法。</p><p>但一般而言，会按照训练样本提供的信息以及反馈方式不同，将机器学习算法分为以下几类，下面将一一细说。</p></blockquote><ul><li>监督学习</li><li>无监督学习</li><li>强化学习</li></ul><h1 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h1><p>如果机器学习的目标是通过建模样本的特征x和标签y之间的关系：</p><script type="math/tex; mode=display">y=f(x,\theta )</script><p>或</p><script type="math/tex; mode=display">p(y|x,\theta)</script><p>并且训练集中的每个样本都有标签，那么这类学习称之为监督学习（Supervised Learning）。根据标签类型的不同，监督学习又可以分为回归和分类两种。</p><h2 id="回归（Regression）"><a href="#回归（Regression）" class="headerlink" title="回归（Regression）"></a>回归（Regression）</h2><p>回归问题中的标签y是连续值（实数或者连续整数）</p><script type="math/tex; mode=display">y=f(x,\theta )</script><p>的输出也是连续值。</p><h2 id="分类（Classification）"><a href="#分类（Classification）" class="headerlink" title="分类（Classification）"></a>分类（Classification）</h2><p>分类问题中的标签y是离散的类别，在分类问题中，学习到的模型也成为分类器（Classifier）。分类问题根据其类别的数量又可以分为二分类（Binary Clssification）和多分类（Mutil-class Classification）。</p><h2 id="机构化学习（Structured-Learning）"><a href="#机构化学习（Structured-Learning）" class="headerlink" title="机构化学习（Structured Learning）"></a>机构化学习（Structured Learning）</h2><p>结构化学习的输出对象是结构化的对象，比如序列、树、图等，由于结构化学习的输出空间比较大，因此我们一般定义一个联合特征空间，将x,y映射为该空间中的联合特征向量（x,y）,预测模型可以写为：</p><script type="math/tex; mode=display">\hat{y}=\underset{y \in Gen(x)}{arg max f(\phi (x,y),\theta )}</script><p>其中gen(x)表示x所有可能的输出目标集合。计算arg max的过程也称为解码（decoding）过程，一般通过动态规划的方法来计算。</p><h1 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h1><p>无监督学习（Unsupervised Learning）是指从不包含目标标签的训练样本中自动学习到一些有价值的信息。典型的无监督学习问题有聚类、密度估计、特征学习、降维等。</p><h1 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h1><p>强化学习（Reinforcement Learning）是一类通过交互来学习的机器学习算法。在强化学习中，智能体根据环境的状态作出一个动作，并得到即时或延时的奖励。智能体在和环境的交互中不断学习并调整策略，以取得最大化的期望总回报。</p><p>下表给出了三种机器学习类型</p><p><img src="https://img-blog.csdnimg.cn/20190326174240856.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>监督学习需要每个样本都有标签，而无监督学习则不需要标签。一般而言，监督学习通常大量的有标签数据集，这些数据集是一般都需要由人工进行标注，成本很高。因此，也出现了很多弱监督学习（Weak Supervised Learning）和半监督学习（Semi-Supervised Learning）的方法，希望从大规模的无标注数据中充分挖掘有用的信息，降低对标注样本数量的要求。强化学习和监督学习的不同在于强化学习不需要显式地以“输入/输出对”的方式给出训练样本，是一种在线的学习机制。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="机器学习" scheme="http://thinkgamer.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="算法" scheme="http://thinkgamer.cn/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>从线性回归看偏差-方差分解（Bias-Variance Decomposition）</title>
    <link href="http://thinkgamer.cn/2019/03/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%BB%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9C%8B%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE%E5%88%86%E8%A7%A3%EF%BC%88Bias-Variance%20Decomposition%EF%BC%89/"/>
    <id>http://thinkgamer.cn/2019/03/25/机器学习/从线性回归看偏差-方差分解（Bias-Variance Decomposition）/</id>
    <published>2019-03-25T15:18:55.000Z</published>
    <updated>2019-04-13T05:12:00.678Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>对于数字序列1，3，5，7，？，正常情况下大家脑海里蹦出的是9，但是217314也是其一个解<br>9对应的数学公式为</p><script type="math/tex; mode=display">f(x)=2x-1</script><p>217314对应的数学公式为</p><script type="math/tex; mode=display">f(x)=\frac{18111}{2} x^{4}-90555x^{3}+\frac{633885}{2}x^{2}-452773x+217331</script><p>Python 实现为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; def f(x):</span><br><span class="line">...     return 18111/2 * pow(x,4) -90555 * pow(x,3) + 633885/2 * pow(x,2) -452773 * x +217331</span><br><span class="line">... </span><br><span class="line">&gt;&gt;&gt; f(1)</span><br><span class="line">1.0</span><br><span class="line">&gt;&gt;&gt; f(2)</span><br><span class="line">3.0</span><br><span class="line">&gt;&gt;&gt; f(3)</span><br><span class="line">5.0</span><br><span class="line">&gt;&gt;&gt; f(4)</span><br><span class="line">7.0</span><br><span class="line">&gt;&gt;&gt; f(5)</span><br><span class="line">217341.0</span><br></pre></td></tr></table></figure></p><p>当机器学习模型进行预测的时候，通常都需要把握一个非常微妙的平衡，一方面我们希望模型能够匹配更多的训练数据，相应的增加其复杂度，否则会丢失相关特征的趋势（即模型过拟合）；但是另一方面，我们又不想让模型过分的匹配训练数据，相应的舍弃部分复杂的，因为这样存在过度解析所有异常值和伪规律的风险，导致模型的泛化能力差（即模型欠拟合）。因此在模型的拟合能力和复杂度之前取得一个比较好的权衡，对于一个模型来讲十分重要。而偏差-方差分解（Bias-Variance Decomposition）就是用来指导和分析这种情况的工具。</p><h1 id="偏差和方差定义"><a href="#偏差和方差定义" class="headerlink" title="偏差和方差定义"></a>偏差和方差定义</h1><ul><li>偏差（Bias）：即预测数据偏离真实数据的情况。</li><li>方差（Variance）：描述的是随机变量的离散程度，即随机变量在其期望值附近的波动程度。</li></ul><h1 id="偏差-方差推导过程"><a href="#偏差-方差推导过程" class="headerlink" title="偏差-方差推导过程"></a>偏差-方差推导过程</h1><p>以回归问题为例，假设样本的真实分布为p_r(x,y)，并采用平方损失函数，模型f(x)的期望错误为(公式2.1)：</p><script type="math/tex; mode=display">R(f) = E_{(x,y)\sim p_r{(x,y)}} \left [ (y-f(x))^2 \right ]</script><p>那么最优模型为(公式2.2)：</p><script type="math/tex; mode=display">f^*(x) = E_{y\sim p_r{(y|x)}} \left [ y \right ]</script><p>其中p_r(y|x)为真实的样本分布，f^*(x)为使用平方损失作为优化目标的最优模型，其损失为(公式2.3)：</p><script type="math/tex; mode=display">\varepsilon  = E_{(x,y)\sim p_r{(x,y)}} \left [ (y-f^*(x))^2 \right ]</script><p>损失</p><script type="math/tex; mode=display">\varepsilon</script><p>通常是由于样本分布及其噪声引起的，无法通过优化模型来减少。<br>期望错误可以分解为(公式2.4)：</p><script type="math/tex; mode=display">R(f)</script><script type="math/tex; mode=display">= E_{(x,y)\sim p_r{(x,y)}} \left [ (y- f^*(x) + f^*(x) -f(x))^2 \right ]</script><script type="math/tex; mode=display">= E_{x\sim p_r{(x)}}\left [ (f(x) - f^*(x))^2 \right ] + \varepsilon</script><p>公式2.4中的第一项是机器学习可以优化的真实目标，是当前模型和最优模型之间的差距。</p><p>在实际训练一个模型f(x)时，训练集D是从真实分布p_r(x,y)上独立同分布的采样出来的有限样本集合。不同的训练集会得到不同的模型。令f_D(x)表示在训练集D上学习到的模型，一个机器学习算法（包括模型和优化算法）的能力可以通过模型在不同训练集上的平均性能来体现。</p><p>对于单个样本x，不同训练集D得到的模型f_D(x)和最优模型f^*(x)的上的期望差距为（公式2.5）：</p><script type="math/tex; mode=display">E_D[( f_D(x)-f^*(x) )^2]</script><script type="math/tex; mode=display">=E_D\left [  ( f_D(x)  - E_D[f_D(x)] +E_D[f_D(x)]   -f^*(x) )^2   \right ]</script><script type="math/tex; mode=display">=( E_D[f_D(x)]   -f^*(x) )^2 )  + E_D[(f_D(x)  - E_D[f_D(x)] )^2]</script><p>公式2.5最后一行中的第一项为偏差(bias)，是指一个模型在不同训练集上的平均性能和与最优模型的差异，偏差可以用来衡量一个模型的拟合能力；第二项是方差（variance），是指一个模型在不同训练集上的差异，可以用来衡量一个模型是否容易过拟合。</p><p>集合公式2.4和公式2.5，期望错误可以分解为(公式2.6)：</p><script type="math/tex; mode=display">R(f)= (bias)^2 + variance +  \varepsilon</script><p>其中</p><script type="math/tex; mode=display">(bias)^2 =  E_X[E_D[f_D(x)]   -f^*(x) )^2 ]</script><script type="math/tex; mode=display">variance = E_X [ E_D[(f_D(x)  - E_D[f_D(x)] )^2] ]</script><p>最小化期望错误等价于最小化偏差和方差之和。</p><h1 id="偏差和方差分析"><a href="#偏差和方差分析" class="headerlink" title="偏差和方差分析"></a>偏差和方差分析</h1><p><img src="https://img-blog.csdnimg.cn/20190325231703314.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>上图为机器学习中偏差和方差的四种不同情况。每个图的中心点为最优模型f*(x)，蓝点为不同训练集D 上得到的模型f_D(x)。</p><ul><li>(a)给出了一种理想情况，方差和偏差都比较小</li><li>(b)为高偏差低方差的情况，表示模型的泛化能力很好，但拟合能力不足</li><li>(c)为低偏差高方差的情况，表示模型的拟合能力很好，但泛化能力比较差。当训练数据比较少时会导致过拟合</li><li>(d)为高偏差高方差的情况，是一种最差的情况</li></ul><p>方差一般会随着训练样本的增加而减少。当样本比较多时，方差比较少，我们可以选择能力强的模型来减少偏差。然而在很多机器学习任务上，训练集上往往都比较有限，最优的偏差和最优的方差就无法兼顾。</p><p>随着模型复杂度的增加，模型的拟合能力变强，偏差减少而方差增大，从而导致过拟合。以结构错误最小化为例，我们可以调整正则化系数λ来控制模型的复杂度。当λ变大时，模型复杂度会降低，可以有效地减少方差，避免过拟合，但偏差会上升。当λ过大时，总的期望错误反而会上升。因此，正则化系数λ需要在偏差和方差之间取得比较好的平衡。下图给出了机器学习模型的期望错误、偏差和方差随复杂度的变化情况。最优的模型并不一定是偏差曲线和方差曲线的交点。</p><p><img src="https://img-blog.csdnimg.cn/20190325231720819.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>偏差和方差分解给机器学习模型提供了一种分析途径，但在实际操作中难以直接衡量。一般来说，当一个模型在训练集上的错误率比较高时，说明模型的拟合能力不够，偏差比较高。这种情况可以增加数据特征、提高模型复杂度，减少正则化系数等操作来改进模型。当模型在训练集上的错误率比较低，但验证集上的错误率比较高时，说明模型过拟合，方差比较高。这种情况可以通过降低模型复杂度，加大正则化系数，引入先验等方法来缓解。此外，还有一种有效的降低方差的方法为集成模型，即通过多个高方差模型的平均来降低方差。 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="线性模型" scheme="http://thinkgamer.cn/tags/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="线性回归" scheme="http://thinkgamer.cn/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="方差" scheme="http://thinkgamer.cn/tags/%E6%96%B9%E5%B7%AE/"/>
    
      <category term="偏差" scheme="http://thinkgamer.cn/tags/%E5%81%8F%E5%B7%AE/"/>
    
  </entry>
  
  <entry>
    <title>基于神经网络实现Mnist数据集的多分类</title>
    <link href="http://thinkgamer.cn/2019/03/09/TensorFlow/%E5%9F%BA%E4%BA%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0Mnist%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%A4%9A%E5%88%86%E7%B1%BB/"/>
    <id>http://thinkgamer.cn/2019/03/09/TensorFlow/基于神经网络实现Mnist数据集的多分类/</id>
    <published>2019-03-09T13:19:08.000Z</published>
    <updated>2019-04-13T05:56:37.338Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><blockquote><p>在之前的文章中介绍了基于Logistic Regression实现Mnist数据集的多分类，本篇文章主要介绍基于TensorFlow实现Mnist数据集的多分类。</p></blockquote><p>一个典型的神经网络训练图如下所示：<br><img src="https://img-blog.csdnimg.cn/20190308005540655.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><p>只不过在Mnist数据集是十分类的，起输出由y1和y2换成y1，….，y10。本文实现的神经网络如下所示：</p><p>这是使用的是两层的神经网络，第一层神经元个数是256，第二层为128，最终输出的是10个类别。对应的神经网络结果如下图所示：<br><img src="https://img-blog.csdnimg.cn/20190309205947261.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><p>接着我们创建一个MutilClass类，并初始化相关参数用来实现基于神经网络的多分类函数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"> </span><br><span class="line">class MutilClass:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        # 加载数据集</span><br><span class="line">        self.Mnsit = input_data.read_data_sets(&quot;./data/&quot;, one_hot=True)</span><br><span class="line"> </span><br><span class="line">        # 设置神经网络层参数</span><br><span class="line">        self.n_hidden_1 = 256</span><br><span class="line">        self.n_hidden_2 = 128</span><br><span class="line">        self.n_input = 784</span><br><span class="line">        self.n_classes = 10</span><br><span class="line"> </span><br><span class="line">        self.x = tf.placeholder(dtype=float, shape=[None, self.n_input], name=&quot;x&quot;)</span><br><span class="line">        self.y = tf.placeholder(dtype=float, shape=[None, self.n_classes], name=&quot;y&quot;)</span><br><span class="line">        # random_normal 高斯分布初始化权重</span><br><span class="line">        self.weights = &#123;</span><br><span class="line">            &quot;w1&quot;: tf.Variable(tf.random_normal([self.n_input, self.n_hidden_1],stddev = 0.1)),</span><br><span class="line">            &quot;w2&quot;: tf.Variable(tf.random_normal([self.n_hidden_1, self.n_hidden_2], stddev = 0.1)),</span><br><span class="line">            &quot;out&quot;: tf.Variable(tf.random_normal([self.n_hidden_2, self.n_classes], stddev = 0.1))</span><br><span class="line">        &#125;</span><br><span class="line">        self.bias = &#123;</span><br><span class="line">            &quot;b1&quot;: tf.Variable(tf.random_normal([ self.n_hidden_1 ])),</span><br><span class="line">            &quot;b2&quot;: tf.Variable(tf.random_normal([ self.n_hidden_2 ])),</span><br><span class="line">            &quot;out&quot;: tf.Variable(tf.random_normal([ self.n_classes ]))</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        print(&quot;参数初始化完成！&quot;)</span><br></pre></td></tr></table></figure><p>神经网络首次循环，是根据初始化的参数和偏置，向前传播，经过两层隐层，最终的到一个对应各个类别的概率，然后再根据反向传播，最小化损失函数求解参数，所以这里创建一个前向传播和反向传播的函数，如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 定义一个MLP，前向感知器</span><br><span class="line">def _multilayer_perceptron(self,_X, _weights, _bias):</span><br><span class="line">    layer_1 = tf.nn.sigmoid(tf.add ( tf.matmul(_X, _weights[&quot;w1&quot;]), _bias[&quot;b1&quot;] ) )</span><br><span class="line">    layer_2 = tf.nn.sigmoid(tf.add ( tf.matmul(layer_1, _weights[&quot;w2&quot;]), _bias[&quot;b2&quot;] ) )</span><br><span class="line">    return (tf.matmul( layer_2 ,_weights[&quot;out&quot;] ) + _bias[&quot;out&quot;])</span><br><span class="line"> </span><br><span class="line"># 定义反向传播</span><br><span class="line">def _back_propagation(self):</span><br><span class="line">    pred = self._multilayer_perceptron(self.x, self.weights, self.bias)</span><br><span class="line">    # logits 未归一化的概率</span><br><span class="line">    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=self.y) )</span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer( learning_rate= 0.001).minimize(cost)</span><br><span class="line">    corr = tf.equal(tf.argmax(pred, 1), tf.argmax(self.y, 1) )</span><br><span class="line">    accr =tf.reduce_mean(tf.cast(corr, dtype=float))</span><br><span class="line"> </span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    return init, optimizer,cost, accr</span><br></pre></td></tr></table></figure></p><p>接着就是训练网络了，指定的迭代次数为：100，batch_size：100，其对应的函数未：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># 训练模型</span><br><span class="line">def _train_model(self, _init, _optimizer, _cost, _accr):</span><br><span class="line">    epochs = 100</span><br><span class="line">    batch_size = 100</span><br><span class="line">    display_steps = 1</span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    sess.run(_init)</span><br><span class="line"> </span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        avg_cost = 0</span><br><span class="line">        total_batch = int (self.Mnsit.train.num_examples / batch_size)</span><br><span class="line">        for i in range(total_batch):</span><br><span class="line">            batch_xs, batch_ys = self.Mnsit.train.next_batch(batch_size)</span><br><span class="line">            feeds = &#123;self.x: batch_xs, self.y: batch_ys&#125;</span><br><span class="line">            sess.run(_optimizer, feed_dict=feeds)</span><br><span class="line">            avg_cost += sess.run(_cost, feed_dict=feeds)</span><br><span class="line">        avg_cost = avg_cost / total_batch</span><br><span class="line"> </span><br><span class="line">        if (epoch +1) % display_steps ==0:</span><br><span class="line">            print(&quot;Epoch: &#123;&#125; / &#123;&#125;, cost: &#123;&#125;&quot;.format(epoch, epochs, avg_cost))</span><br><span class="line">            feeds = &#123;self.x: batch_xs, self.y: batch_ys&#125;</span><br><span class="line">            train_acc = sess.run(_accr, feed_dict=feeds)</span><br><span class="line">            print(&quot;Train Accuracy: &#123;&#125;&quot;.format(train_acc))</span><br><span class="line"> </span><br><span class="line">            feeds = &#123;self.x : self.Mnsit.test.images, self.y: self.Mnsit.test.labels&#125;</span><br><span class="line">            test_acc = sess.run(_accr, feed_dict= feeds)</span><br><span class="line">            print(&quot;Test Accuracy: &#123;&#125;&quot;.format(test_acc))</span><br><span class="line">            print(&quot;-&quot; * 50)</span><br></pre></td></tr></table></figure><p>创建主函数，进行迭代训练<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    network = MutilClass()</span><br><span class="line">    init, optimizer, cost, accr = network._back_propagation()</span><br><span class="line">    network._train_model(init, optimizer,cost, accr)</span><br></pre></td></tr></table></figure></p><p>最后的迭代结果为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 0 / 100, cost: 2.4407591546665537</span><br><span class="line">Train Accuracy: 0.12999999523162842</span><br><span class="line">Test Accuracy: 0.12960000336170197</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Epoch: 1 / 100, cost: 2.290777679356662</span><br><span class="line">Train Accuracy: 0.12999999523162842</span><br><span class="line">Test Accuracy: 0.1469999998807907</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Epoch: 2 / 100, cost: 2.2774649468335237</span><br><span class="line">Train Accuracy: 0.17000000178813934</span><br><span class="line">Test Accuracy: 0.21799999475479126</span><br><span class="line">--------------------------------------------------</span><br><span class="line"> </span><br><span class="line">.......</span><br><span class="line"> </span><br><span class="line">--------------------------------------------------</span><br><span class="line">Epoch: 98 / 100, cost: 0.7186844098567963</span><br><span class="line">Train Accuracy: 0.8299999833106995</span><br><span class="line">Test Accuracy: 0.8371999859809875</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Epoch: 99 / 100, cost: 0.7124480505423112</span><br><span class="line">Train Accuracy: 0.8100000023841858</span><br><span class="line">Test Accuracy: 0.8377000093460083</span><br><span class="line">--------------------------------------------------</span><br></pre></td></tr></table></figure></p><p>从结果中可以看出，cost是一直在减少，训练集和测试集评估模型的准确率也在一直提高。当然我们也可以通过调节epoch，batch_size来重新训练模型。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="深度学习" scheme="http://thinkgamer.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="TensorFlow" scheme="http://thinkgamer.cn/tags/TensorFlow/"/>
    
      <category term="LR" scheme="http://thinkgamer.cn/tags/LR/"/>
    
  </entry>
  
  <entry>
    <title>机器学习在微博信息流推荐中的应用实践</title>
    <link href="http://thinkgamer.cn/2019/03/05/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9C%A8%E5%BE%AE%E5%8D%9A%E4%BF%A1%E6%81%AF%E6%B5%81%E6%8E%A8%E8%8D%90%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%E5%AE%9E%E8%B7%B5/"/>
    <id>http://thinkgamer.cn/2019/03/05/推荐系统/机器学习在微博信息流推荐中的应用实践/</id>
    <published>2019-03-05T00:03:35.000Z</published>
    <updated>2019-04-13T05:53:28.074Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><p>本文分为四部分介绍机器学习在微博信息流中的应用实践，分别为：微博信息流推荐场景介绍，内容理解与用户画像，大规模推荐系统实践和总结展望。</p><h1 id="微博信息流推荐场景介绍"><a href="#微博信息流推荐场景介绍" class="headerlink" title="微博信息流推荐场景介绍"></a>微博信息流推荐场景介绍</h1><blockquote><p>微博的feed流内容形态各异，有视频，图片，文字，长文，问答等，其用户量也很大，2018年Q2统计DAU（日活）为1.9亿，MAU（月活）为4.3亿，这么庞大的用户量，如何做好首页feed流的个性化推荐就显得格外重要。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/20190305162718873.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305162740161.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><h1 id="内容理解与用户画像"><a href="#内容理解与用户画像" class="headerlink" title="内容理解与用户画像"></a>内容理解与用户画像</h1><blockquote><p>由于个性化推荐是给用户推荐其感兴趣的内容，所以对于微博的内容理解和用户画像部分就显得格外重要。内容理解即通过文本内容理解和视觉理解技术，对微博内容进行细粒度表征，即形成每篇微博内容的表征向量。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/20190305162850198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305162905934.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><blockquote><p>用户画像即基于用户的发博内容，行为数据，自填信息等进行深度挖掘，精准分析刻画用户，从而在进行微博内容推送时能够实现其个性化。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/20190305162934416.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><h1 id="大规模推荐系统实践"><a href="#大规模推荐系统实践" class="headerlink" title="大规模推荐系统实践"></a>大规模推荐系统实践</h1><blockquote><p>目前推荐架构的实现思路都是先从海量原始数据中，依据用户画像，召回用户偏好的数据，在利用排序算法对其进行排序，最终选择top K返回给用户。微博推荐亦是如此。其整体的流程图如下所示：</p><p>物料召回：即从候选物料集合中粗筛物料，作为进行模型的待排序物料。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/20190305163036955.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><p><img src="https://img-blog.csdnimg.cn/20190305163059488.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><p><img src="https://img-blog.csdnimg.cn/20190305163120489.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><blockquote><p>算法排序则是结合相关特征对物料召回的内容进行预估排序，其特征主要分为：用户特征，内容特征，环境特征，组合特征和上下文特征等。</p></blockquote><p><img src="https://img-blog.csdnimg.cn/20190305163427457.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163455977.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163515432.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163530356.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163645420.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163705966.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163719819.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163749462.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163805882.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163818878.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"><br><img src="https://img-blog.csdnimg.cn/20190305163833250.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70"></p><h1 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a>总结与展望</h1><ul><li><p>总结</p><ul><li>业务和数据决定了模型算法的应用场景</li><li>模型算法殊途同归</li><li>工程能力和算法架构是基本保障</li></ul></li><li><p>展望</p><ul><li>采用多模型融合，能更好的对非结构化内容进行表征</li><li>更多的融合网络结构适用于CTR预估场景</li></ul></li></ul><hr><p>完整内容请阅读原文：<a href="https://blog.csdn.net/Gamer_gyt/article/details/88164127" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/88164127</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="深度学习" scheme="http://thinkgamer.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="推荐系统" scheme="http://thinkgamer.cn/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="机器学习" scheme="http://thinkgamer.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow实现Mnist数据集的多分类逻辑回归模型</title>
    <link href="http://thinkgamer.cn/2019/02/27/TensorFlow/TensorFlow%E5%AE%9E%E7%8E%B0Mnist%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%A4%9A%E5%88%86%E7%B1%BB%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
    <id>http://thinkgamer.cn/2019/02/27/TensorFlow/TensorFlow实现Mnist数据集的多分类逻辑回归模型/</id>
    <published>2019-02-27T05:15:55.000Z</published>
    <updated>2019-04-13T05:46:04.119Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><blockquote><p>多分类逻辑回归基于逻辑回归（Logistic Regression，LR）和softMax实现，其在多分类分类任务中应用广泛，本篇文章基于tf实现多分类逻辑回归，使用的数据集为Mnist。</p></blockquote><p>多分类逻辑回归的基础概要和在Spark上的实现可参考：</p><ul><li>多分类逻辑回归（Multinomial Logistic Regression）</li><li>多分类实现方式介绍和在Spark上实现多分类逻辑回归（Multinomial Logistic Regression）</li></ul><p>本篇文章涉及到的tf相关接口函数及释义如下：</p><h2 id="tf-nn-softmax"><a href="#tf-nn-softmax" class="headerlink" title="tf.nn.softmax"></a>tf.nn.softmax</h2><p>Softmax 在机器学习和深度学习中有着非常广泛的应用。尤其在处理多分类（C &gt; 2）问题，分类器最后的输出单元需要Softmax 函数进行数值处理。关于Softmax 函数的定义如下所示：</p><p>…</p><hr><p>完整内容请阅读原文：<a href="https://blog.csdn.net/Gamer_gyt/article/details/87970776" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/87970776</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="深度学习" scheme="http://thinkgamer.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="TensorFlow" scheme="http://thinkgamer.cn/tags/TensorFlow/"/>
    
      <category term="LR" scheme="http://thinkgamer.cn/tags/LR/"/>
    
  </entry>
  
  <entry>
    <title>深度学习中的epochs、batch_size、iterations详解</title>
    <link href="http://thinkgamer.cn/2019/02/26/TensorFlow/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84epochs%E3%80%81batch_size%E3%80%81iterations%E8%AF%A6%E8%A7%A3/"/>
    <id>http://thinkgamer.cn/2019/02/26/TensorFlow/深度学习中的epochs、batch_size、iterations详解/</id>
    <published>2019-02-25T16:32:29.000Z</published>
    <updated>2019-04-13T05:38:21.475Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><blockquote><p>深度学习中涉及到很多参数，如果对于一些参数不了解，那么去看任何一个框架都会有难度，在TensorFlow中有一些模型训练的基本参数，这些参数是训练模型的前提，也在一定程度上影响着模型的最终效果。下面主要介绍几个参数。</p></blockquote><ul><li>batch_size</li><li>iterations</li><li>epochs</li></ul><h1 id="batch-size"><a href="#batch-size" class="headerlink" title="batch_size"></a>batch_size</h1><p>深度学习的优化算法，其实就是梯度下降，在之前的文章中我们也介绍过梯度下降，这里就不详细说明。梯度下降分为三种：</p><ul><li>批量梯度下降算法（BGD，Batch gradient descent algorithm）</li><li>随机梯度下降算法（SGD，Stochastic gradient descent algorithm）</li><li>小批量梯度下降算法（MBGD，Mini-batch gradient descent algorithm）</li></ul><p>批量梯度下降算法，每一次计算都需要遍历全部数据集，更新梯度，计算开销大，花费时间长，不支持在线学习。</p><p>随机梯度下降算法，每次随机选取一条数据，求梯度更新参数，这种方法计算速度快，但是收敛性能不太好，可能在最优点附近晃来晃去，hit不到最优点。两次参数的更新也有可能互相抵消掉，造成目标函数震荡的比较剧烈。</p><p>为了克服两种方法的缺点，现在一般采用的是一种折中手段，mini-batch gradient decent，小批的梯度下降，这种方法把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也不是很大。</p><p>tf框架中的batch_size指的就是更新梯度中使用的样本数。当然这里如果把batch_size设置为数据集的长度，就成了批量梯度下降算法，batch_size设置为1就是随机梯度下降算法。</p><h1 id="iterations"><a href="#iterations" class="headerlink" title="iterations"></a>iterations</h1><p>迭代次数，每次迭代更新一次网络结构的参数。</p><p>迭代是重复反馈的动作，神经网络中我们希望通过迭代进行多次的训练以到达所需的目标或结果。</p><p>每一次迭代得到的结果都会被作为下一次迭代的初始值。</p><p>一个迭代 = 一个（batch_size）数据正向通过（forward）+ 一个（batch_size）数据反向（backward）</p><p><img src="https://img-blog.csdnimg.cn/2019022523170956.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly90aGlua2dhbWVyLmJsb2cuY3Nkbi5uZXQ=,size_16,color_FFFFFF,t_70" alt="神经网络"></p><p>前向传播：构建由（x1,x2,x3）得到Y（hwb(x)）的表达式</p><p>反向传播：基于给定的损失函数，求解参数的过程</p><h1 id="epochs"><a href="#epochs" class="headerlink" title="epochs"></a>epochs</h1><p>epochs被定义为前向和反向传播中所有批次的单次训练迭代。这意味着1个周期是整个输入数据的单次前向和反向传递。</p><p>简单说，epochs指的就是训练过程中数据将被“轮”多少次</p><p>例如在某次模型训练过程中，总的样本数是10000，batch_size=100，epochs=10，其对应的伪代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data = </span><br><span class="line">batch_size = 100</span><br><span class="line">for i in range(epochs):</span><br><span class="line">    for j in range(int(data_length / batch_size - 1)):</span><br><span class="line">        x_data = data[begin:end, ]</span><br><span class="line">        y_data = data[begin:end, ]</span><br><span class="line">        mode.train(x_data, y_data)</span><br><span class="line">        begin += batch_size</span><br><span class="line">        end += batch_size</span><br></pre></td></tr></table></figure></p><p>其中iterations = data_length / batch_size</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="深度学习" scheme="http://thinkgamer.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="tensorFlow" scheme="http://thinkgamer.cn/tags/tensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Spark排序算法系列之（MLLib、ML）GBTs使用方式介绍</title>
    <link href="http://thinkgamer.cn/2019/01/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Spark%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%E4%B9%8B%EF%BC%88MLLib%E3%80%81ML%EF%BC%89GBTs%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F%E4%BB%8B%E7%BB%8D/"/>
    <id>http://thinkgamer.cn/2019/01/29/机器学习/Spark排序算法系列之（MLLib、ML）GBTs使用方式介绍/</id>
    <published>2019-01-29T13:16:35.000Z</published>
    <updated>2019-04-13T05:43:53.236Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>【Spark排序算法系列】主要介绍的是目前推荐系统或者广告点击方面用的比较广的几种算法，和他们在Spark中的应用实现，本篇文章主要介绍GBDT算法，本系列还包括（持续更新）：</p><ul><li>Spark排序算法系列之LR（逻辑回归）</li><li>Spark排序算法系列之模型融合（GBDT+LR）</li><li>Spark排序算法系列之XGBoost</li><li>Spark排序算法系列之FTRL（Follow-the-regularized-Leader）</li><li>Spark排序算法系列之FM与FFM</li></ul><p>在本篇文章中你可以学到：</p><ul><li>Spark MLLib包中的GBDT使用方式</li><li>模型的通过保存、加载、预测</li><li>PipeLine</li><li>ML包中的GBDT</li></ul><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>LR因为其容易并行最早应用到推荐排序中的，但学习能力有限，需要大量的特征工程来增加模型的学习能力。但大量的特征工程耗时耗力，且不一定带来效果的提升，因此在如何能有效的发现特征组合，来缩短LR特征实验周期的背景下，GBDT被应用了起来。GBDT模型全称是Gradient Boosting Decision Tree，梯度提升决策树。是属于Boosing算法中的一种，关于Boosting的介绍可以参考文章集成学习（Ensemble Learning)</p><p>关于GBDT算法理解可参考：</p><ul><li>Spark排序算法系列之GBTs基础——梯度上升和梯度下降</li><li>梯度提升决策树GBDT（Gradient Boosting Decision Tree）</li></ul><p>其实相信很多人对Spark 机器学习包（ml和mllib）中的GBDT傻傻分不清楚，这里我们先来捋一捋。Spark中的GBDT较GBTs——梯度提升树，因为其是基于决策树（Decision Tree，DT）实现的，所以叫GBDT。Spark 中的GBDT算法存在于ml包和mllib包中，mllib是基于RDD的，ml包则是针对DataFrame的，ml包中的GBDT分为分类和回归，在实际使用过程中，需要根据具体情况进行衡量和选择。由于在实际生产环境中使用基于RDD的较多，所以下面将着重介绍下MLLib包中的GBTs，ML包中的将进行简单说明。</p><hr><p>完整内容请阅读原文：<a href="https://blog.csdn.net/Gamer_gyt/article/details/86695837" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/86695837</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="Spark" scheme="http://thinkgamer.cn/tags/Spark/"/>
    
      <category term="机器学习" scheme="http://thinkgamer.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>资源分享：从数理统计到DL、RL，还不快来！</title>
    <link href="http://thinkgamer.cn/2019/01/28/NLP/%E8%B5%84%E6%BA%90%E5%88%86%E4%BA%AB%EF%BC%9A%E4%BB%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E5%88%B0DL%E3%80%81RL%EF%BC%8C%E8%BF%98%E4%B8%8D%E5%BF%AB%E6%9D%A5/"/>
    <id>http://thinkgamer.cn/2019/01/28/NLP/资源分享：从数理统计到DL、RL，还不快来/</id>
    <published>2019-01-27T19:16:19.000Z</published>
    <updated>2019-04-13T05:41:18.266Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><p>之前在自己的年度总结里写到：19年的目标就是技术沉淀与突破。技术突破不仅包含现有技术的总结和反思，更是对未知技术的探索和求知，希望19年能够更上一层楼。</p><p>这个repo是我一直维护和整理的一个技术资料分享的repo，是我包括群友一块整理的一个免费技术资料分享的库，不仅包含了机器学习，数据挖掘，深度学习，还包含了大数据，数理统计，强化学习等，希望在技术这条路上你能跑的更快。</p><p>repo：<a href="https://github.com/Thinkgamer/books" target="_blank" rel="external">https://github.com/Thinkgamer/books</a></p><p>先来张图片镇楼！！！<br><img src="https://img-blog.csdnimg.cn/20190128031446639.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==,size_16,color_FFFFFF,t_70"></p><h1 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h1><p>建立该Repo的目的有两个：</p><ul><li>本人在各个平台共享书籍，进行一个统一管理</li><li>分享给各个搞技术的朋友，知识无私藏之说</li></ul><hr><h1 id="What"><a href="#What" class="headerlink" title="What"></a>What</h1><p>该Repo会涉及包含以下类别书籍：</p><ul><li>机器学习</li><li>数据挖掘</li><li>深度学习</li><li>NLP</li><li>云计算</li><li>统计学概率论</li><li>收藏的论文</li><li>杂乱无章</li></ul><hr><h1 id="List"><a href="#List" class="headerlink" title="List"></a>List</h1><p>注明：排名无先后顺序</p><h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><ul><li>scikit-learn 中英文</li><li>机器学习-周志华</li><li>机器学习实战</li><li>机器学习导论</li><li>集体智慧编程中文版</li><li>[英文版]叶斯思维：统计建模的Python学习法</li></ul><h2 id="数据挖掘"><a href="#数据挖掘" class="headerlink" title="数据挖掘"></a>数据挖掘</h2><ul><li>python数据分析与挖掘实战</li><li>利用python进行数据分析</li><li>面向程序员的数据挖掘指南</li><li>数据挖掘：概念与技术（中文第三版）</li><li>数据挖掘应用20个案例分析</li><li>数据挖掘与数据化运营实战_思路_方法_技巧与应用_完整版</li></ul><h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><ul><li>神经网络与机器学习（加）Simon Haykin</li><li>TensorFlow实战-黄文坚</li><li>深度学习 中文版</li><li>神经网络与深度学习</li></ul><h2 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h2><ul><li>模式识别与机器学习 中文版</li><li>NLP汉语自然语言处理原理与实践</li><li>PYTHON自然语言处理中文版</li></ul><h2 id="推荐系统"><a href="#推荐系统" class="headerlink" title="推荐系统"></a>推荐系统</h2><ul><li>推荐系统实践</li><li>learning-to-rank.pdf </li><li>Recommender Systems Handbook.pdf</li><li>Context-Aware-Recommender-Systems-chapter-7.pdf</li></ul><h2 id="云计算"><a href="#云计算" class="headerlink" title="云计算"></a>云计算</h2><ul><li>《快学Scala》</li><li>Learning PySpark.pdf</li><li>SparkMLlib机器学习</li><li>Spark快速大数据分析</li><li>数据算法  Hadoop Spark大数据处理技巧</li></ul><h2 id="统计学与概率论"><a href="#统计学与概率论" class="headerlink" title="统计学与概率论"></a>统计学与概率论</h2><ul><li>《概率论与数理统计》浙大版（第四版）教材</li><li>《概率论与数理统计习题全解指南》.浙大版（第四版）</li><li>数理统计与数据分析原书第3版</li><li>应用商务统计分析 王汉生(2008).pdf</li></ul><h2 id="收藏的论文"><a href="#收藏的论文" class="headerlink" title="收藏的论文"></a>收藏的论文</h2><ul><li>平滑系数自适应的二次指数平滑模型及其应用</li><li>The Structure of Collaborative Tagging Systems</li><li>FM</li><li>FFM</li><li>DeepFFM</li><li>Focal Loss for Dense Object Detection</li><li>Attentive Group Recommendation.pdf</li><li>Real-time Personalization using Embeddings for Search.pdf</li></ul><h2 id="杂乱无章"><a href="#杂乱无章" class="headerlink" title="杂乱无章"></a>杂乱无章</h2><ul><li>阿里技术之瞳-p260</li><li>数据敏感性测试</li><li>正则表达式经典实例.（美）高瓦特斯，（美）利维森</li><li>阿里广告中的机器学习平台.pdf</li><li>广告数据上的大规模机器学习.pdf</li><li>绿盟大数据安全分析平台 产品白皮书.pdf</li><li>Xdef2013-基于机器学习和NetFPGA的智能高速入侵防御系统.ppt</li><li>04-程佳-推荐广告机器学习实践</li><li>A Gentle Introduction to Gradient Boosting.pdf</li><li>GBDT算法原理与系统设计简介.pdf</li><li>[微博] 机器学习在微博信息流推荐应用实践.pdf</li><li>[知乎] 首页信息流系统的框架及机器学习技术在推荐策略中的应用.pdf</li></ul><h2 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h2><ul><li>强化学习在阿里的技术演进与业务创新.pdf</li></ul><h2 id="技术集锦"><a href="#技术集锦" class="headerlink" title="技术集锦"></a>技术集锦</h2><ul><li>AAAI2018.pdf</li><li>数字经济下的算法力量.pdf</li><li>2018美团点评算法系列.pdf</li><li>Learning To Rank在个性化电商搜索中的应用</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="资源" scheme="http://thinkgamer.cn/tags/%E8%B5%84%E6%BA%90/"/>
    
      <category term="DL" scheme="http://thinkgamer.cn/tags/DL/"/>
    
      <category term="RL" scheme="http://thinkgamer.cn/tags/RL/"/>
    
      <category term="ML" scheme="http://thinkgamer.cn/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>多分类实现方式介绍和在Spark上实现多分类逻辑回归</title>
    <link href="http://thinkgamer.cn/2019/01/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%9A%E5%88%86%E7%B1%BB%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F%E4%BB%8B%E7%BB%8D%E5%92%8C%E5%9C%A8Spark%E4%B8%8A%E5%AE%9E%E7%8E%B0%E5%A4%9A%E5%88%86%E7%B1%BB%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://thinkgamer.cn/2019/01/12/机器学习/多分类实现方式介绍和在Spark上实现多分类逻辑回归/</id>
    <published>2019-01-12T14:06:02.000Z</published>
    <updated>2019-04-13T05:38:13.219Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>在之前的文章中介绍了多分类逻辑回归算法的数据原理，参考文章链接</p><p>CSDN文章链接：<a href="https://blog.csdn.net/Gamer_gyt/article/details/85209496" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/85209496</a><br>公众号：多分类逻辑回归（Multinomial Logistic Regression）<br>该篇文章介绍一下Spark中多分类算法，主要包括的技术点如下</p><ul><li><p>多分类实现方式</p><ul><li>一对一 （One V One）</li><li>一对其余（One V Remaining）</li><li>多对多 （More V More）</li></ul></li><li><p>Spark中的多分类实现</p><h1 id="多分类实现方式"><a href="#多分类实现方式" class="headerlink" title="多分类实现方式"></a>多分类实现方式</h1><h2 id="一对一"><a href="#一对一" class="headerlink" title="一对一"></a>一对一</h2><p>假设某个分类中有N个类别，将这N个类别两两配对（继而转化为二分类问题），这样可以得到 N（N-1）/ 2个二分类器，这样训练模型时需要训练 N（N-1）/ 2个模型，预测时将样本输送到这些模型中，最终统计出现次数较多的类别结果作为最终类别。</p></li></ul><p>假设现在有三个类别：类别A，类别B，类别C，类别D。一对一实现多分类如下图所示：<br><img src="https://img-blog.csdnimg.cn/20190112220044121.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0dhbWVyX2d5dA==,size_16,color_FFFFFF,t_70"></p><hr><p>完整内容请阅读原文：<a href="https://blog.csdn.net/Gamer_gyt/article/details/86378882" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/86378882</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="Spark" scheme="http://thinkgamer.cn/tags/Spark/"/>
    
      <category term="LR" scheme="http://thinkgamer.cn/tags/LR/"/>
    
      <category term="机器学习" scheme="http://thinkgamer.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Hive Join 分析和优化</title>
    <link href="http://thinkgamer.cn/2019/01/03/Spark/Hive%20Join%20%E5%88%86%E6%9E%90%E5%92%8C%E4%BC%98%E5%8C%96/"/>
    <id>http://thinkgamer.cn/2019/01/03/Spark/Hive Join 分析和优化/</id>
    <published>2019-01-03T05:34:48.000Z</published>
    <updated>2019-04-13T05:33:15.758Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>Sku对应品牌进行关联，大表对应非大表（这里的非大表并不能用小表来定义）</p><h1 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h1><p>进行表左关联时，最后一个reduce任务卡到99%，运行时间很长，发生了严重的数据倾斜。</p><p>什么是数据倾斜？数据倾斜主要表现在，map /reduce程序执行时，reduce节点大部分执行完毕，但是有一个或者几个reduce节点运行很慢，导致整个程序的处理时间很长，这是因为某一个key的条数比其他key多很多（有时是百倍或者千倍之多），这条key所在的reduce节点所处理的数据量比其他节点就大很多，从而导致某几个节点迟迟运行不完。</p><p>完整内容请阅读原文：<a href="https://blog.csdn.net/Gamer_gyt/article/details/85690885" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/85690885</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="Spark" scheme="http://thinkgamer.cn/tags/Spark/"/>
    
      <category term="Hive" scheme="http://thinkgamer.cn/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 特征工程:feature_column</title>
    <link href="http://thinkgamer.cn/2019/01/03/TensorFlow/TensorFlow%20%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B:%20feature_column/"/>
    <id>http://thinkgamer.cn/2019/01/03/TensorFlow/TensorFlow 特征工程: feature_column/</id>
    <published>2019-01-03T05:18:16.000Z</published>
    <updated>2019-04-13T05:28:32.930Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><h1 id="特征工程-feature-column"><a href="#特征工程-feature-column" class="headerlink" title="特征工程: feature_column"></a>特征工程: feature_column</h1><p>在使用很多模型的时候，都需要对输入的数据进行必要的特征工程处理。最典型的就是:one-hot处理，还有hash分桶等处理。为了方便处理这些特征，tensorflow提供了一些列的特征工程方法来方便使用.</p><h2 id="公共的import"><a href="#公共的import" class="headerlink" title="公共的import"></a>公共的import</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.python.estimator.inputs import numpy_io</span><br><span class="line">import numpy as np</span><br><span class="line">import collections</span><br><span class="line">from tensorflow.python.framework import errors</span><br><span class="line">from tensorflow.python.platform import test</span><br><span class="line">from tensorflow.python.training import coordinator</span><br><span class="line">from tensorflow import feature_column</span><br><span class="line"></span><br><span class="line">from tensorflow.python.feature_column.feature_column import _LazyBuilder</span><br></pre></td></tr></table></figure><h2 id="numeric-column"><a href="#numeric-column" class="headerlink" title="numeric_column"></a>numeric_column</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">numeric_column(</span><br><span class="line">    key,</span><br><span class="line">    shape=(1,),</span><br><span class="line">    default_value=None,</span><br><span class="line">    dtype=tf.float32,</span><br><span class="line">    normalizer_fn=None</span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>key：特征的名字。也就是对应的列名称</li><li>shape：该key所对应的特征的shape. 默认是1，但是比如one-hot类型的，shape就不是1，而是实际的维度。总之，这里是key所对应的维度，不一定是1</li><li>default_value：如果不存在使用的默认值</li><li>normalizer_fn：对该特征下的所有数据进行转换。如果需要进行normalize，那么就是使用normalize的函数.这里不仅仅局限于normalize，也可以是任何的转换方法，比如取对数，取指数，这仅仅是一种变换方法</li></ul><p>完整内容请阅读：<a href="https://blog.csdn.net/Gamer_gyt/article/details/85689840" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/85689840</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="深度学习" scheme="http://thinkgamer.cn/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="tensorFlow" scheme="http://thinkgamer.cn/tags/tensorFlow/"/>
    
      <category term="特征过程" scheme="http://thinkgamer.cn/tags/%E7%89%B9%E5%BE%81%E8%BF%87%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>集成学习（Ensemble Learning)</title>
    <link href="http://thinkgamer.cn/2019/01/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%EF%BC%88Ensemble%20Learning)/"/>
    <id>http://thinkgamer.cn/2019/01/03/机器学习/集成学习（Ensemble Learning)/</id>
    <published>2019-01-03T05:14:38.000Z</published>
    <updated>2019-04-13T05:24:56.867Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast"></center><center>打开微信扫一扫，关注微信公众号【数据与算法联盟】 </center><p>转载请注明出处：<a href="http://blog.csdn.net/gamer_gyt" target="_blank" rel="external">http://blog.csdn.net/gamer_gyt</a><br>博主微博：<a href="http://weibo.com/234654758" target="_blank" rel="external">http://weibo.com/234654758</a><br>Github：<a href="https://github.com/thinkgamer" target="_blank" rel="external">https://github.com/thinkgamer</a></p><hr><blockquote><p>在机器学习的有监督学习算法中，我们的目标是学习出一个稳定的且在各个方面表现都较好的模型，但实际情况往往不这么理想，有时我们只能得到多个有偏好的模型（弱监督模型，在某些方面表现的比较好）。集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。</p></blockquote><p>集成学习在各个规模的数据集上都有很好的策略。</p><ul><li>数据集大：划分成多个小数据集，学习多个模型进行组合</li><li>数据集小：利用Bootstrap方法进行抽样，得到多个数据集，分别训练多个模型再进行组合</li></ul><p>这篇博客介绍一下集成学习的几类：Bagging，Boosting以及Stacking。</p><h1 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h1><p>Bagging是bootstrap aggregating的简写。先说一下bootstrap，bootstrap也称为自助法，它是一种有放回的抽样方法，目的为了得到统计量的分布以及置信区间。具体步骤如下</p><ul><li>采用重抽样方法（有放回抽样）从原始样本中抽取一定数量的样本</li><li>根据抽出的样本计算想要得到的统计量T</li><li>重复上述N次（一般大于1000），得到N个统计量T</li><li>根据这N个统计量，即可计算出统计量的置信区间</li></ul><p>在Bagging方法中，利用bootstrap方法从整体数据集中采取有放回抽样得到N个数据集，在每个数据集上学习出一个模型，最后的预测结果利用N个模型的输出得到，具体地：分类问题采用N个模型预测投票的方式，回归问题采用N个模型预测平均的方式。</p><p>例如随机森林（Random Forest）就属于Bagging。随机森林简单地来说就是用随机的方式建立一个森林，森林由很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。</p><p>在我们学习每一棵决策树的时候就需要用到Bootstrap方法。在随机森林中，有两个随机采样的过程：对输入数据的行（数据的数量）与列（数据的特征）都进行采样。对于行采样，采用有放回的方式，若有N个数据，则采样出N个数据（可能有重复），这样在训练的时候每一棵树都不是全部的样本，相对而言不容易出现overfitting；接着进行列采样从M个feature中选择出m个（m&lt;&lt;M）。最近进行决策树的学习。</p><p>预测的时候，随机森林中的每一棵树的都对输入进行预测，最后进行投票，哪个类别多，输入样本就属于哪个类别。这就相当于前面说的，每一个分类器（每一棵树）都比较弱，但组合到一起（投票）就比较强了。</p><h1 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h1><p>提升方法（Boosting）是一种可以用来减小监督学习中偏差的机器学习算法。主要也是学习一系列弱分类器，并将其组合为一个强分类器。Boosting中有代表性的是AdaBoost（Adaptive boosting）算法：刚开始训练时对每一个训练例赋相等的权重，然后用该算法对训练集训练t轮，每次训练后，对训练失败的训练例赋以较大的权重，也就是让学习算法在每次学习以后更注意学错的样本，从而得到多个预测函数。具体可以参考《统计学习方法》。</p><h1 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h1><p>Stacking方法是指训练一个模型用于组合其他各个模型。首先我们先训练多个不同的模型，然后把之前训练的各个模型的输出为输入来训练一个模型，以得到一个最终的输出。理论上，Stacking可以表示上面提到的两种Ensemble方法，只要我们采用合适的模型组合策略即可。但在实际中，我们通常使用logistic回归作为组合策略。</p><p>如下图，先在整个训练数据集上通过bootstrap抽样得到各个训练集合，得到一系列分类模型，称之为Tier 1分类器（可以采用交叉验证的方式学习），然后将输出用于训练Tier 2 分类器。</p><p>完整内容请阅读： <a href="https://blog.csdn.net/Gamer_gyt/article/details/85689424" target="_blank" rel="external">https://blog.csdn.net/Gamer_gyt/article/details/85689424</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;
&lt;img src=&quot;http://img.blog.csdn.net/20171231111930492?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvR2FtZXJfZ3l0/font/5a6L5L2T/fontsi
      
    
    </summary>
    
      <category term="技术篇" scheme="http://thinkgamer.cn/categories/%E6%8A%80%E6%9C%AF%E7%AF%87/"/>
    
    
      <category term="机器学习" scheme="http://thinkgamer.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="集成学习" scheme="http://thinkgamer.cn/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
